
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Math for Machines</title>
    <link href="https://fonts.googleapis.com/css?family=Merriweather:400&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro&display=swap" rel="stylesheet"> 
    <link href="https://unpkg.com/primer/build/build.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/style.css" />
      <!-- Syntax highlighting -->
  <link rel="stylesheet" href="../css/github.css" />
  <script src="../scripts/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class="bg-gray-dark">
    <div class="container-xxxl h-100 my-3">
      <div class="box-shadow-large bg-gray">

        <header id="header" class="bg-white">

  <div class="p-4" style="text-align: center">

    <a class="title" href="../">
      Math for Machines
    </a>

  </div>
  
  <nav class="UnderlineNav UnderlineNav--right px-2 border-top">
    <div class="UnderlineNav-body">
      <a class="UnderlineNav-item " href="../about/">
        <span>About</span>
      </a>
      <a class="UnderlineNav-item " href="../archive/">
        <span>Archive</span>
      </a>
    </div>
  </nav>


  <!-- Scripts -->
  <!-- Enable MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic:true } },
    });
    MathJax.Hub.Config({
        TeX: { extensions: ["color.js"] }
    });
    MathJax.Hub.Config({
        TeX: {
            Macros: {
                formbox: ["\\bbox[15px, border:1px solid Gray]{#1}", 1],
            }
        }
    });
  </script>
  <script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133546767-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133546767-1', { 'optimize_id': 'GTM-T3XD3JM'});
  </script>

</header>

        
        <div id="holy">

          <div id="left" class="mr-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Recent Posts</h1>
                <ul>
    
        <li>
          <a href="../posts/discriminant-analysis/">Six Varieties of Gaussian Discriminant Analysis</a>
        </li>
    
        <li>
          <a href="../posts/decision/">Optimal Decision Boundaries</a>
        </li>
    
        <li>
          <a href="../posts/least-squares-with-the-mp-inverse/">Least Squares with the Moore-Penrose Inverse</a>
        </li>
    
        <li>
          <a href="../posts/eigenvalues-and-singular-values/">Understanding Eigenvalues and Singular Values</a>
        </li>
    
        <li>
          <a href="../posts/visualizing-linear-transformations/">Visualizing Linear Transformations</a>
        </li>
    
        <li>
          <a href="../posts/bayes-and-means/">What I'm Reading 1: Bayes and Means</a>
        </li>
    
        <li>
          <a href="../posts/investmentsim/">investmentsim - an R Package for Simulating Investment Portfolios</a>
        </li>
    
        <li>
          <a href="../posts/introduction-to-categories/">Talk: An Introduction to Categories with Haskell and Databases</a>
        </li>
    
        <li>
          <a href="../posts/retirement-formula/">A Somewhat Better Retirement Formula</a>
        </li>
    
        <li>
          <a href="../posts/permitted-and-forbidden-sets/">Journal Review: Permitted and Forbidden Sets in STLNs</a>
        </li>
    
</ul>

              </div>
            </div>
          </div>
            
          <div class="my-2">
          <div role="main" id="main">
  
  
  <div class="rounded-2 box-shadow-medium px-4 pb-4 mb-4 bg-white">

        <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../posts/eigenvalues-and-singular-values/">Understanding Eigenvalues and Singular Values</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../tags/tutorial/">tutorial</a>, <a href="../tags/linear-algebra/">linear-algebra</a>, <a href="../tags/eigenvalues/">eigenvalues</a>, <a href="../tags/singular-values/">singular-values</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: November 15, 2019
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></li>
<li><a href="#singular-values-and-singular-vectors">Singular Values and Singular Vectors</a></li>
<li><a href="#matrix-approximation-with-svd">Matrix Approximation with SVD</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <h1 id="introduction">Introduction</h1>
<p>What are eigenvalues? What are singular values? They both describe the behavior of a matrix on a certain set of vectors. The difference is this: The eigenvectors of a matrix describe the directions of its <em>invariant</em> action. The singular vectors of a matrix describe the directions of its <em>maximum</em> action. And the corresponding eigen- and singular values describe the magnitude of that action.</p>
<p>They are defined this way. A scalar <span class="math inline">\(\lambda\)</span> is an <strong><a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigenvalue</a></strong> of a linear transformation <span class="math inline">\(A\)</span> if there is a vector <span class="math inline">\(v\)</span> such that <span class="math inline">\(A v = \lambda v\)</span>, and <span class="math inline">\(v\)</span> is called an <strong>eigenvector</strong> of <span class="math inline">\(\lambda\)</span>. A scalar <span class="math inline">\(\sigma\)</span> is a <strong><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value</a></strong> of <span class="math inline">\(A\)</span> if there are (unit) vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> such that <span class="math inline">\(A v = \sigma u\)</span> and <span class="math inline">\(A^* u = \sigma v\)</span>, where <span class="math inline">\(A^*\)</span> is the <a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate transpose</a> of <span class="math inline">\(A\)</span>; the vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are <strong>singular vectors</strong>. The vector <span class="math inline">\(u\)</span> is called a <strong>left</strong> singular vector and <span class="math inline">\(v\)</span> a <strong>right</strong> singular vector.</p>
<h1 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h1>
<p>That eigenvectors give the directions of invariant action is obvious from the definition. The definition says that when <span class="math inline">\(A\)</span> acts on an eigenvector, it just multiplies it by a constant, the corresponding eigenvalue. In other words, when a linear transformation acts on one of its eigenvectors, it shrinks the vector or stretches it and reverses its direction if <span class="math inline">\(\lambda\)</span> is negative, but never changes the direction otherwise. The action is invariant.</p>
<p>Take this matrix, for instance:</p>
<p><span class="math display">\[ A = \begin{bmatrix}
0 &amp; 2 \\
2 &amp; 0
\end{bmatrix} \]</span></p>
<figure>
<img src="../images/eigen-circle-1.png" title="eigen-circle-1" alt="Eigenvectors of A" width="400" /><figcaption>Eigenvectors of <span class="math inline">\(A\)</span></figcaption>
</figure>
<p>We can see how the transformation just stretches the red vector by a factor of 2, while the blue vector it stretches but also reflects over the origin.</p>
<p>And this matrix:</p>
<p><span class="math display">\[ A = \begin{bmatrix}
1 &amp; \frac{1}{3} \\
\frac{4}{3} &amp; 1
\end{bmatrix} \]</span></p>
<figure>
<img src="../images/eigen-circle-2.png" title="eigen-circle-2" alt="Eigenvectors of A" width="400" /><figcaption>Eigenvectors of <span class="math inline">\(A\)</span></figcaption>
</figure>
<p>It stretches the red vector and shrinks the blue vector, but reverses neither.</p>
<p>The point is that in every case, when a matrix acts on one of its eigenvectors, the action is always in a parallel direction.</p>
<h1 id="singular-values-and-singular-vectors">Singular Values and Singular Vectors</h1>
<p>This invariant direction does not necessarily give the transformation’s direction of <em>greatest effect</em>, however. You can see that in the previous example. But say <span class="math inline">\(\sigma_1\)</span> is the <em>largest</em> singular value of <span class="math inline">\(A\)</span> with right singular vector <span class="math inline">\(v\)</span>. Then <span class="math inline">\(v\)</span> is a solution to</p>
<p><span class="math display">\[ \operatorname*{argmax}_{x, ||x||=1} ||A x|| \]</span></p>
<p>In other words, <span class="math inline">\( ||A v|| = \sigma_1 \)</span> is at least as big as <span class="math inline">\( ||A x|| \)</span> for any other unit vector <span class="math inline">\(x\)</span>. It’s not necessarily the case that <span class="math inline">\(A v\)</span> is parallel to <span class="math inline">\(v\)</span>, though.</p>
<p>Compare the eigenvectors of the matrix in the last example to its singular vectors:</p>
<figure>
<img src="../images/singular-circle-1.png" title="singular-circle-1" alt="Singular vectors of A" width="400" /><figcaption>Singular vectors of <span class="math inline">\(A\)</span></figcaption>
</figure>
<p>The directions of maximum effect will be exactly the semi-axes of the ellipse, the ellipse which is the image of the unit circle under <span class="math inline">\(A\)</span>.</p>
<p>Let’s extend this idea to 3-dimensional space to get a better idea of what’s going on. Consider this transformation:</p>
<p><span class="math display">\[A = \begin{bmatrix}
\frac{3}{2} \, \sqrt{2} &amp; -\sqrt{2} &amp; 0 \\
\frac{3}{2} \, \sqrt{2} &amp; \sqrt{2} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>This will have the effect of transforming the unit sphere into an <a href="https://en.wikipedia.org/wiki/Ellipsoid">ellipsoid</a>:</p>
<figure>
<img src="../images/transform3d-0.png" title="transform3d-0" alt="The unit sphere transformed into an ellipsoid." width="800" /><figcaption>The unit sphere transformed into an ellipsoid.</figcaption>
</figure>
<p>Its singular values are 3, 2, and 1. You can see how they again form the semi-axes of the resulting figure.</p>
<figure>
<img src="../images/transform3d-1.png" title="transform3d-1" alt="The singular vectors as semi-axes in the ellipsoid." width="800" /><figcaption>The singular vectors as semi-axes in the ellipsoid.</figcaption>
</figure>
<h1 id="matrix-approximation-with-svd">Matrix Approximation with SVD</h1>
<p>Now, the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) will tell us what <span class="math inline">\(A\)</span>’s singular values are:</p>
<p><span class="math display">\[ A = U \Sigma V^* = 
\begin{bmatrix}
\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} &amp; 0.0 \\
\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} &amp; 0.0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
3 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>The diagonal entries of the matrix <span class="math inline">\(\Sigma\)</span> are the singular values of <span class="math inline">\(A\)</span>. We can obtain a lower-dimensional approximation to <span class="math inline">\(A\)</span> by setting one or more of its singular values to 0.</p>
<p>For instance, say we set the largest singular value, 3, to 0. We then get this matrix:</p>
<p><span class="math display">\[ A_1 = \begin{bmatrix}
\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} &amp; 0.0 \\
\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} &amp; 0.0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} = \begin{bmatrix}
0 &amp; -\frac{\sqrt{2}}{2} &amp; 0 \\
0 &amp; \frac{\sqrt{2}}{2} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>which transforms the unit sphere like this:</p>
<figure>
<img src="../images/ellipse-2.png" title="ellipse-2" alt="The transformation with the largest singular value set to 0." width="400" /><figcaption>The transformation with the largest singular value set to 0.</figcaption>
</figure>
<p>The resulting figure now lives in a 2-dimensional space. Further, the largest singular value of <span class="math inline">\(A_1\)</span> is now 2. Set it to 0:</p>
<p><span class="math display">\[ A_2 = \begin{bmatrix}
\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} &amp; 0.0 \\
\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} &amp; 0.0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} = \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>And we get a 1-dimensional figure, and a final largest singular value of 1:</p>
<figure>
<img src="../images/ellipse-1.png" title="ellipse-1" alt="The transformation with the two largest singular values set to 0." width="400" /><figcaption>The transformation with the two largest singular values set to 0.</figcaption>
</figure>
<p>This is the point: Each set of singular vectors will form an <a href="https://en.wikipedia.org/wiki/Orthonormal_basis">orthonormal basis</a> for some <a href="https://en.wikipedia.org/wiki/Linear_subspace">linear subspace</a> of <span class="math inline">\(\mathbb{R}^n\)</span>. A singular value and its singular vectors give the direction of maximum action among all directions orthogonal to the singular vectors of any larger singular value.</p>
<p>This has important applications. There are many problems in statistics and machine learning that come down to finding a <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">low-rank approximation</a> to some matrix at hand. <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a> is a problem of this kind. It says: approximate some matrix <span class="math inline">\(X\)</span> of observations with a number of its uncorrelated components of maximum variance. This problem is solved by computing its singular value decomposition and setting some of its smallest singular values to 0.</p>
<figure>
<img src="../images/approximations.png" title="approximations" alt="Low-rank approximations of A." width="1000" /><figcaption>Low-rank approximations of <span class="math inline">\(A\)</span>.</figcaption>
</figure>
  </section>
  
</article>


        <!-- Post Footer -->
        <div>

        </div>

    
  </div>
  
  <div class="rounded-2 box-shadow-medium px-4 pb-4 mb-4 bg-white">

        <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../posts/visualizing-linear-transformations/">Visualizing Linear Transformations</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../tags/tutorial/">tutorial</a>, <a href="../tags/linear-algebra/">linear-algebra</a>, <a href="../tags/matrix-decomposition/">matrix-decomposition</a>, <a href="../tags/geometry/">geometry</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: November 12, 2019
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#introduction">Introduction</a></li>
<li><a href="#three-primitive-transformations">Three Primitive Transformations</a><ul class="incremental">
<li><a href="#scaling">Scaling</a></li>
<li><a href="#rotation">Rotation</a></li>
<li><a href="#reflection">Reflection</a></li>
</ul></li>
<li><a href="#decomposing-matricies-into-primitives">Decomposing Matricies into Primitives</a><ul class="incremental">
<li><a href="#example">Example</a></li>
<li><a href="#example-1">Example</a></li>
</ul></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <h1 id="introduction">Introduction</h1>
<p>Say <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are <a href="https://en.wikipedia.org/wiki/Vector_space">vector spaces</a> with scalars in some <a href="https://en.wikipedia.org/wiki/Field_(mathematics)">field</a> <span class="math inline">\(\mathbb{F}\)</span> (the real numbers, maybe). A <strong><a href="https://en.wikipedia.org/wiki/Linear_map">linear map</a></strong> is a function <span class="math inline">\(T : V \rightarrow W \)</span> satisfying two conditions:</p>
<ul>
<li><strong>additivity</strong> <span class="math inline">\(T(x + y) = T x + T y\)</span> for all <span class="math inline">\(x, y \in V\)</span></li>
<li><strong>homogeneity</strong> <span class="math inline">\(T(c x) = c (T x)\)</span> for all <span class="math inline">\(c \in \mathbb{F} \)</span> and all <span class="math inline">\(x \in V\)</span></li>
</ul>
<p>

<p>Defining a linear map this way just ensures that anything that acts like a vector in <span class="math inline">\(V\)</span> also acts like a vector in <span class="math inline">\(W\)</span> after you map it over. It means that the map preserves all the structure of a vector space after it’s applied.</p>
<p>It’s a simple definition – which is good – but doesn’t speak much to the imagination. Since linear algebra is possibly the <a href="https://math.stackexchange.com/questions/256682/why-study-linear-algebra">most useful</a> and <a href="https://math.stackexchange.com/questions/256682/why-study-linear-algebra">most ubiquitous</a> of all the branches of mathematics, we’d like to have some intuition about what linear maps are so we have some idea of what we’re doing <a href="https://en.wikipedia.org/wiki/Linear_regression">when</a> <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">we</a> <a href="https://en.wikipedia.org/wiki/Backpropagation">use</a> <a href="https://en.wikipedia.org/wiki/Mapreduce">it</a>. Though not all vectors live there, the <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean plane</a> <span class="math inline">\(\mathbb{R}^2\)</span> is certainly the easiest to visualize, and the way we <a href="https://en.wikipedia.org/wiki/Euclidean_distance">measure distance</a> there is very similar to the way we <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">measure error</a> in statistics, so we can feel that our intuitions will carry over.</p>
<p>It turns out that all linear maps in <span class="math inline">\(\mathbb{R}^2\)</span> can be factored into just a few primitive geometric operations: <a href="https://en.wikipedia.org/wiki/Scaling_(geometry)">scaling</a>, <a href="https://en.wikipedia.org/wiki/Rotation_(mathematics)">rotation</a>, and <a href="https://en.wikipedia.org/wiki/Reflection_(mathematics)">reflection</a>. This isn’t the only way to factor these maps, but I think it’s the easiest to understand. (We could get by <a href="https://en.wikipedia.org/wiki/Cartan%E2%80%93Dieudonn%C3%A9_theorem">without rotations</a>, in fact.)</p>
<figure>
<img src="../images/primitives.png" title="primitives" alt="The unit circle, rotated, reflected, and scaled." /><figcaption>The unit circle, rotated, reflected, and scaled.</figcaption>
</figure>
<h1 id="three-primitive-transformations">Three Primitive Transformations</h1>
<h2 id="scaling">Scaling</h2>
<p>A (non-uniform) <strong>scaling transformation</strong> <span class="math inline">\(D\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span> is given by a <a href="https://en.wikipedia.org/wiki/Diagonal_matrix">diagonal matrix</a>:</p>
<p><span class="math display">\[Scl(d1, d2) = \begin{bmatrix}
d_1 &amp; 0   \\
0   &amp; d_2 \\
\end{bmatrix}\]</span></p>
<p>where <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> are non-negative. The transformation has the effect of stretching or shrinking a vector along each coordinate axis, and, so long as <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> are positive, it will also preserve the <a href="https://en.wikipedia.org/wiki/Orientation_(vector_space)">orientation</a> of vectors after mapping because in this case <span class="math inline">\(\det(D) = d_1 d_2 &gt; 0\)</span>.</p>
<p>For instance, here is the effect on a vector of this matrix: <span class="math display">\[D = \begin{bmatrix}
0.75 &amp; 0 \\
0    &amp; 1.25 \\
\end{bmatrix}\]</span></p>
<figure>
<img src="../images/vector-scaled.png" title="vector-scaled" alt="A vector, scaled." width="400" /><figcaption>A vector, scaled.</figcaption>
</figure>
<p>It will shrink a vector by a factor of 0.75 along the x-axis and stretch a vector by a factor of 1.25 along the y-axis.</p>
<p>If we think about all the vectors of length 1 as being the points of the <a href="https://en.wikipedia.org/wiki/Unit_circle">unit circle</a>, then we can get an idea of how the transformation will affect any vector. We can see a scaling as a continous transformation beginning at the <a href="https://en.wikipedia.org/wiki/Identity_matrix">identity matrix</a>.</p>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/scaling.webm" type="video/webm">
  <source src="../../images/scaling.mp4" type="video/mp4">
  <source src="../../images/scaling.ogg" type="video/ogg">
</video>

<p>If one of the diagonal entries is 0, then it will collapse the circle on the other axis.</p>
<p><span class="math display">\[D = \begin{bmatrix}
0 &amp; 0 \\
0 &amp; 1.25 \\
\end{bmatrix}\]</span></p>
<p>This is an example of a <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank-deficient</a> matrix. It maps every vector onto the y-axis, and so its image has a dimension less than the dimension of the full space.</p>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/collapsed.webm" type="video/webm">
  <source src="../../images/collapsed.mp4" type="video/mp4">
  <source src="../../images/collapsed.ogg" type="video/ogg">
</video>

<h2 id="rotation">Rotation</h2>
<p>A <strong>rotation transformation</strong> <span class="math inline">\(Ref\)</span> is given by a matrix: <span class="math display">\[Ref(\theta) = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta) \\
\sin(\theta) &amp; \cos(\theta) \\
\end{bmatrix}\]</span></p>
<p>This transformation will have the effect of rotating a vector counter-clockwise by an angle <span class="math inline">\(\theta\)</span>, when <span class="math inline">\(\theta\)</span> is positive, and clockwise by <span class="math inline">\(\theta\)</span> when <span class="math inline">\(\theta\)</span> is negative.</p>
<figure>
<img src="../images/vector-rotated.png" title="vector-rotated" alt="A vector, rotated by 3\pi/4" width="400" /><figcaption>A vector, rotated by <span class="math inline">\(3\pi/4\)</span></figcaption>
</figure>
<p>And the unit circle gets mapped onto itself.</p>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/rotation.webm" type="video/webm">
  <source src="../../images/rotation.mp4" type="video/mp4">
  <source src="../../images/rotation.ogg" type="video/ogg">
</video>

<p>It shouldn’t be too hard to convince ourselves that the matrix we’ve written down is the one we want. Take some unit vector and write its coordinates like <span class="math inline">\((\cos\gamma, \sin\gamma)\)</span>. Multiply it by <span class="math inline">\(Ref(\theta)\)</span> to get <span class="math inline">\((\cos\gamma \cos\theta - \sin\gamma \sin\theta, \cos\gamma \sin\theta + \sin\gamma \cos\theta)\)</span>. But by a <a href="https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Angle_sum_and_difference_identities">trigonometric identity</a>, this is exactly the vector <span class="math inline">\((\cos(\gamma + \theta), \sin(\gamma + \theta))\)</span>, which is our vector rotated by <span class="math inline">\(\theta\)</span>.</p>
<p>A rotation should preserve not only orientations, but also distances. Now, recall that the determinant for a <span class="math inline">\(2\times 2\)</span> matrix <span class="math inline">\(\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span> is <span class="math inline">\(a d - b c\)</span>. So a rotation matrix will have determinant <span class="math inline">\(\cos^2(\theta) + \sin^2(\theta)\)</span>, which, by the <a href="https://en.wikipedia.org/wiki/Pythagorean_trigonometric_identity">Pythagorean identity</a>, is equal to 1. This, together with the fact that its columns are <a href="https://en.wikipedia.org/wiki/Orthonormality">orthonormal</a> means that it does preserve both. It is a kind of <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a>, which is a kind of <a href="https://en.wikipedia.org/wiki/Isometry">isometry</a>.</p>
<h2 id="reflection">Reflection</h2>
<p>A <strong>reflection</strong> in <span class="math inline">\(\mathbb{R}^2\)</span> can be described with matricies like: <span class="math display">\[Ref(\theta) = \begin{bmatrix}
\cos(2\theta) &amp; \sin(2\theta) \\
\sin(2\theta) &amp; -\cos(2\theta) \\
\end{bmatrix}\]</span> where the reflection is through a line crossing the origin and forming an angle <span class="math inline">\(\theta\)</span> with the x-axis.</p>
<figure>
<img src="../images/vector-reflected.png" title="vector-reflected" alt="A vector, reflected over a line at angle \pi/4." width="400" /><figcaption>A vector, reflected over a line at angle <span class="math inline">\(\pi/4\)</span>.</figcaption>
</figure>
<p>And the unit circle gets mapped onto itself.</p>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/reflection.webm" type="video/webm">
  <source src="../../images/reflection.mp4" type="video/mp4">
  <source src="../../images/reflection.ogg" type="video/ogg">
</video>

<p>Note that the determinant of this matrix is -1, which means that it <em>reverses</em> orientation. But its columns are still orthonormal, and so it too is an isometry.</p>
<h1 id="decomposing-matricies-into-primitives">Decomposing Matricies into Primitives</h1>
<p>The <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) will factor any matrix <span class="math inline">\(A\)</span> having like this:</p>
<p><span class="math display">\[ A = U \Sigma V^* \]</span></p>
<p>We are working with real matricies, so <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> will both be orthogonal matrices. This means each of these will be either a reflection or a rotation, depending on the pattern of signs in its entries. The matrix <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with non-negative entries, which means that it is a scaling transform. (The <span class="math inline">\(*\)</span> on the <span class="math inline">\(V\)</span> is the <a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate-transpose</a> operator, which just means ordinary <a href="https://en.wikipedia.org/wiki/Transpose">transpose</a> when <span class="math inline">\(V\)</span> doesn’t contain any imaginary entries. So, for us, <span class="math inline">\(V^* = V^\top\)</span>.) Now with the SVD we can rewrite any linear transformation as:</p>
<ol>
<li><span class="math inline">\(V^*\)</span>: Rotate/Reflect</li>
<li><span class="math inline">\(\Sigma\)</span>: Scale</li>
<li><span class="math inline">\(U\)</span>: Rotate/Reflect</li>
</ol>
<h2 id="example">Example</h2>
<p><span class="math display">\[\begin{bmatrix}
0.5 &amp; 1.5 \\
1.5 &amp; 0.5
\end{bmatrix} \approx \begin{bmatrix}
-0.707 &amp; -0.707 \\
-0.707 &amp; 0.707
\end{bmatrix} \begin{bmatrix}
2.0 &amp; 0.0 \\
0.0 &amp; 1.0
\end{bmatrix} \begin{bmatrix}
-0.707 &amp; -0.707 \\
0.707 &amp; -0.707
\end{bmatrix} \]</span></p>
<p>This turns out to be:</p>
<ol>
<li><span class="math inline">\(V^*\)</span>: Rotate clockwise by <span class="math inline">\(\theta = \frac{3 \pi}{4}\)</span>.</li>
<li><span class="math inline">\(\Sigma\)</span>: Scale x-coordinate by <span class="math inline">\(d_1 = 2\)</span> and y-coordinate by <span class="math inline">\(d_2 = 1\)</span>.</li>
<li><span class="math inline">\(U\)</span>: Reflect over the line with angle <span class="math inline">\(-\frac{3\pi}{8}\)</span>.</li>
</ol>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/rot-scale-ref.webm" type="video/webm">
  <source src="../../images/rot-scale-ref.mp4" type="video/mp4">
  <source src="../../images/rot-scale-ref.ogg" type="video/ogg">
</video>

<h2 id="example-1">Example</h2>
<p>And here is a <a href="https://en.wikipedia.org/wiki/Shear_mapping">shear transform</a>, represented as: rotation, scale, rotation.</p>
<span class="math display">\[\begin{bmatrix}
1.0 &amp; 1.0 \\
0.0 &amp; 1.0
\end{bmatrix} \approx \begin{bmatrix}
0.85 &amp; -0.53 \\
0.53 &amp; 0.85
\end{bmatrix} \begin{bmatrix}
1.62 &amp; 0.0 \\
0.0 &amp; 0.62
\end{bmatrix} \begin{bmatrix}
0.53 &amp; 0.85 \\
-0.85 &amp; 0.53
\end{bmatrix}
\]</span>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/shear.webm" type="video/webm">
  <source src="../../images/shear.mp4" type="video/mp4">
  <source src="../../images/shear.ogg" type="video/ogg">
</video>

  </section>
  
</article>


        <!-- Post Footer -->
        <div>

        </div>

    
  </div>
  
  <div class="rounded-2 box-shadow-medium px-4 pb-4 mb-4 bg-white">

        <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../posts/bayes-and-means/">What I'm Reading 1: Bayes and Means</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../tags/R/">R</a>, <a href="../tags/bayesian/">bayesian</a>, <a href="../tags/data-science/">data-science</a>, <a href="../tags/stacking/">stacking</a>, <a href="../tags/BMA/">BMA</a>, <a href="../tags/review/">review</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: October  4, 2019
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#bayesian-aggregation">Bayesian Aggregation</a></li>
<li><a href="#bayesian-stacking">Bayesian Stacking</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <h1 id="bayesian-aggregation">Bayesian Aggregation</h1>
<p>Yang, Y., &amp; Dunson, D. B., <em>Minimax Optimal Bayesian Aggregation</em> 2014 (<a href="https://arxiv.org/abs/1403.1345">arXiv</a>)</p>
<p>Say we have a number of estimators <span class="math inline">\(\hat f_1, \ldots, \hat f_K\)</span> derived from a number of models <span class="math inline">\(M_1, \ldots, M_K\)</span> for some regression problem <span class="math inline">\(Y = f(X) + \epsilon\)</span>, but, as is the nature of things when estimating with limited data, we don’t know which estimator represents the true model (assuming the true model is in our list). The Bayesian habit is to stick a prior on the uncertainty, compute posteriors probabilities, and then average across the unknown parameter using the posterior probabilities as weights. Since the posterior probabilities (call them <span class="math inline">\(\lambda_1, \ldots, \lambda_K\)</span>) have to sum to 1, we obtain a <em>convex combination</em> of our estimators <span class="math display">\[ \hat f = \sum_{1\leq i \leq K} \lambda_i \hat f_i \]</span> This is the approach of <a href="https://www.stat.colostate.edu/~jah/papers/statsci.pdf">Bayesian Model Averaging</a> (BMA). Yang <em>et al.</em> propose to find such combinations using a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet prior</a> on the weights <span class="math inline">\(\lambda_i\)</span>. If we remove the restriction that the weights sum to 1 and instead only ask that they have finite sum in absolute value, then we obtain <span class="math inline">\(\hat f\)</span> as a <em>linear combination</em> of <span class="math inline">\(\hat f_i\)</span>. The authors then place a Gamma prior on <span class="math inline">\(A = \sum_i |\lambda_i|\)</span> and a Dirichlet prior on <span class="math inline">\(\mu_i = \frac{|\lambda_i|}{A}\)</span>. In both the linear and the convex cases they show that the resulting estimator is minimax optimal in the sense that it will give the best worst-case predictions for a given number of observations, including the case where a sparsity restriction is placed on the number of estimators <span class="math inline">\(\hat f_i\)</span>; in other words, <span class="math inline">\(\hat f\)</span> converges to the true estimator as the number of observations increases with minimax optimal risk. The advantage to previous non-bayesian methods of linear or convex aggregation is that the sparsity parameter can be learned from the data. The Dirichlet convex combination gives good performance against Best Model selection, Majority Voting, and <a href="https://biostats.bepress.com/ucbbiostat/paper266/">SuperLearner</a>, especially when there are both a large number of observations and a large number of estimators.</p>
<p>I implemented the convex case in R for use with <a href="https://github.com/paul-buerkner/brms">brms</a>. The Dirichlet distribution has been <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Gamma_distribution">reparameterized</a> as a sum of Gamma RVs to aid in sampling. The Dirichlet concentration parameter is <span class="math inline">\(\frac{\alpha}{K^\gamma}\)</span>; the authors recommend choosing <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\gamma = 2\)</span>.</p>
<pre class="r" data-org-language="R"><code>convex_regression &lt;- function(formula, data,
                              family = &quot;gaussian&quot;,
                              ## Yang (2014) recommends alpha = 1, gamma = 2
                              alpha = 1, gamma = 2,
                              verbose = 0,
                              ...) {
  if (gamma &lt;= 1) {
    warning(paste(&quot;Parameter gamma should be greater than 1. Given:&quot;, gamma))
  }
  if (alpha &lt;= 0) {
    warning(paste(&quot;Parameter alpha should be greater than 0. Given:&quot;, alpha))
  }
  ## Set up priors.
  K &lt;- length(terms(formula))
  alpha_K &lt;- alpha / (K^gamma)
  stanvars &lt;-
    stanvar(alpha_K,
      &quot;alpha_K&quot;,
      block = &quot;data&quot;,
      scode = &quot;  real&lt;lower = 0&gt; alpha_K;  // dirichlet parameter&quot;
    ) +
    stanvar(
      name = &quot;b_raw&quot;,
      block = &quot;parameters&quot;,
      scode = &quot;  vector&lt;lower = 0&gt;[K] b_raw; &quot;
    ) +
    stanvar(
      name = &quot;b&quot;,
      block = &quot;tparameters&quot;,
      scode = &quot;  vector[K] b = b_raw / sum(b_raw);&quot;
    )
  prior &lt;- prior(&quot;target += gamma_lpdf(b_raw | alpha_K, 1)&quot;,
    class = &quot;b_raw&quot;, check = FALSE
  )
  f &lt;- update.formula(formula, . ~ . - 1)
  if (verbose &gt; 0) {
    make_stancode(f,
      prior = prior,
      data = data,
      stanvars = stanvars
    ) %&gt;% message()
  }
  fit_dir &lt;- brm(f,
    prior = prior,
    family = family,
    data = data,
    stanvars = stanvars,
    ...
  )
  fit_dir
}
</code></pre>
<p>Here is a <a href="https://gist.github.com/ryanholbrook/b5c7d44c0c7642eeee1a3034b48f29d7">gist</a> that includes an interface to <a href="https://tidymodels.github.io/parsnip/">parsnip</a>.</p>
<p>In my own experiments, I found the performance of the convex aggregator to be comparable to a <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a> SuperLearner at the cost of the lengthier training that goes with MCMC methods and the finicky convergence of sparse priors. So I would likely reserve this for when I had lots of features and lots of estimators to work through, where I presume it would show an advantage. But in that case it would definitely be on my list of things to try.</p>
<h1 id="bayesian-stacking">Bayesian Stacking</h1>
<p>Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A., <em>Using Stacking to Average Bayesian Predictive Distributions</em> (<a href="https://projecteuclid.org/euclid.ba/1516093227">pdf</a>)</p>
<p>Another approach to model combination is <a href="https://doi.org/10.1080/01621459.1996.10476733">stacking</a>. With stacking, model weights are chosen by cross-validation to minimize <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSE</a> predictive error. Now, BMA finds the aggregated model that best fits the data, while stacking finds the aggregated model that gives the best predictions. Stacking therefore is usually better when predictions are what you want. A drawback is that stacking produces models through <em>point</em> estimates. So, they don’t give you all the information of a full distribution like BMA would. Yao <em>et al.</em> propose a method of stacking that instead finds the optimal <a href="https://en.wikipedia.org/wiki/Posterior_predictive_distribution">predictive distribution</a> by convex combinations of distributions with weights chosen by some scoring rule: the authors use the minimization of KL-divergence. Hence, they choose weights <span class="math inline">\(w\)</span> empirically through <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation">LOO</a> by <span class="math display">\[ \max_w \frac{1}{n} \sum_{1\leq i \leq n} \log \sum_{1\leq k \leq K} w_k p(y_i | y_{-i}, M_k) \]</span> where <span class="math inline">\(y_1, \ldots, y_n\)</span> are the observed data and <span class="math inline">\(y_{-i}\)</span> is the data with <span class="math inline">\(y_i\)</span> left out. The following figure shows how stacking of predictive distributions gives the “best of both worlds” for BMA and point prediction stacking.</p>
<figure>
<img src="../images/stacking.png" alt="From Yao (2018)" /><figcaption>From Yao (2018)</figcaption>
</figure>
<p>They have implemented stacking for <a href="https://mc-stan.org/users/interfaces/rstan">Stan</a> models in the R package <a href="https://cran.r-project.org/web/packages/loo/vignettes/loo2-weights.html">loo</a>.</p>
  </section>
  
</article>


        <!-- Post Footer -->
        <div>

        </div>

    
  </div>
  

<!-- Pagination -->
<nav class="paginate-container" aria-label="Pagination">
  <div class="pagination">
    
    <a class="previous_page text-gray-dark" rel="older" aria-label="Older Posts" href="../3/">⮜ Older</a>
    

    
    <a class="next_page text-gray-dark" rel="newer" aria-label="Newer Posts" href="../">Newer ⮞</a>
    
  </div>
</nav>

  
</div>


          </div>
          
          <div id="right" class="ml-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Tags</h1>
                <a style="font-size: 105%" href="../tags/bayesian/">bayesian</a> <a style="font-size: 100%" href="../tags/BMA/">BMA</a> <a style="font-size: 100%" href="../tags/calculator/">calculator</a> <a style="font-size: 100%" href="../tags/category-theory/">category-theory</a> <a style="font-size: 105%" href="../tags/classification/">classification</a> <a style="font-size: 100%" href="../tags/coordinates/">coordinates</a> <a style="font-size: 100%" href="../tags/covectors/">covectors</a> <a style="font-size: 100%" href="../tags/cql/">cql</a> <a style="font-size: 115%" href="../tags/data-science/">data-science</a> <a style="font-size: 105%" href="../tags/decision-boundaries/">decision-boundaries</a> <a style="font-size: 100%" href="../tags/eigenvalues/">eigenvalues</a> <a style="font-size: 100%" href="../tags/engrams/">engrams</a> <a style="font-size: 105%" href="../tags/finance/">finance</a> <a style="font-size: 100%" href="../tags/functional-programming/">functional-programming</a> <a style="font-size: 100%" href="../tags/generalized-inverse/">generalized-inverse</a> <a style="font-size: 100%" href="../tags/geometry/">geometry</a> <a style="font-size: 100%" href="../tags/haskell/">haskell</a> <a style="font-size: 100%" href="../tags/investing/">investing</a> <a style="font-size: 100%" href="../tags/julia/">julia</a> <a style="font-size: 100%" href="../tags/LDA/">LDA</a> <a style="font-size: 100%" href="../tags/least-squares/">least-squares</a> <a style="font-size: 115%" href="../tags/linear-algebra/">linear-algebra</a> <a style="font-size: 100%" href="../tags/linear-equations/">linear-equations</a> <a style="font-size: 100%" href="../tags/matrix-decomposition/">matrix-decomposition</a> <a style="font-size: 100%" href="../tags/MCMC/">MCMC</a> <a style="font-size: 100%" href="../tags/memory/">memory</a> <a style="font-size: 100%" href="../tags/moore-penrose-inverse/">moore-penrose-inverse</a> <a style="font-size: 100%" href="../tags/neural-networks/">neural-networks</a> <a style="font-size: 100%" href="../tags/neuroscience/">neuroscience</a> <a style="font-size: 100%" href="../tags/NLP/">NLP</a> <a style="font-size: 100%" href="../tags/numpy/">numpy</a> <a style="font-size: 100%" href="../tags/python/">python</a> <a style="font-size: 100%" href="../tags/QDA/">QDA</a> <a style="font-size: 110%" href="../tags/R/">R</a> <a style="font-size: 100%" href="../tags/ReLUs/">ReLUs</a> <a style="font-size: 100%" href="../tags/retirement/">retirement</a> <a style="font-size: 100%" href="../tags/review/">review</a> <a style="font-size: 100%" href="../tags/sage/">sage</a> <a style="font-size: 100%" href="../tags/simulation/">simulation</a> <a style="font-size: 100%" href="../tags/singular-values/">singular-values</a> <a style="font-size: 100%" href="../tags/stacking/">stacking</a> <a style="font-size: 100%" href="../tags/talk/">talk</a> <a style="font-size: 100%" href="../tags/tensors/">tensors</a> <a style="font-size: 110%" href="../tags/tutorial/">tutorial</a> <a style="font-size: 100%" href="../tags/vectors/">vectors</a>
              </div>
              <div>
                <h1>Links</h1>
                <a href="https://www.r-bloggers.com/">R-bloggers</a>
              </div>
            </div>
          </div>
        </div>

        <footer class="border p-3 bg-white text-gray">
  Site proudly generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
</footer>

        
      </div>
  </body>
</html>
