
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Math for Machines</title>
    <link href="https://fonts.googleapis.com/css?family=Merriweather:400&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro&display=swap" rel="stylesheet"> 
    <link href="https://unpkg.com/primer/build/build.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/style.css" />
      <!-- Syntax highlighting -->
  <link rel="stylesheet" href="../css/github.css" />
  <script src="../scripts/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class="bg-gray-dark">
    <div class="container-xxxl h-100 my-3">
      <div class="box-shadow-large bg-gray">

        <header id="header" class="bg-white">

  <div class="p-4" style="text-align: center">

    <a class="title" href="../">
      Math for Machines
    </a>

  </div>
  
  <nav class="UnderlineNav UnderlineNav--right px-2 border-top">
    <div class="UnderlineNav-body">
      <a class="UnderlineNav-item " href="../about/">
        <span>About</span>
      </a>
      <a class="UnderlineNav-item " href="../archive/">
        <span>Archive</span>
      </a>
    </div>
  </nav>


  <!-- Scripts -->
  <!-- Enable MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic:true } },
    });
    MathJax.Hub.Config({
        TeX: { extensions: ["color.js"] }
    });
    MathJax.Hub.Config({
        TeX: {
            Macros: {
                formbox: ["\\bbox[15px, border:1px solid Gray]{#1}", 1],
            }
        }
    });
  </script>
  <script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133546767-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133546767-1', { 'optimize_id': 'GTM-T3XD3JM'});
  </script>

</header>

        
        <div id="holy">

          <div id="left" class="mr-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Recent Posts</h1>
                <ul>
    
        <li>
          <a href="../posts/discriminant-analysis/">Six Varieties of Gaussian Discriminant Analysis</a>
        </li>
    
        <li>
          <a href="../posts/decision/">Optimal Decision Boundaries</a>
        </li>
    
        <li>
          <a href="../posts/least-squares-with-the-mp-inverse/">Least Squares with the Moore-Penrose Inverse</a>
        </li>
    
        <li>
          <a href="../posts/eigenvalues-and-singular-values/">Understanding Eigenvalues and Singular Values</a>
        </li>
    
        <li>
          <a href="../posts/visualizing-linear-transformations/">Visualizing Linear Transformations</a>
        </li>
    
        <li>
          <a href="../posts/bayes-and-means/">What I'm Reading 1: Bayes and Means</a>
        </li>
    
        <li>
          <a href="../posts/investmentsim/">investmentsim - an R Package for Simulating Investment Portfolios</a>
        </li>
    
        <li>
          <a href="../posts/introduction-to-categories/">Talk: An Introduction to Categories with Haskell and Databases</a>
        </li>
    
        <li>
          <a href="../posts/retirement-formula/">A Somewhat Better Retirement Formula</a>
        </li>
    
        <li>
          <a href="../posts/permitted-and-forbidden-sets/">Journal Review: Permitted and Forbidden Sets in STLNs</a>
        </li>
    
</ul>

              </div>
            </div>
          </div>
            
          <div class="my-2">
          <div role="main" id="main">
  
  
  <div class="rounded-2 box-shadow-medium px-4 pb-4 mb-4 bg-white">

        <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../posts/permitted-and-forbidden-sets/">Journal Review: Permitted and Forbidden Sets in STLNs</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../tags/julia/">julia</a>, <a href="../tags/engrams/">engrams</a>, <a href="../tags/neuroscience/">neuroscience</a>, <a href="../tags/memory/">memory</a>, <a href="../tags/ReLUs/">ReLUs</a>, <a href="../tags/neural-networks/">neural-networks</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: April  8, 2019
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#a-model-of-associative-memory">A Model of Associative Memory</a></li>
<li><a href="#symmetric-threshold-linear-networks-a.k.a.-symmetric-rectifier-networks">Symmetric Threshold-Linear Networks (a.k.a. Symmetric Rectifier Networks)</a></li>
<li><a href="#three-theorems">Three Theorems</a><ul class="incremental">
<li><a href="#theorem-1---steady-states">Theorem 1 - Steady States</a></li>
<li><a href="#theorems-2-and-3---permitted-and-forbidden-sets">Theorems 2 and 3 - Permitted and Forbidden Sets</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
<li><a href="#appendix---computing-permitted-sets-in-julia">Appendix - Computing Permitted Sets in Julia</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <h1 id="a-model-of-associative-memory">A Model of Associative Memory</h1>
<p>How memories are encoded in neural matter is still an open question. The name for these supposed neural correlates of memory is “engram”, and papers about engrams tend to have titles like <a href="https://jflab.ca/pdfs/josselyn-et-al-2015.pdf"><em>Finding the engram</em></a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3462696/"><em>Catching the engram</em></a>, <a href="https://psycnet.apa.org/record/1952-05966-020"><em>In search of the engram</em></a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2895151/"><em>Continuing the search for the engram</em></a>, which, though I’m not an expert, makes me feel like the problem isn’t well understood.</p>
<p>(Also, <a href="https://www.ncbi.nlm.nih.gov/pubmed/15450162"><em>Rite of passage of the engram</em></a> and <a href="http://www.jneurosci.org/content/34/42/14115"><em>Manipulating a cocaine engram</em></a>, making the practice of neuroscience sometimes sound like a fraternity hazing. Possibly related, while researching this post I learned that a typical experiment will usually involve things like shocking the feet of a fruit fly, sewing shut one eye of a barn owl, and shaving half the whiskers off a mouse.)</p>
<p>A popular theory is that memories are encoded as patterns of synaptic connections. Perception creates neural activity. Neural activity leaves an impression upon the brain as a pattern of modified synaptic connections (perhaps by <a href="https://en.wikipedia.org/wiki/Dendritic_spine#Importance_to_learning_and_memory">dendritic spines</a>, which become larger and more numerous to make the connection stronger). A later perception might partly activate this pattern, but this partial activation is often enough to activate the rest of the pattern, too. This is supposed to be a neural model of associative memory. (The tradition is to cite <a href="https://en.wikipedia.org/wiki/In_Search_of_Lost_Time#Memory">Proust</a> at this point; evidently, a <a href="https://en.wikipedia.org/wiki/Madeleine_(cake)">sponge cake</a> was sufficient to activate in him the neural substrate of a <a href="https://en.wikipedia.org/wiki/List_of_longest_novels">1,267,069</a> word novel. It’s remarkably illustrative, at least.)</p>
<p>Artificial neural networks are often used to model the networks of the brain. <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feedforward networks</a> have typically been used to model the visual system, while <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent networks</a> have more often been used to model memory. When an input is applied to certain of these recurrent networks, the neural activity will always converge to a stable <a href="https://en.wikipedia.org/wiki/Steady_state">steady state</a>. This stable pattern of activity is supposed to be a memory, stored within the connections of the network.</p>
<p>Some of the most studied networks are those that are <em>symmetrically</em> connected, like the <a href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield network</a>. A network is symmetrically connected if every neuron is connected with the same weight as whatever is connected to it. A symmetrically connected network with a <em>linear</em> <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> can, for a given set of connection weights, be activated only to a <em>single</em> stable steady state (whose values depend upon the input to the network). The drawback of these networks then is that the activity at future states will be independent of the activity at past states. Past recall cannot influence future recall.</p>
<p><a href="https://papers.nips.cc/paper/1793-permitted-and-forbidden-sets-in-symmetric-threshold-linear-networks.pdf">Hahnloser and Seung</a> present a model of associative memory in symmetrically connected networks using instead a <em>threshold-linear</em> activation function (or <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear</a> function).</p>
<figure class="floatright"> <img src="../images/rectifier.png" alt="Graph of a rectified linear activation function." /> </figure>

<p>They show that, due to some nice properties of the rectifier, such networks can in general represent multiple patterns of stable activation even for a single input. What pattern the network will fall into upon new input, depends upon what pattern it was in before. Memories linger.</p>
<p>Their main contribution in this paper is in classifying neurons into what they call “permitted” and “forbidden” sets, which describe what sets of neurons may be activated together in a stable steady-state. They describe a method of determining what patterns of stable activity the network can achieve.</p>
<blockquote>
<p>The existence of permitted and forbidden sets suggests a new way of thinking about memory in neural networks. When an input is applied, the network must select a set of active neurons, and this selection is constrained to be one of the permitted sets. Therefore the permitted sets can be regarded as memories stored in the synaptic connections.</p>
</blockquote>
<h1 id="symmetric-threshold-linear-networks-a.k.a.-symmetric-rectifier-networks">Symmetric Threshold-Linear Networks (a.k.a. Symmetric Rectifier Networks)</h1>
<p>A threshold-linear network has the form <span class="math display">\[
\dot{x} = -x + \bigg[W x + b \bigg]_+ \tag 1 \label 1
\]</span> where <span class="math inline">\(x\)</span> is a vector with <span class="math inline">\(n\)</span> components representing neural activation, <span class="math inline">\(W\)</span> an <span class="math inline">\(n \times n\)</span> matrix representing the connection weights between neurons, <span class="math inline">\(b\)</span> is a vector representing (constant) external input, and <span class="math inline">\([\cdot]_+ = \operatorname{max}\{0, \cdot\}\)</span>, the rectifier function. Hahnloser and Seung assume the weight matrix <span class="math inline">\(W\)</span> is symmetric (meaning, neurons are connected symmetrically).</p>
<p>For a single neuron we can write <span class="math display">\[
\dot{x}_i = -x_i + \bigg[\sum_{j=1}^n w_{ij} x_j + b_i\bigg]_+ \tag 2 \label 2
\]</span> Whenever <span class="math inline">\(\sum_{j=1}^n w_{ij} x_j + b_i \leq 0\)</span> the input to the neuron is 0. Its dynamics become <span class="math inline">\(\dot x_i = -x_i\)</span> and its activation will decay exponentially to 0; it is “off”. What this means is that generally only a subset of neurons will be active at any time, and which neurons are active may change as the system evolves.</p>
<p>It helps to think about neurons as being active within “chambers” of the activation space. (These chambers can be found by considering when the expression inside <span class="math inline">\([\cdot]_+\)</span> is equal to 0.) In each chamber, some of the neurons will be active and some will be inactive. Within each chamber, the network will evolve according to that chamber’s <em>linear</em> equations: <span class="math display">\[
\dot{x} = -x + W_\sigma x + b_\sigma \tag 3 \label 3
\]</span> (Here, <span class="math inline">\(\sigma\)</span> means the set of neurons that are currently active, and <span class="math inline">\(W_\sigma\)</span> and <span class="math inline">\(b_\sigma\)</span> have entries set to 0 for those neurons not in <span class="math inline">\(\sigma\)</span>.) Whenever the system enters a new chamber, some neurons will switch on and some will switch off, and a new set of linear equations takes over. Each chamber has a set of eigenvectors given by <span class="math inline">\(W_\sigma\)</span>. These eigenvectors show straight line flows within that chamber.</p>
<p>Let’s take a look at the dynamics of a two neuron system with weight matrix <span class="math inline">\(\begin{bmatrix}0 &amp; -\frac12 \\
-\frac12 &amp; 0\end{bmatrix}\)</span>.</p>
<p>First, the rectified version. The activation space is divided into four chambers; the labels indicate which neurons are active in that chamber. Each curve represents different initialization values for the neurons; the input vector <span class="math inline">\(b\)</span> is always the same. On the right is a plot for one initialization. In this example, the network always converges to a single steady state, though in other networks there may be more than one.</p>
<img src="../images/rectified.png" alt="graphs showing dynamics of a rectifier network">

<p>Notice how the dynamics change when the system enters in innermost chamber <span class="math inline">\(\{1,2\}\)</span>. Compare this to the same system lacking the rectifier <span class="math inline">\([\cdot]_+\)</span>; it is a linear system.</p>
<img src="../images/linear.png" alt="graphs showing dynamics of a network with linear activation" />

<h1 id="three-theorems">Three Theorems</h1>
<p>The authors prove three theorems. The first gives the conditions under which a network will have a set of global, stable steady states (aka. globally asypmtotic fixed points, equilibrium points), depending on connection weights and input. These steady states, when they exist, are fixed points of activation to which the network will always converge.</p>
<p>Assuming these conditions, in the second and third theorems the authors give two possibilities for this set of steady states. The first possibility is that the network contains <em>forbidden sets</em> of neurons, neurons that may not be activated together at a steady state; in this case the network will be <em>multistable</em>: for a given input, it may converge to one of several steady states depending on initial activations. The second possibility is that there are <em>no</em> forbidden sets; in this case, for a given input, the network will always converge to the same steady state; as far as stable points go, it is just like a linear system, without the rectifier.</p>
<h2 id="theorem-1---steady-states">Theorem 1 - Steady States</h2>
<p>Again, this theorem gives the conditions under which a network may have a set of stable steady states.</p>
<p>The authors present their results in terms of the matrix <span class="math inline">\(I-W\)</span>. We can rewrite the linear system <span class="math inline">\(\ref 3\)</span> as <span class="math display">\[ \dot x = (-I + W)x + b \tag 4 \]</span> The <a href="https://en.wikipedia.org/wiki/Hurwitz_matrix#Hurwitz_stable_matrices">stability</a> of the system can be determined from the eigenvalues of the matrix <span class="math inline">\(-I + W\)</span>; specifically, the system is <a href="https://en.wikipedia.org/wiki/Lyapunov_stability">globally asymptotically stable</a> if the real parts of the matrix are all <em>negative</em>. Since <span class="math inline">\(-I + W\)</span> is symmetric and real, its eigenvalues will all be real; so, we are looking for negative eigenvalues. It is, however, usually more convenient to work with positive numbers, so instead we can look for <em>positive</em> eigenvalues of <span class="math inline">\(I - W\)</span> (or even eigenvalues of <span class="math inline">\(W\)</span> that are less than 1).</p>
<blockquote>
<p><strong>Theorem 1</strong></p>
<p>If W is symmetric, then the following conditions are equivalent:</p>
<ol>
<li>All nonnegative eigenvectors of all principal submatrices of <span class="math inline">\(I - W\)</span> have positive eigenvalues.</li>
<li>The matrix <span class="math inline">\(I - W\)</span> is copositive. That is, <span class="math inline">\(x^\top (I - W)x \gt 0\)</span> for all nonnegative <span class="math inline">\(x\)</span>, except <span class="math inline">\(x = 0\)</span>.</li>
<li>For all <span class="math inline">\(b\)</span>, the network has a nonempty set of steady states that are globally asymptotically stable.</li>
</ol>
</blockquote>
<figure class="floatright"><img src="../images/lagrange.png" alt="plot of the Lagrange function for non-negative v on the unit circle" /><figcaption>\(R(v)\) for \(\left\lVert v \right\rVert = 1\)</figcaption></figure>

<p>One of the things I liked about this paper was that they proved their results using methods from both <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyanpunov functions</a> and <a href="https://en.wikipedia.org/wiki/Quadratic_programming">quadratic programming</a>. They prove that <span class="math inline">\((1)\)</span> implies <span class="math inline">\((2)\)</span>, for instance, by minimizing <span class="math inline">\(v^\top (I - W) v\)</span> (a quadratic function) for nonnegative vectors <span class="math inline">\(v\)</span> on the unit sphere (that is, <span class="math inline">\(\left\lVert v \right\rVert = 1\)</span>). The quantity <span class="math inline">\(R(v) = v^\top (I - W) v\)</span> is equivalent to the <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotient</a>. Optimizing <span class="math inline">\(R\)</span> will find the eigenvectors of the matrix <span class="math inline">\(I - W\)</span>. Because of the rectifier, neural activations (provided they start above 0) can never fall below 0. Any steady state therefore will occur along a non-negative eigenvector. This, I think, is one of the most important insights about the effect of the rectification.</p>
<p>Here are the authors again:</p>
<blockquote>
<p>The meaning of these stability conditions is best appreciated by comparing with the analogous conditions for the purely linear network obtained by dropping the rectification from (1). In a linear network, all eigenvalues of W would have to be smaller than unity to ensure asymptotic stability. Here only nonnegative eigenvectors are able to grow without bound, due to the rectification, so that only their eigenvalues must be less than unity. All principal submatrices of W must be considered because different sets of feedback connections are active, depending on the set of neurons that are above threshold. In a linear network, <span class="math inline">\(I - W\)</span> would have to be positive definite to ensure asymptotic stability, but because of the rectification, here this condition is replaced by the weaker condition of copositivity.</p>
</blockquote>
<p>So, the tradeoff for the rectification is that we get stability for more general sets of weight matricies, but we have to analyze all <span class="math inline">\(2^n\)</span> <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)#Submatrix">principal submatrices</a> to find out if we get it.</p>
<h2 id="theorems-2-and-3---permitted-and-forbidden-sets">Theorems 2 and 3 - Permitted and Forbidden Sets</h2>
<p>These two theorems classify the permitted and forbidden sets of a network.</p>
<p>The first theorem tells us that if a network has a set of global, stable steady states, then all of the nonnegative eigenvectors of all principal submatrices of <span class="math inline">\(I-W\)</span> will have positive eigenvalues. When the system begins with positive activations, the activation will flow along time-varying superpositions of the (nonnegative) eigenvectors toward some fixed point. We might think that <em>every</em> subsystem has to have a fixed point, then. But this is not so. It could turn out that what would be the fixed point for the subsystem lies outside of its chamber, and then the dynamics will have changed before the system ever reaches it. In this case the system has a forbidden set, because the neurons in that subsystem cannot be coactivated together at a stable steady state.</p>
<blockquote>
<p><strong>Theorem 2</strong></p>
<p>If the matrix <span class="math inline">\(I - W\)</span> is copositive, then the following statements are equivalent:</p>
<ol>
<li>The matrix <span class="math inline">\(I - W\)</span> is not positive definite.</li>
<li>There exists a forbidden set.</li>
<li>The network is conditionally multistable. That is, there exists an input <span class="math inline">\(b\)</span> such that there is more than one stable steady state.</li>
</ol>
</blockquote>
<figure><img src="../images/twofp.png" alt="Plots of a three neuron system with two stable points." /><figcaption>A three neuron system with two steady states.</figcaption></figure>

<p>They prove that (2) implies (3) by examining a Lyapunov function <span class="math inline">\(V(x) = \frac12 x^\top (I - W) x - b^\top x\)</span>. They argue as follows: a forbidden set implies the existence of a negative eigenvalue of <span class="math inline">\(I - W\)</span> in the corresponding active submatrix. The function <span class="math inline">\(V\)</span> therefore forms a saddle. The system can be initially activated on either side of the saddle, and will descend to a different minimum on each side. These are two different stable steady states.</p>
<figure><img src="../images/multistable.png" alt="3D plot of Lyapunov function and a contour plot with line given by a positive eigenvector"><figcaption>The Lyapunov function for a two neuron system with connection weights equal to 2. On the right, a line in the direction of an eigenvector with positive eigenvalue is in red.</figcaption></figure>

<blockquote>
<p><strong>Theorem 3</strong> If <span class="math inline">\(W\)</span> is symmetric, then the following conditions are equivalent:</p>
<ol>
<li>The matrix <span class="math inline">\(I - W\)</span> is positive definite.</li>
<li>All sets are permitted.</li>
<li>For all <span class="math inline">\(b\)</span> there is a unique steady state, and it is stable.</li>
</ol>
</blockquote>
<p>A linear system, like <span class="math inline">\(\ref 3\)</span>, will have a global steady state if <span class="math inline">\(I-W\)</span> is positive definite (all eigenvalues are positive). So, in a rectified system if <em>all</em> the neurons may be activated together at a stable steady state, the system behaves much like a linear system in regard to its steady states. Rectified systems are more interesting when they have some forbidden sets.</p>
<p>If I am understanding the paper correctly, we could characterize permitted and forbidden sets like this:</p>
<table>
<tbody>
<tr class="odd">
<td>permitted set</td>
<td>forbidden set</td>
</tr>
<tr class="even">
<td>principal submatrix with only positive eigenvalues</td>
<td>principal submatrix with a negative eigenvalue</td>
</tr>
<tr class="odd">
<td>neurons that can be coactivated at a stable steady state</td>
<td>neurons that cannot be coactivated at a stable steady state</td>
</tr>
<tr class="even">
<td>positive eigenvectors and positive eigenvalues</td>
<td>eigenvectors with negative components that give negative eigenvalues</td>
</tr>
</tbody>
</table>
<p>Finally, they show with the <a href="https://en.wikipedia.org/wiki/Min-max_theorem#Cauchy_interlacing_theorem">interlacing theorem</a> that the sets of neurons that may be coactivated together at stable states are constant in some sense throughout the system, for the reason that eigenvalues of a submatrix have to be contained in the radius of eigenvalues of the parent matrix.</p>
<blockquote>
<p><strong>Theorem 4</strong></p>
<p>Any subset of a permitted set is permitted. Any superset of a forbidden set is forbidden.</p>
</blockquote>
<p>Here for instance are the permitted sets for a network of ten neurons with randomly generated weights.</p>
<figure><img src="../images/permitted.png" alt="Diagram of permitted sets for a ten neuron network." /><figcaption>Permitted sets for a ten neuron network.</figcaption></figure>

<p>(This only shows “maximal” permitted sets; that is, those permitted sets not contained in any other permitted set.)</p>
<p>And this shows the steady state of the topmost permitted set with each neuron receiving an input of 1.</p>
<figure><img src="../images/steadystate.png" /><figcaption>Left: Neural activations. Right: Steady states.</figcaption></figure>

<p>And here is a (different) network transitioning through stable states as inputs and activations vary.</p>
<video controls loop src="../images/stability.mp4"></video>

<h1 id="conclusion">Conclusion</h1>
<p>If a connection pattern in a network is a memory, then multistability allows the brain to store memories much more efficiently. Patterns of activation can overlap within a network. One neuron can partake of several memories, much like a single gene can be implicated in the expression of a multitude of traits or behaviors. I imagine that whatever process the brain uses for memory storage, it must make a tradeoff between robustness and efficiency. It wants to minimize the cost of storing memories and so should use as few neurons as possible to do so, yet the death of a single neuron shouldn’t disrupt the system as a whole. The model of overlapping patterns seems to me like a plausible solution.</p>
<p>(I decided to read this paper after becoming interested in <a href="http://www.personal.psu.edu/cpc16/">Carina</a> <a href="https://www.quantamagazine.org/mathematician-carina-curto-thinks-like-a-physicist-to-solve-neuroscience-problems-20180619/">Curto</a>’s work on <a href="http://sites.psu.edu/mathneurolab/ctln/">combinatorial threshold networks</a>. She and her collaborators have extended the ideas presented here to more general threshold networks that can display various kind of dynamic behavior. I hope I can review some of her work in the future.)</p>
<h1 id="appendix---computing-permitted-sets-in-julia">Appendix - Computing Permitted Sets in Julia</h1>
<pre class="julia"><code>using Combinatorics
using LinearAlgebra

&quot;&quot;&quot;Determine whether the list `l1` is a numerical translation of the
list `l2`. The function will return `true` when `l1 == k+.l2` for some `k` 
modulo `n+1`.&quot;&quot;&quot;
function istranslation(l1, l2, n::Int)
    any([l1 == map(x -&gt; mod(x+i, n+1), l2) for i in 1:n])
end

&quot;&quot;&quot;Returns a maximal set of lists from `lists` that are unique up to translation.&quot;&quot;&quot;
function removetranslations(lists, n::Int)
    ls = []
    for l in lists
        if !any(map(x-&gt;istranslation(l, x, n), ls))
            push!(ls, l)
        end
    end
    return ls
end

&quot;&quot;&quot;Returns a set of lists from `lists` that are not properly contained in 
any other list.&quot;&quot;&quot;
function removesubsets(lists)
    isproper(a, b) = issubset(a, b) &amp;&amp; a != b
    ls = []
    for a in lists
        if !any(map(b -&gt; isproper(a, b), lists))
            push!(ls, a)
        end
    end
    return ls
end

&quot;&quot;&quot;Determines whether a matrix `A` represents a permitted set of neurons. `A` 
should be of the form `I-W`, where `W` is the weight matrix.&quot;&quot;&quot;
function ispermitted(A)
    all(map(x -&gt; x&gt;0, eigvals(A)))
end

&quot;&quot;&quot;Returns a matrix `P` of all permitted sets represented by a matrix
`A` of the form `I-W`. If neuron `j` is contained in permitted set
`i`, then `P[i,j] == 1`; otherwise, `P[i,j] == 0`. Each permitted set
is unique up to translation, and is not contained in any other
permitted set in `P`.&quot;&quot;&quot;
function permittedparents(A)
    ps = []
    n = length(A[:,1])
    idxs = removetranslations(powerset(1:n), n)
    filter!(!isempty, idxs)
    for idx in idxs
        submatrix = A[idx, idx]
        if ispermitted(submatrix)
            push!(ps, idx) 
        end
    end
    ps = removesubsets(ps)
    P = zeros(length(ps), n)
    for (i, pp) in enumerate(ps)
        for j in pp
            P[i, j] = 1
        end
    end
    return P
end
</code></pre>
  </section>
  
</article>


        <!-- Post Footer -->
        <div>

        </div>

    
  </div>
  
  <div class="rounded-2 box-shadow-medium px-4 pb-4 mb-4 bg-white">

        <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../posts/change-of-basis-for-vectors-and-covectors/">Change of Basis for Vectors and Covectors</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../tags/coordinates/">coordinates</a>, <a href="../tags/covectors/">covectors</a>, <a href="../tags/vectors/">vectors</a>, <a href="../tags/linear-algebra/">linear-algebra</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: March 18, 2019
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#vectors">Vectors</a><ul class="incremental">
<li><a href="#example---part-1">Example - Part 1</a></li>
<li><a href="#formulas">Formulas</a></li>
<li><a href="#example---part-2">Example - Part 2</a></li>
</ul></li>
<li><a href="#covectors">Covectors</a><ul class="incremental">
<li><a href="#example---part-3">Example - Part 3</a></li>
<li><a href="#the-dual-basis">The Dual Basis</a></li>
<li><a href="#formulas-1">Formulas</a></li>
<li><a href="#example---part-4">Example - Part 4</a></li>
</ul></li>
<li><a href="#summary-of-formulas">Summary of Formulas</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <blockquote>
<p>We share a philosophy about linear algebra: we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury.</p>
</blockquote>
<p><a href="https://mathoverflow.net/questions/11669/what-is-the-difference-between-matrix-theory-and-linear-algebra/19923">Irving Kaplansky</a></p>
<p>Often, the first step in analyzing a problem is to <em>transform</em> it into something more amenable to our analysis. We would like the <em>representation</em> of our problem to reflect as naturally as possible whatever features of it we are most interested in. We might normalize data through a scaling transform, for instance, to eliminate spurious differences among like quantities. Or we might rotate data to align some of its salient dimensions with the coordinate axes, simplifying computations. Many matrix decompositions take the form <span class="math inline">\(M = BNA\)</span>. When <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> are non-singular, we can think of <span class="math inline">\(N\)</span> as being a simpler representation of <span class="math inline">\(M\)</span> under coordinate transforms <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span>. The <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">spectral decomposition</a> and the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> are of this form.</p>
<p>All of these kinds of coordinate transformations are <em>linear</em> transformations. Linear coordinate transformations come about from operations on basis vectors that leave any vectors represented by them <a href="https://en.wikipedia.org/wiki/Active_and_passive_transformation">unchanged</a>. They are, in other words, a change of basis.</p>
<p>This post came about from my frustration at not finding simple formulas for these transformations with simple explanations to go along with them. So here, I tried to give a simple exposition of coordinate transformations for vectors in vector spaces along with transformations of their cousins, the covectors in the dual space. I’ll get into matrices and some applications in a future post.</p>
<h1 id="vectors">Vectors</h1>
<h2 id="example---part-1">Example - Part 1</h2>
<p>Let’s go through an example to see how it works. (I’ll assume the field is <span class="math inline">\(\mathbb{R}\)</span> throughout.)</p>
<p>Let <span class="math inline">\(e\)</span> be the standard basis in <span class="math inline">\(\mathbb{R}^2\)</span> and let <span class="math inline">\(e'\)</span> be another basis where <span class="math display">\[
\begin{array}{cc}
e'_1 = \frac12 e_1, &amp; e'_2 = -e_1 + 2 e_2
\end{array}
\]</span> So we have written the basis <span class="math inline">\(e'\)</span> in terms of the standard basis <span class="math inline">\(e\)</span>. As vectors they look like this:</p>
<figure><img src="../images/bases.jpg" alt="Drawing of the basis vectors e_1, e_2, and e'_1, e'_2" /></figure>

<p>And each will induce its own coordinate system, indicating the angle and orientation of each axis, and each axis’ unit of measure.</p>
<figure><img src="../images/coordinates.jpg" alt="Drawing of basis vectors and the coordinates they induce."></figure>

<p>We can write any vector in <span class="math inline">\(\mathbb{R}^2\)</span> as a linear combination of these basis elements.</p>
<p><span class="math display">\[\begin{array}{cc}
v = e_1 + 2 e_2, &amp; w' = 5 e'_1 + \frac12 e'_2
\end{array}\]</span></p>
<figure><img src="../images/vectors.jpg" alt="Drawing of vectors in two coordinate systems."></figure>

<p>We call the coefficients on the basis elements the <strong>coordinates</strong> of the vector in that basis. So, in basis <span class="math inline">\(e\)</span> the vector <span class="math inline">\(v\)</span> has coordinates <span class="math inline">\((1, 2)\)</span>, and in basis <span class="math inline">\(e'\)</span> the vector <span class="math inline">\(w'\)</span> has coordinates <span class="math inline">\((5, \frac12)\)</span>.</p>
<h2 id="formulas">Formulas</h2>
<p>The choice of basis is just a choice of representation. The vector itself should stay the same. So the question is – how can we rewrite a vector in a <em>different</em> basis without changing the vector itself?</p>
<p>Let’s establish some notation. First, whenever we are talking about a vector in the abstract, let’s write <span class="math inline">\(\mathbf{v}\)</span>, and whenever we are talking about a vector represented in some basis let’s write <span class="math inline">\(v\)</span>. So the same vector <span class="math inline">\(\mathbf{v}\)</span> might have two different basis representations <span class="math inline">\(v\)</span> and <span class="math inline">\(v'\)</span>, which nevertheless all stand for the same vector: <span class="math inline">\(\mathbf{v} = v = v'\)</span>. However, when we write <span class="math inline">\(e\)</span> for a basis, we mean a list of vectors <span class="math inline">\(e_i\)</span> that form a basis in some vector space <span class="math inline">\(V\)</span>. So, <span class="math inline">\(v = v'\)</span> always, but in general <span class="math inline">\(e \neq e'\)</span>.</p>
<p>Our basis elements let’s index with subscripts (like <span class="math inline">\(e_1\)</span>), and coordinates let’s index with superscripts (like <span class="math inline">\(v^1\)</span>). This will help us keep track of which one we’re working with. Also, let’s write basis elements as row vectors, and coordinates as column vectors. This way we can write a vector as a matrix product of the basis elements and the coordinates:</p>
<p><span class="math display">\[v = \begin{bmatrix} e_1 &amp; e_2 \end{bmatrix}\begin{bmatrix}v^1 \\
v^2\end{bmatrix} = v^1 e_1 + v^2 e_2
\]</span></p>
<p>Now we can also write the transformation given above of <span class="math inline">\(e\)</span> into <span class="math inline">\(e'\)</span> using matrix multiplication:</p>
<p><span class="math display">\[e' = \begin{bmatrix}e'_1&amp; e'_2\end{bmatrix} = \begin{bmatrix}e_1&amp;  e_2\end{bmatrix}\begin{bmatrix}
\frac12 &amp; -1 \\
0 &amp; 2 
\end{bmatrix} = \begin{bmatrix}\frac12 e_1 &amp; -e_1 + 2 e_2\end{bmatrix}\]</span></p>
<p>The <span class="math inline">\(2 \times 2\)</span> matrix used in that transformation is called the <strong>transformation matrix</strong> from the basis <span class="math inline">\(e\)</span> to the basis <span class="math inline">\(e'\)</span>.</p>
<p>The general formula is</p>
<p><span class="math display">\[\formbox{e' = e A}\]</span></p>
<p>where <span class="math inline">\(A\)</span> is the transformation matrix. We can use this <em>same</em> matrix to transform coordinate vectors, but we shouldn’t necessarily expect that we can use the same formula. The bases and the coordinates are playing different roles here: the basis elements are vectors that describe the coordinate system, while the coordinates are scalars that describe a vector’s position in that system.</p>
<p>Let’s think about how this should work. Generally we write <span class="math inline">\(v = v^1 e_1 + v^2 e_2 \cdots + v^n e_n\)</span>. If we make some new basis <span class="math inline">\(e'\)</span> by multiplying all the <span class="math inline">\(e_i\)</span>’s by 2, say, and <em>also</em> multiplied all the <span class="math inline">\(v_j\)</span>’s by 2, then we would end up with a vector <em>four times</em> the size of the original. Instead, we should have multiplied all the <span class="math inline">\(v_j\)</span>’s by <span class="math inline">\(\frac12\)</span>, the inverse of 2, and then we would have <span class="math inline">\(v' = v\)</span>, as needed. The vector must be the same in either basis.</p>
<p>So, if we change the <span class="math inline">\(e\)</span>’s by some factor then, the <span class="math inline">\(v\)</span>’s need to change in the <em>inverse</em> manner to maintain equality. This suggests that to change <span class="math inline">\(v\)</span> into a representation <span class="math inline">\(v'\)</span> in basis <span class="math inline">\(e'\)</span> we should use instead</p>
<p><span class="math display">\[\formbox{v' = A^{-1} v}\]</span></p>
<p>(We’ll prove it a little bit later.)</p>
<p>The fact that basis elements change in one way (<span class="math inline">\(e' = e A\)</span>) while coordinates change in the inverse way (<span class="math inline">\(v' = A^{-1} v\)</span>), is why we sometimes call the basis elements <strong>covariant</strong> and the vector coordinates <strong>contravariant</strong>, and distinguish them with the position of their indices.</p>
<h2 id="example---part-2">Example - Part 2</h2>
<p>Let’s go back to our example. Using our formula, we get</p>
<p><span class="math display">\[
v' = \begin{bmatrix}2 &amp; 1 \\
0 &amp; \frac12 \end{bmatrix} \begin{bmatrix}1 \\
2\end{bmatrix} = \begin{bmatrix}4 \\
1\end{bmatrix}
\]</span></p>
<p>But what about <span class="math inline">\(w'\)</span>? Well, since its representation is in <span class="math inline">\(e'\)</span>, to convert in the opposite direction, to <span class="math inline">\(e\)</span>, we need to use the transformation that’s the inverse of <span class="math inline">\(A^{-1}\)</span>, namely, <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[
w = \begin{bmatrix}\frac12 &amp; -1 \\
0 &amp; 1 \end{bmatrix} \begin{bmatrix}5 \\
\frac12\end{bmatrix} = \begin{bmatrix}2 \\
1\end{bmatrix}
\]</span></p>
<p>And now we have:</p>
<figure><img src="../images/transformed.jpg" alt="Drawing of vectors v, v', w, and w'." /></figure>

<p>Each vector is unchanged after a change of basis.</p>
<h1 id="covectors">Covectors</h1>
<p>Recall the <a href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a> on a vector space.</p>
<p>We might ask, given some vector <span class="math inline">\(v\)</span> how does an inner product vary as we range over vectors <span class="math inline">\(w\)</span>? In this case, we could think of <span class="math inline">\(\langle v, \cdot\rangle\)</span> as a function of vectors in <span class="math inline">\(V\)</span> whose outputs are scalars. In fact, these sorts of functions themselves form a vector space, called the <strong>dual space</strong> of <span class="math inline">\(V\)</span>, which we write <span class="math inline">\(V^*\)</span>. The members of <span class="math inline">\(V^*\)</span> are called <strong>linear functionals</strong> or <strong>covectors</strong>. The covector given by <span class="math inline">\(\langle v, \cdot\rangle\)</span> we denote <span class="math inline">\(v^*\)</span>.</p>
<p>We’ve been working with vectors in <span class="math inline">\(\mathbb{R}^n\)</span>, and in <span class="math inline">\(\mathbb{R}^n\)</span> the (canonical) inner product is the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>. This means that if we denote the covectors in <span class="math inline">\(V^*\)</span> as <em>rows</em> and the vectors in <span class="math inline">\(V\)</span> as <em>columns</em> (as usual), then we can write</p>
<p><span class="math display">\[
v^*(w) = \begin{bmatrix} v_1 &amp; \cdots &amp; v_n\end{bmatrix}\begin{bmatrix}w^1 \\
\vdots\\
w^n\end{bmatrix} = v_1 w^1 + \cdots + v_n w^n
\]</span></p>
<p>So, the covectors are functions <span class="math inline">\(\mathbb{R}^n \to \mathbb{R}\)</span>, but we can do computations with them just like we do with vectors, using matrix multiplication. We still write the indices of the row vectors as subscripts and the indices of the column vectors as superscripts.</p>
<p>If we can think about vectors in <span class="math inline">\(\mathbb{R}^n\)</span> as arrows, how should we think about covectors? To simplify things, let’s restrict our attention to the two-dimensional case. Now, consider the action of a covector <span class="math inline">\(v^*\)</span> on some unknown vector <span class="math inline">\(w = \begin{bmatrix}x&amp; y\end{bmatrix}^\top\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<p><span class="math display">\[
v^*(w) = v_1 x + v_2 y
\]</span></p>
<p>Now if we look at the level sets of this function, <span class="math inline">\(v_1 x + v_2 y = k\)</span>, it should start to look familiar…</p>
<p>It’s a family of <a href="https://en.wikipedia.org/wiki/Linear_equation#Two-point_form">lines</a>!</p>
<p>And to find out the value of <span class="math inline">\(v^*(w)\)</span> we just count how many lines of <span class="math inline">\(v^*\)</span> the vector <span class="math inline">\(w\)</span> passes through (including maybe “fractional” valued lines – <span class="math inline">\(k\)</span> doesn’t have to just be an integer). More generally, the covectors of <span class="math inline">\(\mathbb{R}^n\)</span> can be thought of as <a href="https://en.wikipedia.org/wiki/Hyperplane">hyperplanes</a>, and the value of <span class="math inline">\(v^*(w)\)</span> can be determined by how many hyperplanes of <span class="math inline">\(v^*\)</span> the vector <span class="math inline">\(w\)</span> passes through. And furthermore, the vector <span class="math inline">\(v\)</span> will be the <a href="https://en.wikipedia.org/wiki/Normal_(geometry)">normal</a> vector to the hyperplanes given by <span class="math inline">\(v^*\)</span>, that is, they are perpendicular.</p>
<h2 id="example---part-3">Example - Part 3</h2>
<p>In the standard basis, let <span class="math inline">\(v^*\)</span> be given by <span class="math inline">\(\begin{bmatrix}1 &amp; 2\end{bmatrix}\)</span>. Its family of lines will then be <span class="math inline">\(x + 2 y = k\)</span>. Now let <span class="math inline">\(w\)</span> be given by <span class="math inline">\(\begin{bmatrix}2 &amp; 1\end{bmatrix}\)</span>, and count how many lines <span class="math inline">\(w\)</span> crosses through:</p>
<figure><img src="../images/covectors.jpg" alt="Left: Drawing of v and v^*. Right: Drawing of v^* and w." /></figure>

<p>It’s exactly the same as <span class="math inline">\(v^*(w) = 2 + 2(1) = 4\)</span>! I think that’s pretty cool.</p>
<h2 id="the-dual-basis">The Dual Basis</h2>
<p>Okay, so what about bases in <span class="math inline">\(V^*\)</span>? We’d like to have a basis for <span class="math inline">\(V^*\)</span> that is the “best fit” for whatever basis we have in <span class="math inline">\(V\)</span>. This turns out to be the basis given by: <span class="math display">\[
e^i(e_j) =
\begin{cases}
  1 &amp; \text{if } i = j\\
  0 &amp; \text{if } i \ne j
\end{cases}\]</span></p>
<p>where <span class="math inline">\((e_j)\)</span> is a basis in <span class="math inline">\(V\)</span>. Or sometimes people write instead <span class="math inline">\(e^i(e_j) = \delta^i_j\)</span>, where <span class="math inline">\(\delta^i_j\)</span> is the <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a>. We call this basis <span class="math inline">\((e^i)\)</span> the <strong>dual basis</strong> of <span class="math inline">\((e_j)\)</span>. You can see that a basis and its dual have a kind of “bi-orthogonality” property that turns out to be very convenient.</p>
<p>Let’s look at formulas for changing bases now. If we have a vector <span class="math inline">\(v\)</span> in <span class="math inline">\(V\)</span> written as a column, how can we find the corresponding vector <span class="math inline">\(v^*\)</span> in <span class="math inline">\(V^*\)</span>? The obvious thing to do would be to take the transposition of <span class="math inline">\(v\)</span>. This will not always work. Recall the definitions of <span class="math inline">\(v, v', w\)</span> and <span class="math inline">\(w'\)</span> from the <em>first section</em>, and consider:</p>
<p><span class="math display">\[v^\top v = \begin{bmatrix} 1 &amp; 2\end{bmatrix}\begin{bmatrix}1 \\
2\end{bmatrix} = 1 + 4 = 5\]</span></p>
<p><span class="math display">\[v'^\top v' = \begin{bmatrix} 4 &amp; 1\end{bmatrix}\begin{bmatrix}4 \\
1\end{bmatrix} = 16 + 1 = 17\]</span></p>
<p>This is no good. We get two different values for <span class="math inline">\(\bar v^*(\bar w)\)</span> depending on which basis we use, but the values of a function on a vector space shouldn’t depend on the basis. The trouble is that the dual of <span class="math inline">\((e'_i)\)</span> isn’t the transpose of those basis vectors (they don’t satisfy the bi-orthogonality property), so the duals of those vectors represented in it won’t be the transposes of those vectors either.</p>
<p>This <em>will</em> be true for <a href="https://en.wikipedia.org/wiki/Orthonormality">orthonormal</a> bases, however. The standard basis <span class="math inline">\((e_i)\)</span> <em>is</em> orthonormal, and the duals of the vectors represented in it will in fact be those transposes.</p>
<p><span class="math display">\[ \formbox{v^* = v^\top \text{for any vector } v \text{ written in an orthonormal basis.}} \]</span></p>
<h2 id="formulas-1">Formulas</h2>
<p>The next question is, if we perform a change of basis in <span class="math inline">\(V\)</span>, what is the corresponding change in <span class="math inline">\(V^*\)</span>? Let’s use the same reasoning that we did before. For a vector <span class="math inline">\(w\)</span> in <span class="math inline">\(\mathbb{R}\)</span> and a covector <span class="math inline">\(v^*\)</span>, we have</p>
<p><span class="math display">\[
v^*(w) = v_1 w^1 + \cdots + v_n w^n
\]</span></p>
<p>And so, like before, if we change the values of the <span class="math inline">\(w_j\)</span>’s, the values of the <span class="math inline">\(v^i\)</span>’s should change in the inverse manner to preserve equality. But <span class="math inline">\(w\)</span> changes as <span class="math inline">\(w' = A^{-1} w\)</span>, so <span class="math inline">\(v^*\)</span> must change as <span class="math inline">\(v'^* = v^* A\)</span>. And its basis (the dual basis) must change as <em>its</em> inverse: <span class="math inline">\(e'^* = A^{-1} e^*\)</span>.</p>
<p><span class="math display">\[\formbox{\begin{align}
e'^* &amp;= A^{-1} e^*\\
v'^* &amp;= v^* A
\end{align}}\]</span></p>
<p>Notice that this time the basis vectors are playing the <strong>contravariant</strong> part, while the coordinates are playing the <strong>covariant</strong> part with respect to the original vector space.</p>
<h2 id="example---part-4">Example - Part 4</h2>
<p>Lets continue our example. Since <span class="math inline">\(e\)</span> is the standard basis, it is orthonormal, and we can therefore find the duals of <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> by taking transposes. We can then apply our formula to find the duals of <span class="math inline">\(v'^*\)</span> and <span class="math inline">\(w'^*\)</span>.</p>
<span class="math display">\[\begin{align}
v'^*(x, y) &amp;= \begin{bmatrix}1 &amp; 2\end{bmatrix}\begin{bmatrix}\frac12 &amp; -1\\
0 &amp; 2\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= \begin{bmatrix}\frac12 &amp; 3\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= \frac12 x + 3y\\
\\
w'^*(x, y) &amp;= \begin{bmatrix}2 &amp; 1\end{bmatrix}\begin{bmatrix}\frac12 &amp; -1\\
0 &amp; 2\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= \begin{bmatrix}1 &amp; 0\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= 2x + 2y
\end{align}
\]</span>
<p>The duals too are unchanged after a change of basis.</p>
<figure><img src="../images/covectors2.jpg" alt="Left: Drawing of $v^*$ and $w^*$. Right: Drawing of $v'^*$ and $w'^*$." /></figure>

<h1 id="summary-of-formulas">Summary of Formulas</h1>
<p><span class="math display">\[\formbox{\begin{array}{llr} 
e'   &amp;= e A      &amp; &amp;(1)\\ 
v'   &amp;= A^{-1} v &amp; &amp;(2)\\
e'^* &amp;= A^{-1} e^* &amp; &amp;(3)\\
v'^* &amp;= v^* A      &amp; &amp;(4)
\end{array} }\]</span></p>
<p>Suppose <span class="math inline">\((1)\)</span>, that <span class="math inline">\(e' = e A\)</span>, where <span class="math inline">\(A\)</span> is a non-singular matrix.</p>
<p><strong>Proof of (2):</strong> We know <span class="math inline">\(e v = e' v'\)</span>. Now</p>
<p><span class="math display">\[
e' v' = e v = e A A^{-1} v = e' (A^{-1} v)
\]</span></p>
<p>But then it must be that <span class="math inline">\(v' = A^{-1} v\)</span> since basis representations are unique.</p>
<p><strong>Proof of (4):</strong> We also know <span class="math inline">\(v'^* w' = v^* w\)</span> for all vectors <span class="math inline">\(w\)</span>. But then</p>
<p><span class="math display">\[
v'^* w' = v^* w = v^* w = v^* A A^{-1} w = (v^* A) w'
\]</span></p>
<p>for all <span class="math inline">\(w'\)</span>. So, <span class="math inline">\(v'^* = v^* A\)</span>.</p>
<p><strong>Proof of (3):</strong> Lastly,</p>
<p><span class="math display">\[
v'^* e'^* = v^* e^* = v^* A A^{-1} e^* = v'^* A^{-1} e^*
\]</span></p>
<p>for all <span class="math inline">\(w'\)</span>. So, <span class="math inline">\(e'^* = A^{-1}e^*\)</span>. <strong>QED</strong></p>
  </section>
  
</article>


        <!-- Post Footer -->
        <div>

        </div>

    
  </div>
  
  <div class="rounded-2 box-shadow-medium px-4 pb-4 mb-4 bg-white">

        <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../posts/a-tour-of-tensors/">A Tour of Tensors</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../tags/numpy/">numpy</a>, <a href="../tags/sage/">sage</a>, <a href="../tags/tensors/">tensors</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: February  5, 2019
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#abstract-tensors">Abstract Tensors</a><ul class="incremental">
<li><a href="#construction-of-the-tensor-space">Construction of the Tensor Space</a></li>
</ul></li>
<li><a href="#tensors-as-arrays">Tensors as Arrays</a></li>
<li><a href="#tensors-as-maps">Tensors as Maps</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <p>Tensors can sometimes have a fearsome reputation. They are at heart, however, no more difficult to define than polynomials. I’ve tried in these notes to take a computational focus and to avoid formalism when possible; I haven’t assumed any more than what you might encounter in an undergraduate linear algebra course. If you’re interested in tensors applied to machine learning, or have wondered why arrays in Tensorflow are called tensors, you might find this useful. I’ll do some computations in <a href="http://www.sagemath.org/">Sage</a> and also in <a href="http://www.numpy.org/">Numpy</a> for illustration.</p>
<h1 id="abstract-tensors">Abstract Tensors</h1>
<p>First, let’s take brief look at tensors in the abstract. This is just to give us an idea of what properties they have and how they function. I’ll gloss over most of the details of the construction.</p>
<p>A tensor is a vector. It is an element of a vector space. Being a vector, if we have a basis for the space we can write the tensor as a list of coordinates (or maybe something like a matrix or an array – we’ll see how).</p>
<p>A tensor is a vector in a product vector space. This means that part of it comes from one vector space and part of it comes from another. These parts combine in a way that fits with the usual notions of how products should work. Why would we want these tensors, these products of vectors? It turns out that lots of useful things are tensors. Matrices and linear maps are tensors, and so are determinants and inner products and cross products. Tensors give us power to express many useful ideas.</p>
<p>A simple product of vectors looks like <span class="math inline">\(v \otimes w\)</span> and the product space looks like <span class="math inline">\(V \otimes W\)</span>, where <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are vector spaces. The elements of <span class="math inline">\(V \otimes W\)</span> are linear combinations of these simple products. So, a typical element of <span class="math inline">\(V \otimes W\)</span> might look like <span class="math inline">\(v_1 \otimes w_2 + 5(v_4 \otimes w_1) + 3(v_3 \otimes w_2)\)</span>.</p>
<p>Again, <span class="math inline">\(V \otimes W\)</span> is a vector space. Its vectors are called tensors. Tensors are linear combinations of simple tensors like <span class="math inline">\(v \otimes w\)</span>.</p>
<p>The tensor space <span class="math inline">\(V \otimes W\)</span> is a vector space, but its vectors have some special properties given to them by <span class="math inline">\(\otimes\)</span>. This product has many of the same useful properties as products of numbers. They are:</p>
<p><span class="math display">\[ \textbf{Distributivity:  } v \otimes (w_1 + w_2) = v \otimes w_1 + v \otimes w_2 \]</span></p>
<p>(Just like <span class="math inline">\(x(y + z) = xy + xz\)</span>.)</p>
<p>and</p>
<p><span class="math display">\[ \textbf{Scalar Multiples: } a (v \otimes w) = (av) \otimes w = v \otimes (aw) \]</span></p>
<p>(Just like <span class="math inline">\(a(xy) = (ax)y = x(ay)\)</span>.)</p>
<p>The tensor product also does what we expect with the zero vector, namely: <span class="math inline">\(v \otimes w = 0\)</span> if and only if <span class="math inline">\(v = 0\)</span> or <span class="math inline">\(w = 0\)</span>. The tensor product does not have the commutivity property however. A tensor <span class="math inline">\(v \otimes w\)</span> doesn’t have to be the same as <span class="math inline">\(w \otimes v\)</span>. For one, the vector on the left has to come from <span class="math inline">\(V\)</span> and the vector on the right has to come from <span class="math inline">\(W\)</span>.</p>
<p>Using these properties we can manipulate tensors just like we do polynomials. For instance:</p>
<span class="math display">\[\begin{equation}
 \begin{split}
 &amp; 2(v_1 \otimes w_1) + 3(v_1 + v_2) \otimes w_1 \\
 = &amp; 2(v_1 \otimes w_1) + 3(v_1 \otimes w_1) + 3(v_2 \otimes w_1) \\
 = &amp; 5(v_1 \otimes w_1) + 3(v_2 \otimes w_1)
 \end{split}
\end{equation}
\]</span>
<p>You could think of an abstract tensor as a sort of polynomial where the odd-looking product <span class="math inline">\(\otimes\)</span> reminds us that the <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> don’t generally commute.</p>
<p>Here’s an example. <code>FiniteRankFreeModule</code> is creating a vector space of dimension 2 over the quotients <span class="math inline">\(\mathbb Q\)</span>. A module is a kind of generalized vector space.</p>
<pre class="python"><code>M = FiniteRankFreeModule(QQ, 2, name='M', start_index=1)
v = M.basis('v')
s = M.tensor((2, 0), name='s')
s[v,:] = [[1, 2], [3, 4]]
t = M.tensor((2, 0), name='t')
t[v,:] = [[5, 6], [7, 8]]
latex(s.display(v))
latex(t.display(v))
latex((s + t).display(v))
</code></pre>
<div class="RESULTS drawer">
<p><span class="math display">\[ s = v_{1}\otimes v_{1} + 2 v_{1}\otimes v_{2} + 3 v_{2}\otimes v_{1} + 4 v_{2}\otimes v_{2} \]</span> <span class="math display">\[ t = 5 v_{1}\otimes v_{1} + 6 v_{1}\otimes v_{2} + 7 v_{2}\otimes v_{1} + 8 v_{2}\otimes v_{2} \]</span> <span class="math display">\[ s+t = 6 v_{1}\otimes v_{1} + 8 v_{1}\otimes v_{2} + 10 v_{2}\otimes v_{1} + 12 v_{2}\otimes v_{2} \]</span></p>
</div>
<h2 id="construction-of-the-tensor-space">Construction of the Tensor Space</h2>
<p>This is just a note on how the tensor space <span class="math inline">\(V \otimes W\)</span> can be constructed from <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>. It’s not essential to anything that follows.</p>
<p>Basically, we can construct <span class="math inline">\(V \otimes W\)</span> the same way that we can construct the complex numbers from the real numbers. To get the complex numbers from the reals, we just add in some new number <span class="math inline">\(i\)</span> to the real numbers and then define a simplification rule that says <span class="math inline">\(i^2 = -1\)</span>. To get <span class="math inline">\(V \otimes W\)</span> from <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, we just take all linear combinations of vectors from <span class="math inline">\(V\)</span> and vectors from <span class="math inline">\(W\)</span> and then define the Distributivity and Scalar Multiplication rules. The formalism that does this is called a <a href="https://en.wikipedia.org/wiki/">quotient space</a>, or see <a href="https://en.wikipedia.org/wiki/Tensor_product#The_definition_of_the_abstract_tensor_product">here</a> for the tensor product construction.</p>
<p>By constructing the space <span class="math inline">\(V \otimes W\)</span> in the most general way possible (meaning, not adding any other rules except distribution and scalar multiplication), we ensure that any kind of space or object that has these kinds of linear or multilinear properties has a representation as a tensor, and any other kind of construction that satisfies these rules will be essentially equivalent to the tensor construction. (The property is called a <a href="https://en.wikipedia.org/wiki/Universal_property">universal property</a>. It occurs all the time in mathematics and is very useful.) Tensors are the general language of linearity.</p>
<h1 id="tensors-as-arrays">Tensors as Arrays</h1>
<p>We can represent tensors as arrays, which is nice for doing computations.</p>
<p>If we have a basis for <span class="math inline">\(V\)</span> and a basis for <span class="math inline">\(W\)</span>, then we can make a basis for <span class="math inline">\(V \otimes W\)</span> in just the way we should expect: by taking all the products of the basis vectors. Namely, if <span class="math inline">\((e_i)\)</span> is a basis for <span class="math inline">\(V\)</span> and <span class="math inline">\((f_j)\)</span> is a basis for <span class="math inline">\(W\)</span>, then <span class="math inline">\((e_i \otimes f_j)\)</span> is a basis for <span class="math inline">\(V \otimes W\)</span>. This also means that the dimension of <span class="math inline">\(V \otimes W\)</span> is the product of the dimensions of <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>; that is, <span class="math inline">\(dim(V \otimes W) = dim(V)dim(W)\)</span>.</p>
<p>Recall that if we can write a vector in <span class="math inline">\(V\)</span> as <span class="math inline">\(v = \sum a_i e_i\)</span>, then <span class="math inline">\((a_i)\)</span> is its representation as a vector of coordinates. A tensor in <span class="math inline">\(V \otimes W\)</span> will instead have a representation as a matrix. If <span class="math inline">\(m = dim(V)\)</span> and <span class="math inline">\(n = dim(W)\)</span>, then this will be an <span class="math inline">\(m \times n\)</span> matrix. If we write a tensor in terms of its basis elements as:</p>
<p><span class="math display">\[\sum_i \sum_j c_{i,j} (e_i \otimes f_j)\]</span></p>
<p>then its matrix is <span class="math inline">\([c_{i,j}]\)</span>. The subscript of <span class="math inline">\(e_i\)</span> tells you the row and the subscript of <span class="math inline">\(f_j\)</span> tells you the column. For example, let’s say <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are both two-dimensional. We could write a tensor</p>
<p><span class="math display">\[(e_1 \otimes f_1) + 2(e_1 \otimes f_2) + 3(e_2 \otimes f_1) + 4(e_2 \otimes f_2)\]</span></p>
<p>as</p>
<span class="math display">\[\begin{bmatrix}
 1 &amp; 2 \\
 3 &amp; 4 \\
\end{bmatrix}
\]</span>
<p>But what if we have a vector <span class="math inline">\(v\)</span> in <span class="math inline">\(V\)</span> and a vector <span class="math inline">\(w\)</span> in <span class="math inline">\(W\)</span> and we want to find out what the matrix of <span class="math inline">\(v \otimes w\)</span> is? This is easy too. Say <span class="math inline">\(v = \sum a_i e_i\)</span> and <span class="math inline">\(w = \sum b_j f_j\)</span>. Then</p>
<p><span class="math display">\[v \otimes w = \sum_i \sum_j a_i b_j (e_i \otimes f_j)\]</span></p>
<p>and its matrix is <span class="math inline">\([a_i b_j]\)</span>. In other words, the entry in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> will be <span class="math inline">\(a_i b_j\)</span>.</p>
<p>It’s easy to find this matrix using matrix multiplication. If we write our coordinate vectors as column vectors, then our tensor product becomes an <a href="https://en.wikipedia.org/wiki/Outer_product">outer product</a>:</p>
<p><span class="math display">\[\color{RubineRed}v \color{black}\otimes \color{MidnightBlue}w\color{black} = \color{RubineRed}v\color{MidnightBlue} w^\mathsf{T}\]</span></p>
<p>For instance,</p>
<p><span class="math display">\[
 \color{RubineRed}(1, 2, 3)\color{Black} \otimes \color{RoyalBlue}(4, 5, 6)\color{Black} = 
 \color{RubineRed}\begin{bmatrix}
 1\\
 2\\
 3 \end{bmatrix} \color{black}
 \color{RoyalBlue}[4, 5, 6]\color{black}
 = \begin{bmatrix}
 \color{RubineRed}1\color{black}\cdot \color{RoyalBlue}4\color{black} &amp; \color{RubineRed}1\color{black}\cdot \color{RoyalBlue}5\color{black} &amp; \color{RubineRed}1\color{black}\cdot \color{RoyalBlue}6\color{black} \\ 
 \color{RubineRed}2\color{black}\cdot \color{RoyalBlue}4\color{black} &amp; \color{RubineRed}2\color{black}\cdot \color{RoyalBlue}5\color{black} &amp; \color{RubineRed}2\color{black}\cdot \color{RoyalBlue}6\color{black} \\ 
 \color{RubineRed}3\color{black}\cdot \color{RoyalBlue}4\color{black} &amp; \color{RubineRed}3\color{black}\cdot \color{RoyalBlue}5\color{black} &amp; \color{RubineRed}3\color{black}\cdot \color{RoyalBlue}6\color{black}\end{bmatrix}
 =\begin{bmatrix}
 4 &amp; 5 &amp; 6 \\
 8 &amp; 10 &amp; 15 \\
 12 &amp; 15 &amp; 18\end{bmatrix}
 \]</span></p>
<p>Notice the correspondence between the basis elements and the entries of the matrix in the next example.</p>
<pre class="python" data-results="drawer" data-exports="both" data-session="yes"><code>M = FiniteRankFreeModule(QQ, 3, name='M', start_index=1)
e = M.basis('e')
v = M([-2, 9, 5], basis=e, name='v')
w = M([1, 0, -2], basis=e, name='w')
latex((v*w).display())
latex((v*w)[e,:])
</code></pre>
<div class="RESULTS drawer">
<p><span class="math display">\[
 v\otimes w = -2 e_{1}\otimes e_{1} + 4 e_{1}\otimes e_{3} + 9 e_{2}\otimes e_{1} -18 e_{2}\otimes e_{3} + 5 e_{3}\otimes e_{1} -10 e_{3}\otimes e_{3} \\
 \left(\begin{array}{rrr}
 -2 &amp; 0 &amp; 4 \\
 9 &amp; 0 &amp; -18 \\
 5 &amp; 0 &amp; -10
 \end{array}\right)
 \]</span></p>
</div>
<p>We can extend the tensor product construction to any number of vector spaces. In this way we get multidimensional arrays. We might represent a tensor in a space <span class="math inline">\(U \otimes V \otimes W\)</span> as a “matrix of matricies.”</p>
<p><span class="math display">\[
 \left[\begin{array}{r}
   \left[\begin{array}{rr}
   c_{111} &amp; c_{112} \\
   c_{121} &amp; c_{122}
   \end{array}\right] \\
   \left[\begin{array}{rr}
   c_{211} &amp; c_{212} \\
   c_{221} &amp; c_{222}
   \end{array}\right]
 \end{array}\right]
 \]</span></p>
<p>And we use the more general <a href="https://en.wikipedia.org/wiki/Kronecker_product">Kronecker product</a> to find the product of tensors:</p>
<p><span class="math display">\[
 \color{RubineRed}(1, 2)
   \color{Black} \otimes
   \color{RoyalBlue}
   \left[\begin{array}{rr}
   1 &amp; 2 \\
   3 &amp; 4
   \end{array}\right]
 \color{Black} =
   \color{RubineRed}
   \left[\begin{array}{r}
   1 \\
   2 
   \end{array}\right]
   \color{RoyalBlue}
   \left[\begin{array}{rr}
   1 &amp; 2 \\
   3 &amp; 4
   \end{array}\right]
 \color{Black} =
 \left[\begin{array}{r}
   \color{RubineRed} 1
   \color{RoyalBlue}
   \left[\begin{array}{rr}
   1 &amp; 2 \\
   3 &amp; 4
   \end{array}\right] \\
   \color{RubineRed} 2
   \color{RoyalBlue}
   \left[\begin{array}{rr}
   1 &amp; 2 \\
   3 &amp; 4
   \end{array}\right]
 \color{Black}\end{array}\right] =
 \left[\begin{array}{r}
   \left[\begin{array}{rr}
   1 &amp; 2 \\
   3 &amp; 4
   \end{array}\right] \\
   \left[\begin{array}{rr}
   2 &amp; 4 \\
   6 &amp; 8
   \end{array}\right]
 \color{Black}\end{array}\right]
 \]</span></p>
<pre class="python" data-exports="both" data-session="yes"><code>M = FiniteRankFreeModule(QQ, 2, name='M', start_index=1)
e = M.basis('e')
u = M([1, 2], basis=e, name='u')
vw = M.tensor((2, 0), name='vw')
vw[e,:] = [[1, 2], [3, 4]]
(u*vw).display(e)
print()
(u*vw)[e,:]
</code></pre>
<pre class="example"><code>u*vw = e_1*e_1*e_1 + 2 e_1*e_1*e_2 + 3 e_1*e_2*e_1 + 4 e_1*e_2*e_2 + 2 e_2*e_1*e_1 + 4 e_2*e_1*e_2 + 6 e_2*e_2*e_1 + 8 e_2*e_2*e_2

[[[1, 2], [3, 4]], [[2, 4], [6, 8]]]
</code></pre>
<p>The number of vector spaces in the product space is the same as the number of dimensions in the arrays of its tensors (that is, the number of indices needed to specify a component). This number is called the “order” of a tensor (or sometimes “degree”). The order of the tensor above is 3.</p>
<p>We can extend this product to tensors of any order. The components of a tensor <span class="math inline">\(s \otimes t\)</span> can always be found by taking the product of the respective components of <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span>. For instance, if <span class="math inline">\(s_{12} = 5\)</span> and <span class="math inline">\(t_{345} = 7\)</span>, then <span class="math inline">\((s \otimes t)_{12345} = s_{12}t_{345} = 5\cdot7 = 35\)</span>.</p>
<pre class="python" data-exports="both" data-session="yes"><code>M = FiniteRankFreeModule(QQ, 5, name='M', start_index=1)
e = M.basis('e')
s = M.tensor((2, 0), name='s')
s[e,1,2] = 5
t = M.tensor((3, 0), name='t')
t[e,3,4,5] = 7
(s*t)[e,1,2,3,4,5]
</code></pre>
<pre class="example"><code>35
</code></pre>
<h1 id="tensors-as-maps">Tensors as Maps</h1>
<p>I mentioned earlier that things like cross-products and determinants are tensors. We’ll see how that works now. Recall that every vector space <span class="math inline">\(V\)</span> has a dual vector space <span class="math inline">\(V^*\)</span> which is the space of all linear maps <span class="math inline">\(V \rightarrow F\)</span>, where <span class="math inline">\(F\)</span> is the field of scalars of <span class="math inline">\(V\)</span>. In terms of matricies, we might think of elements in <span class="math inline">\(V\)</span> as column vectors and elements of <span class="math inline">\(V^*\)</span> as row vectors. Then, we can apply an element of <span class="math inline">\(V^*\)</span> to an element of <span class="math inline">\(V\)</span> just like we do when representing linear maps as matricies:</p>
<p><span class="math display">\[
 \left[a_1, a_2, a_3\right]
   \left[\begin{array}{r} 
   b_1 \\
   b_2 \\
   b_3 \end{array}\right] =
 a_1b_1 + a_2b_2 + a_3b_3
 \]</span></p>
<p>This in fact is just the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of the two vectors.</p>
<p>Let’s take a product <span class="math inline">\(T = V \otimes \cdots \otimes V \otimes V^* \otimes \cdots \otimes V^*\)</span>. The number of times <span class="math inline">\(V\)</span> occurs is called the “contravariant” order of the space and the number of times <span class="math inline">\(V^*\)</span> occurs is called the “covariant” order of the space. (The reason for these names is related to the <a href="https://en.wikipedia.org/wiki/Covariance_and_contravariance_of_vectors">change-of-basis</a> on vectors of those types). We say that a tensor has “type <span class="math inline">\((k, l)\)</span>” when it is of contravariant order <span class="math inline">\(k\)</span> and covariant order <span class="math inline">\(l\)</span>. So when we had earlier <code>M.tensor((2, 0), name='t')</code>, the <code>(2, 0)</code> was saying that we wanted a tensor with 2 contravariant parts.</p>
<p>Tensors of order <span class="math inline">\((0, 1)\)</span> are mappings <span class="math inline">\(V \rightarrow F\)</span>. They will map tensors of order <span class="math inline">\((1, 0)\)</span> (that is, column vectors) to the scalar field, and like above, this will just be the dot product of the two vectors.</p>
<pre class="python" data-exports="both" data-session="yes"><code>M = FiniteRankFreeModule(QQ, 3, name='M', start_index=1)
e = M.basis('e')

s = M.tensor((0, 1), name='s')
s[e, :] = [1, 2, 3]
t = M.tensor((1, 0), name='t')
t[e, :] = [4, 5, 6]

v = vector([1, 2, 3])
w = vector([4, 5, 6])

s(t) == v.dot_product(w)
</code></pre>
<pre class="example"><code>True
</code></pre>
<p>Expanding this idea, we can think of a tensor <span class="math inline">\(t\)</span> of order <span class="math inline">\((1,1)\)</span> either as a <a href="https://en.wikipedia.org/wiki/Multilinear_form">multilinear form</a> <span class="math inline">\(t:V^* \otimes V \rightarrow F\)</span> or as a <a href="https://en.wikipedia.org/wiki/Linear_map">linear map</a>, as <span class="math inline">\(t:V \rightarrow V\)</span> or as <span class="math inline">\(t:V^* \rightarrow V^*\)</span>. The difference is just in <a href="https://en.wikipedia.org/wiki/Partial_application">what and how many</a> arguments we pass in to the tensor. For instance, if we pass a column vector <span class="math inline">\(v\)</span> into the tensor <span class="math inline">\(t\)</span> in its second position (the position of <span class="math inline">\(V\)</span>), then we get a map <span class="math inline">\(V \rightarrow V\)</span>; this is the same as multiplying a vector by a matrix representing a linear map. This partial application is called a “contraction.”</p>
<pre class="python" data-exports="both" data-session="yes"><code>s = M.tensor((1, 1), name='s')
s[e, :] = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
t = M.tensor((1, 0), name='t')
t[e, :] = [4, 5, 6]

m = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
w = vector([4, 5, 6])

s.contract(t)[e,:] == list(m*w)
</code></pre>
<pre class="example"><code>True
</code></pre>
<p>Generally, we can represent any kind of multilinear map <span class="math inline">\(V^* \times \cdots \times V^* \times V \times \cdots \times V \rightarrow F\)</span> as a tensor in the space <span class="math inline">\(V \otimes \cdots \otimes V \otimes V^* \otimes \cdots \otimes V^*\)</span>. Since determinants and cross-products are multilinear maps, they too are tensors.</p>
<p>Sage makes a distinction between contravariant and covariant parts, but libraries like <code>numpy</code> and <code>tensorflow</code> do not. When using these, we can contract one tensor with another along any axes whose dimensions are the same. Their contraction operation is called <code>tensordot</code>.</p>
<pre class="python" data-exports="both" data-session="yes"><code>import numpy as np

s = np.ones((2, 3, 4, 5))
t = np.ones((5, 4, 3, 2))
np.tensordot(s, t, axes=[[0, 1, 2], [3, 2, 1]])
</code></pre>
<pre class="example"><code>array([[24., 24., 24., 24., 24.],
       [24., 24., 24., 24., 24.],
       [24., 24., 24., 24., 24.],
       [24., 24., 24., 24., 24.],
       [24., 24., 24., 24., 24.]])
</code></pre>
<p>We could think of the axes in <code>s</code> as representing row vectors (<span class="math inline">\(V^*\)</span>) and the axes in <code>t</code> as representing column vectors (<span class="math inline">\(V\)</span>).</p>
<p>We could also do this using <a href="https://en.wikipedia.org/wiki/Einstein_notation">Einstein notation</a>. Basically, whenever an index appears twice in an expression, it means to sum over that index while multiplying together the respective components (just like a dot product on those two axes).</p>
<pre class="python" data-exports="both" data-session="yes"><code>s = np.ones((2, 3, 4))
t = np.ones((4, 3, 2))

np.einsum('ija, bji -&gt; ab', s, t)
</code></pre>
<pre class="example"><code>array([[6., 6., 6., 6.],
       [6., 6., 6., 6.],
       [6., 6., 6., 6.],
       [6., 6., 6., 6.]])
</code></pre>
<p>Einstein summations are a convenient way to do lots of different kinds of tensor computations. <a href="https://rockt.github.io/2018/04/30/einsum">Here</a> are a bunch of great examples.</p>
<h1 id="conclusion">Conclusion</h1>
<p>That’s all for now! For anyone reading, I hope you found it informative. Tensors can be hard to get started on, but once you see the idea, I think you’ll find them a pleasure to work with.</p>
  </section>
  
</article>


        <!-- Post Footer -->
        <div>

        </div>

    
  </div>
  

<!-- Pagination -->
<nav class="paginate-container" aria-label="Pagination">
  <div class="pagination">
    
    <a class="previous_page text-gray-dark" rel="older" aria-label="Older Posts" href="../5/">⮜ Older</a>
    

    
    <a class="next_page text-gray-dark" rel="newer" aria-label="Newer Posts" href="../3/">Newer ⮞</a>
    
  </div>
</nav>

  
</div>


          </div>
          
          <div id="right" class="ml-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Tags</h1>
                <a style="font-size: 105%" href="../tags/bayesian/">bayesian</a> <a style="font-size: 100%" href="../tags/BMA/">BMA</a> <a style="font-size: 100%" href="../tags/calculator/">calculator</a> <a style="font-size: 100%" href="../tags/category-theory/">category-theory</a> <a style="font-size: 105%" href="../tags/classification/">classification</a> <a style="font-size: 100%" href="../tags/coordinates/">coordinates</a> <a style="font-size: 100%" href="../tags/covectors/">covectors</a> <a style="font-size: 100%" href="../tags/cql/">cql</a> <a style="font-size: 115%" href="../tags/data-science/">data-science</a> <a style="font-size: 105%" href="../tags/decision-boundaries/">decision-boundaries</a> <a style="font-size: 100%" href="../tags/eigenvalues/">eigenvalues</a> <a style="font-size: 100%" href="../tags/engrams/">engrams</a> <a style="font-size: 105%" href="../tags/finance/">finance</a> <a style="font-size: 100%" href="../tags/functional-programming/">functional-programming</a> <a style="font-size: 100%" href="../tags/generalized-inverse/">generalized-inverse</a> <a style="font-size: 100%" href="../tags/geometry/">geometry</a> <a style="font-size: 100%" href="../tags/haskell/">haskell</a> <a style="font-size: 100%" href="../tags/investing/">investing</a> <a style="font-size: 100%" href="../tags/julia/">julia</a> <a style="font-size: 100%" href="../tags/LDA/">LDA</a> <a style="font-size: 100%" href="../tags/least-squares/">least-squares</a> <a style="font-size: 115%" href="../tags/linear-algebra/">linear-algebra</a> <a style="font-size: 100%" href="../tags/linear-equations/">linear-equations</a> <a style="font-size: 100%" href="../tags/matrix-decomposition/">matrix-decomposition</a> <a style="font-size: 100%" href="../tags/MCMC/">MCMC</a> <a style="font-size: 100%" href="../tags/memory/">memory</a> <a style="font-size: 100%" href="../tags/moore-penrose-inverse/">moore-penrose-inverse</a> <a style="font-size: 100%" href="../tags/neural-networks/">neural-networks</a> <a style="font-size: 100%" href="../tags/neuroscience/">neuroscience</a> <a style="font-size: 100%" href="../tags/NLP/">NLP</a> <a style="font-size: 100%" href="../tags/numpy/">numpy</a> <a style="font-size: 100%" href="../tags/python/">python</a> <a style="font-size: 100%" href="../tags/QDA/">QDA</a> <a style="font-size: 110%" href="../tags/R/">R</a> <a style="font-size: 100%" href="../tags/ReLUs/">ReLUs</a> <a style="font-size: 100%" href="../tags/retirement/">retirement</a> <a style="font-size: 100%" href="../tags/review/">review</a> <a style="font-size: 100%" href="../tags/sage/">sage</a> <a style="font-size: 100%" href="../tags/simulation/">simulation</a> <a style="font-size: 100%" href="../tags/singular-values/">singular-values</a> <a style="font-size: 100%" href="../tags/stacking/">stacking</a> <a style="font-size: 100%" href="../tags/talk/">talk</a> <a style="font-size: 100%" href="../tags/tensors/">tensors</a> <a style="font-size: 110%" href="../tags/tutorial/">tutorial</a> <a style="font-size: 100%" href="../tags/vectors/">vectors</a>
              </div>
              <div>
                <h1>Links</h1>
                <a href="https://www.r-bloggers.com/">R-bloggers</a>
              </div>
            </div>
          </div>
        </div>

        <footer class="border p-3 bg-white text-gray">
  Site proudly generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
</footer>

        
      </div>
  </body>
</html>
