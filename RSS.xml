<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Math for Machines</title>
        <link>https://mathformachines.com</link>
        <description><![CDATA[A blog about data science and machine learning, with a lot of math.]]></description>
        <atom:link href="https://mathformachines.com/RSS.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Thu, 09 Jan 2020 00:00:00 UT</lastBuildDate>
        <item>
    <title>Optimal Decision Boundaries</title>
    <link>https://mathformachines.com/posts/decision/index.html</link>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Over the next few posts, we will investigate <em>decision boundaries</em>. A decision boundary is a graphical representation of the solution to a classification problem. Decision boundaries can help us to understand what kind of solution might be appropriate for a problem. They can also help us to understand the how various machine learning classifiers arrive at a solution.</p>
<p>In this post, we will look at a problem’s <em>optimal</em> decision boundary, which we can find when we know exactly how our data was generated. The optimal decision boundary represents the “best” solution possible for that problem. Consequently, by looking at the complexity of this boundary and at how much error it produces, we can get an idea of the inherent difficulty of the problem.</p>
<p>Unless we have generated the data ourselves, we won’t usually be able to find the optimal boundary. Instead, we approximate it using a classifier. A good machine learning classifier tries to approximate the optimal boundary for a problem as closely as possible.</p>
<p>In future posts, we will look at the approximating boundary created by various classification algorithms. We will investigate the strategy the classifier uses to create this boundary and how this boundary evolves as the classifier is trained on more and more data. There are many classification algorithms available to a data scientist – regression, discriminant analysis, decision trees, neural networks, to name a few – and it is important to understand which algorithm is appropritate for the problem at hand. Decision boundaries can help us to do this.</p>
<video autoplay loop mutued playsinline controls>
  <source src="/images/rf_mix.webm" type="video/webm">
  <source src="/images/rf_mix.mp4" type="video/mp4">
  <source src="/images/rf_mix.ogv" type="video/ogg">
</video>

<h1 id="optimal-boundaries">Optimal Boundaries</h1>
<p>A classification problem asks: given some observations of a thing, what is the best way to assign that thing to a class based on some of its features? For instance, we might want to predict whether a person will like a movie or not based on some data we have about them, the “features” of that person.</p>
<p>A solution to the classification problem is a rule that partitions the features and assigns each all the features of a partition to the same class. The “boundary” of this partitioning is the <strong>decision boundary</strong> of the rule.</p>
<p>It might be that two observations have exactly the same features, but are assigned to different classes. (Two things that look the same in the ways we’ve observed might differ in ways we haven’t observed.) In terms of probabilities this means both <span class="math display">\[P(C = 0 \mid X) \gt 0\]</span> and <span class="math display">\[P(C = 1 \mid X) \gt 0\]</span>. In other words, we might not be able with full certainty to classify an observation. We could however assign the observation to its <em>most probable</em> class. This gives us the decision rule <span class="math display">\[ \hat{C} = \operatorname*{argmax}_c P(C = c \mid X) \]</span></p>
<p>The boundary that this rule produces is the <strong>optimal decision boundary</strong>. It is the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">MAP estimate</a> of the class label, and it is the rule that minimizes classification error under the <a href="https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function">zero-one loss function</a>. We will look at error and loss more in a future post.</p>
<p>We will consider <em>binary</em> classification problems, meaning, there will only be two possible classes, 0 or 1. For a binary classification problem, the optimal boundary occurs at those points where each class is equally probable: <span class="math display">\[ P(C = 0 \mid X) = P(C = 1 \mid X) \]</span></p>
<h1 id="prepare-r">Prepare R</h1>
<p>We will use R to do our analysis. We’ll have a chance to try out <code>gganimate</code> and <code>patchwork</code>, a couple of newer packages that <a href="https://www.data-imaginist.com/">Thomas Lin Pedersen</a> has been working on; they are really nice.</p>
<p>Here we’ll define some functions to produce plots of our examples. All of these assume a classification problem where our response is binary, <span class="math inline">\(C \in \{0, 1\}\)</span>, and is predicted by two continuous features, <span class="math inline">\((X, Y)\)</span>.</p>
<p>Briefly, they are</p>
<ol>
<li><code>gg_sample</code> :: creates a layer for a sample of the features colored by class.</li>
<li><code>gg_density</code> :: creates a layer of contour plots for feature densities within each class.</li>
<li><code>gg_optimal</code> :: creates a layer showing an optimal decision boundary.</li>
<li><code>gg_mix_label</code> :: creates a layer labelling components in a mixture distribution.</li>
</ol>
<p>

<pre class="r"><code>library(magrittr)
library(tidyverse)
library(ggplot2)
library(gganimate)
library(patchwork)

theme_set(theme_linedraw() +
          theme(plot.title = element_text(size = 20),
                legend.position = &quot;none&quot;,
                axis.text.x = element_blank(),
                axis.text.y = element_blank(),
                axis.title.x = element_blank(),
                axis.title.y = element_blank(),
                aspect.ratio = 1))

#&#39; Make a sample layer
#&#39;
#&#39; @param data data.frame: a sample with continuous features `x` and `y`
#&#39; grouped by factor `class`
#&#39; @param classes (optional) a vector of which levels of `class` to
#&#39; plot; default is to plot data from all classes
gg_sample &lt;- function(data, classes = NULL, size = 3, alpha = 0.5, ...) {
    if (is.null(classes)) {
        subdata &lt;- data
    } else {
        subdata &lt;- filter(data, class %in% classes)
    }
    list(geom_point(data = subdata,
                    aes(x, y,
                        color = factor(class),
                        shape = factor(class)),
                    size = size,
                    alpha = alpha,
                    ...),
         scale_colour_discrete(drop = TRUE,
                               limits = levels(factor(data$class))))
}

#&#39; Make a density layer
#&#39;
#&#39; @param data data.frame: a data grid of features `x` and `y` with contours `z`
#&#39; @param data character: the name of the contour column 
gg_density &lt;- function(data, z, size = 1, color = &quot;black&quot;, alpha = 1, ...) {
    z &lt;- ensym(z)
    geom_contour(data = data,
                 aes(x, y, z = !!z),
                 size = size,
                 color = color,
                 alpha = alpha,
                 ...)
}

#&#39; Make an optimal boundary layer
#&#39;
#&#39; @param data data.frame: a data grid of features `x` and `y` with a column with
#&#39; the `optimal` boundary contours
#&#39; @param breaks numeric: which contour levels of `optimal` to plot
gg_optimal &lt;- function(data, breaks = c(0), ...) {
    gg_density(data, z = optimal, breaks = breaks, ...)
}

#&#39; Make a layer of component labels for a mixture distribution with two classes
#&#39;
#&#39; @param mus list(data.frame): the means for components of each class; every row
#&#39; is a mean, each column is a coordinate
#&#39; @param classes (optional) a vector of which levels of class to plot
gg_mix_label &lt;- function(mus, classes = NULL, size = 10, ...) {
    ns &lt;- map_int(mus, nrow)
    component &lt;- do.call(c, map(ns, seq_len))
    class &lt;- do.call(c, map2(0:(length(ns) - 1), ns, rep.int))
    mu_all &lt;- do.call(rbind, mus)
    data &lt;- cbind(mu_all, component, class) %&gt;%
        set_colnames(c(&quot;x&quot;, &quot;y&quot;, &quot;component&quot;, &quot;class&quot;)) %&gt;%
        as_tibble()
    if (is.null(classes)) {
        subdata &lt;- data
    } else {
        subdata &lt;- filter(data, class %in% classes)
    }    
    list(shadowtext::geom_shadowtext(data = subdata,
                                     mapping = aes(x, y,
                                                   label = component,
                                                   color = factor(class)),
                                     size = size,
                                     ...),
         scale_colour_discrete(drop = TRUE,
                               limits = levels(factor(data$class))))
}

</code></pre>
<h1 id="decision-boundaries-for-continuous-features">Decision Boundaries for Continuous Features</h1>
<p>Decision boundaries are most easily visualized whenever we have <em>continuous</em> features, most especially when we have <em>two</em> continuous features, because then the decision boundary will exist in a plane.</p>
<p>With two continuous features, the feature space will form a plane, and a decision boundary in this feature space is a set of one or more curves that divide the plane into distinct regions. Inside of a region, all observations will be assigned to the same class.</p>
<p>As mentioned above, whenever we know exactly how our data was generated, we can produce the optimal decision boundary. Though this won’t usually be possible in practice, investigating the optimal boundaries produced from simulated data can still help us to understand their properties.</p>
<p>We will look at the optimal boundary for a binary classification problem on a with features on a couple of common distributions: a multivariate normal distribution and a mixture of normal distributions.</p>
<h2 id="normally-distributed-features">Normally Distributed Features</h2>
<p>In a binary classification problem, whenever the features for each class jointly have a multivariate normal distribution, the optimal decision boundary is relatively simple. We will start our investigation here.</p>
<p>With two features, the feature space is a plane. It can be shown that the optimal decision boundary in this case will either be a line or a <a href="https://en.wikipedia.org/wiki/Conic_section">conic section</a> (that is, an ellipse, a parabola, or a hyperbola). With higher dimesional feature spaces, the decision boundary will form a <a href="https://en.wikipedia.org/wiki/Hyperplane">hyperplane</a> or a <a href="https://en.wikipedia.org/wiki/Quadric">quadric surface</a>.</p>
<p>We will consider classification problems with two classes, <span class="math inline">\(C = {0, 1}\)</span>, and two features, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Each class will be Bernoulli distributed and the features for each class will be distributed normally. Specifically,</p>
<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><span class="math inline">\( C \sim \operatorname{Bernoulli}(p) \)</span></td>
</tr>
<tr class="even">
<td>Features for Class 0</td>
<td><span class="math inline">\( (X, Y) \mid C = 0 \sim \operatorname{Normal}(\mu_0, \Sigma_0) \)</span></td>
</tr>
<tr class="odd">
<td>Features for Class 1</td>
<td><span class="math inline">\( (X, Y) \mid C = 1 \sim \operatorname{Normal}(\mu_0, \Sigma_1) \)</span></td>
</tr>
</tbody>
</table>
<p>Our goal is to produce two kinds of visualizations: one, of a sample from these distributions, and two, the contours of the class-conditional densities for each feature. We’ll use the <code>mvnfast</code> package to help us with computations on the joint MVN.</p>
<h3 id="samples">Samples</h3>
<p>Let’s choose some values for our parameters. We’ll start with the case when the classes occur equally often. For our features, we’ll choose means so that there is some significant overlap between the two classes, and covariance matrices so that the distributions have a nice elliptical shape.</p>
<pre class="r"><code>p &lt;- 0.5
mu_0 &lt;- c(0, 2)
sigma_0 &lt;- matrix(c(1, 0.3, 0.3, 1), nrow = 2)
mu_1 &lt;- c(2, 0)
sigma_1 &lt;- matrix(c(1, -0.3, -0.3, 1), nrow = 2)
</code></pre>
<p>Now we’ll write a function to create a dataframe containing a sample of classified features from our distribution.</p>
<pre class="r"><code>#&#39; Generate normally distributed feature samples for a binary
#&#39; classification problem
#&#39;
#&#39; @param n integer: the size of the sample
#&#39; @param mean_0 vector: the mean vector of the first class
#&#39; @param sigma_0 matrix: the 2x2 covariance matrix of the first class
#&#39; @param mean_1 vector: the mean vector of the second class
#&#39; @param sigma_1 matrix: the 2x2 covariance matrix of the second class
#&#39; @param p_0 double: the prior probability of class 0
make_mvn_sample &lt;- function(n, mu_0, sigma_0, mu_1, sigma_1, p_0) {
    n_0 &lt;- rbinom(1, n, p_0)
    n_1 &lt;- n - n_0
    sample_mvn &lt;- as_tibble(
        rbind(mvnfast::rmvn(n_0,
                            mu = mu_0,
                            sigma = sigma_0),
              mvnfast::rmvn(n_1,
                            mu = mu_1,
                            sigma = sigma_1)))
    sample_mvn[1:n_0, 3] &lt;- 0
    sample_mvn[(n_0 + 1):(n_0 + n_1), 3] &lt;- 1
    sample_mvn &lt;- sample_mvn[sample(nrow(sample_mvn)), ]
    colnames(sample_mvn) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;class&quot;)
    sample_mvn
}

</code></pre>
<p>Finally, we’ll create a sample of 4000 points and plot the result.</p>
<pre class="r"><code>n &lt;- 4000
set.seed(31415)
sample_mvn &lt;- make_mvn_sample(n,
                              mu_0, sigma_0,
                              mu_1, sigma_1,
                              p)

ggplot() +
    gg_sample(sample_mvn) +
    coord_fixed()
</code></pre>
<figure>
<img src="/images/sample_mvn.png" title="sample-mvn" alt="A sample of the feature distributions for each class." width="400" /><figcaption>A sample of the feature distributions for each class.</figcaption>
</figure>
<p>It should be apparent that because of the overlap in these distributions, any decision rule will necessarily misclassify some observations fairly often.</p>
<h3 id="classes-on-the-feature-space">Classes on the Feature Space</h3>
<p>Next, we will produce some contour plots of our feature distributions. Let’s write a function to generate class probabilities at any observation <span class="math inline">\((x, y)\)</span> in the feature space; we will model the optimal decision boundary as those points where the posterior probabilities of the two classes are equal, that is, where <span class="math display">\[ P(X, Y \mid C = 0) P(C = 0) - P(X, Y \mid C = 1) P(C = 1) = 0 \]</span></p>
<pre class="r"><code>#&#39; Make an optimal prediction at a point from two class distributions
#&#39;
#&#39; @param x vector: input
#&#39; @param p_0 double: prior probability of class 0
#&#39; @param dfun_0 function(x): density of features of class 0
#&#39; @param dfun_1 function(x): density of features of class 1
optimal_predict &lt;- function(x, p_0, dfun_0, dfun_1) {
    ## Prior probability of class 1
    p_1 &lt;- 1 - p_0
    ## Conditional probability of (x, y) given class 0
    p_x_0 &lt;- dfun_0(x)
    ## Conditional probability of (x, y) given class 1
    p_x_1 &lt;- dfun_1(x)
    ## Conditional probability of class 0 given (x, y)
    p_0_xy &lt;- p_x_0 * p_0
    ## Conditional probability of class 1 given (x, y)
    p_1_xy &lt;- p_x_1 * p_1
    optimal &lt;- p_1_xy - p_0_xy
    class &lt;- ifelse(optimal &gt; 0, 1, 0)
    result &lt;- c(p_0_xy, p_1_xy, optimal, class)
    names(result) &lt;- c(&quot;p_0_xy&quot;, &quot;p_1_xy&quot;, &quot;optimal&quot;, &quot;class&quot;)
    result
}

#&#39; Construct a dataframe with posterior class probabilities and the
#&#39; optimal decision boundary over a grid on the feature space
#&#39; 
#&#39; @param mean_0 vector: the mean vector of the first class
#&#39; @param sigma_0 matrix: the 2x2 covariance matrix of the first class
#&#39; @param mean_1 vector: the mean vector of the second class
#&#39; @param sigma_1 matrix: the 2x2 covariance matrix of the second class
#&#39; @param p_0 double: the prior probability of class 0
make_density_mvn &lt;- function(mean_0, sigma_0, mean_1, sigma_1, p_0,
                             x_min, x_max, y_min, y_max, delta = 0.05) {
    x &lt;- seq(x_min, x_max, delta)
    y &lt;- seq(y_min, y_max, delta)
    density_mvn &lt;- expand.grid(x, y)
    names(density_mvn) &lt;- c(&quot;x&quot;, &quot;y&quot;)
    dfun_0 &lt;- function(x) mvnfast::dmvn(x, mu_0, sigma_0)
    dfun_1 &lt;- function(x) mvnfast::dmvn(x, mu_1, sigma_1)
    optimal_mvn &lt;- function(x, y) optimal_predict(c(x, y), p_0, dfun_0, dfun_1)
    density_mvn &lt;-as.tibble(
        cbind(density_mvn,
              t(mapply(optimal_mvn,
                       density_mvn$x, density_mvn$y))))
    density_mvn
}

</code></pre>
<p>Now we can generate a grid of points and compute posterior class probabilities over that grid. By plotting these probabilities, we can get describe both the conditional feature distributions for each class as well as the joint feature distribution.</p>
<pre class="r"><code>density_mvn &lt;- make_density_mvn(mu_0, sigma_0, mu_1, sigma_1, p,
                                -3, 5, -3, 5)

(ggplot() +
 gg_sample(sample_mvn, alpha = 0.1) +
 gg_density(density_mvn, z = p_0_xy) +
 gg_density(density_mvn, z = p_1_xy) +
 ggtitle(&quot;Conditional Distributions&quot;)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.1) +
 geom_contour(data = density_mvn,
              aes(x = x, y = y, z = p_0_xy + p_1_xy),
              size = 1,
              color = &quot;black&quot;) +
 ggtitle(&quot;Joint Distribution&quot;))

</code></pre>
<figure>
<img src="/images/density_mvn.png" title="density-mvn" alt="Contours of the feature distributions for each class." width="800" /><figcaption>Contours of the feature distributions for each class.</figcaption>
</figure>
<h3 id="the-optimal-decision-boundary">The Optimal Decision Boundary</h3>
<p>Now let’s add a plot for the optimal decision boundary for this problem.</p>
<pre class="r"><code>(ggplot() +
 gg_density(density_mvn, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mvn, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mvn)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.25) +
 gg_optimal(density_mvn)) +
plot_annotation(&quot;The Optimal Decision Boundary&quot;)

</code></pre>
<figure>
<img src="/images/optimal_mvn.png" title="optimal-mvn" alt="The optimal decision boundary" width="800" /><figcaption>The optimal decision boundary</figcaption>
</figure>
<p>Notice how the boundary runs through the points where the contours of the two conditional distributions intersect. These points of intersection are where the classes have equal posterior probability.</p>
<h2 id="mixture-of-normals">Mixture of Normals</h2>
<p>The features of each class might also be modeled as a <em>mixture</em> of normal distributions. This means that each observation in a class will come from one of <em>several</em> normal distributions; in our case, the distributions from a class will be joined by a common hyperparameter, their mean.</p>
<p>In description, at least, the problem is still relatively simple. The possible decision boundaries produced, however, can be quite complex. This is a much more difficult problem than the one we saw before.</p>
<p>For our examples, we will generate the data as follows:</p>
<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><span class="math inline">\( C \sim Bernoulli(p) \)</span></td>
</tr>
<tr class="even">
<td>Mean of Means for Class 0</td>
<td><span class="math inline">\( \nu_0 \sim Normal((0, 1), I) \)</span></td>
</tr>
<tr class="odd">
<td>Mean of Means for Class 1</td>
<td><span class="math inline">\( \nu_0 \sim Normal((1, 0), I) \)</span></td>
</tr>
<tr class="even">
<td>Means of Components for Class 0</td>
<td><span class="math inline">\( \mu_{0, i=1, \ldots, n_0} \sim Normal(\nu_0, I) \)</span></td>
</tr>
<tr class="odd">
<td>Means of Components for Class 1</td>
<td><span class="math inline">\( \mu_{1, i=1, \ldots, n_1} \sim Normal(\nu_1, I) \)</span></td>
</tr>
<tr class="even">
<td>Features for Class 0</td>
<td><span class="math inline">\( (X, Y) \mid C = 0 \sim w_{0, 1} Normal(\mu_{0, 1}, \Sigma_0) + \cdots + w_{0, l_0} Normal(\mu_{0, 0}, \Sigma_0) \)</span></td>
</tr>
<tr class="odd">
<td>Features for Class 1</td>
<td><span class="math inline">\( (X, Y) \mid C = 1 \sim w_{1, 1} Normal(\mu_{1, 1}, \Sigma_1) + \cdots + w_{1, l_1} Normal(\mu_{1, l_1}, \Sigma_1) \)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(n_0\)</span> is the number of components for class 0, <span class="math inline">\(w_{0, i}\)</span> are the weights on each component, <span class="math inline">\(\Sigma_0 = \frac{1}{2 * l_0} I\)</span>, and <span class="math inline">\(I\)</span> is the identity matrix; similarly for class 1.</p>
<p>This is a bit awful, but we are basically doing this:</p>
<p>For each class, define the distribution of the features <span class="math inline">\((X, Y)\)</span> by</p>
<ol>
<li>Choosing the number of components to go in the mixture.</li>
<li>Choosing a mean for each component by sampling from a normal distribution.</li>
</ol>
<p>Then, to get a sample: Get an observation by</p>
<ol>
<li>Choosing a class, 0 or 1.</li>
<li>Choosing a component from that class, a normal distribution.</li>
<li>Sample the observation from that component.</li>
</ol>
<h3 id="samples-1">Samples</h3>
<p>The computations for the mixture of MVNs are fairly similar to the ones we did before. First let’s define a sampling function. This function just implements the above steps.</p>
<pre class="r"><code>#&#39; Generate normally distributed feature samples for a binary
#&#39; classification problem
#&#39;
#&#39; @param n integer: the size of the sample
#&#39; @param nu_0 numeric: the average mean of the components of the first feature
#&#39; @param sigma_0 matrix: covariance of components of the first feature
#&#39; @param n_0 integer: class frequency of first feature in the sample
#&#39; @param w_0 numeric: vector of weights for components of the first feature
#&#39; @param mean_1 numeric: the average mean of the components of the second feature
#&#39; @param sigma_1 matrix: covariance of components of the second feature
#&#39; @param n_1 integer: class frequency of second feature in the sample
#&#39; @param w_1 numeric: vector of weights for components of the second feature
#&#39; @param p_0 double: the prior probability of class 0
make_mix_sample &lt;- function(n,
                            nu_0, tau_0, n_0, sigma_0, w_0,
                            nu_1, tau_1, n_1, sigma_1, w_1,
                            p_0) {
    ## Number of Components for Each Class
    l_0 &lt;- length(w_0)
    l_1 &lt;- length(w_1)
    ## Sample the Component Means
    mu_0 &lt;- mvnfast::rmvn(n = l_0,
                          mu = nu_0, sigma = tau_0)
    mu_1 &lt;- mvnfast::rmvn(n = l_1,
                          mu = nu_1, sigma = tau_1)
    ## Class Frequency in the Sample
    n_0 &lt;- rbinom(1, n, p_0)
    n_1 &lt;- n - n_0
    ## Sample the Features
    f_0 &lt;- mvnfast::rmixn(n = n_0,
                          mu = mu_0, sigma = sigma_0, w = w_0,
                          retInd = TRUE)
    c_0 &lt;- attr(f_0, &quot;index&quot;)
    f_1 &lt;- mvnfast::rmixn(n = n_1,
                          mu = mu_1, sigma = sigma_1, w = w_1,
                          retInd = TRUE)
    c_1 &lt;- attr(f_1, &quot;index&quot;)
    sample_mix &lt;- as.data.frame(rbind(f_0, f_1))
    sample_mix[, 3] &lt;- c(c_0, c_1)
    ## Define Classes
    sample_mix[1:n_0, 4] &lt;- 0
    sample_mix[(n_0 + 1):(n_0 + n_1), 4] &lt;- 1
    sample_mix &lt;- sample_mix[sample(nrow(sample_mix)), ]
    names(sample_mix) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;component&quot;, &quot;class&quot;)
    ## Store Component Means
    attr(sample_mix, &quot;mu_0&quot;) &lt;- mu_0
    attr(sample_mix, &quot;mu_1&quot;) &lt;- mu_1
    sample_mix
}

</code></pre>
<p>Now we’ll define the parameters, construct a sample, and look at the result.</p>
<pre class="r"><code>
## Bernoulli parameter for class distribution
p = 0.5
## Mean of component means
nu_0 = c(0, 1)
nu_1 = c(1, 0)
## Covariance for component means
tau_0 = matrix(c(1, 0, 0, 1), nrow = 2)
tau_1 = matrix(c(1, 0, 0, 1), nrow = 2)
## Number of components for each class
n_0 &lt;- 10
n_1 &lt;- 10
## Covariance for each class
sigma_0 &lt;- replicate(n_0, matrix(c(1, 0, 0, 1), 2) / n_0 * 2,
                     simplify = FALSE)
sigma_1 &lt;- replicate(n_1, matrix(c(1, 0, 0, 1), 2) / n_1 * 2,
                     simplify = FALSE)
## Weights of mixture components
w_0 &lt;- rep(1 / n_0, n_0)
w_1 &lt;- rep(1 / n_1, n_1)

## Sample size
n &lt;- 4000
set.seed(31)
sample_mix &lt;- make_mix_sample(n,
                              nu_0, tau_0, n_0, sigma_0, w_0,
                              nu_1, tau_1, n_1, sigma_1, w_1,
                              p)
## Retrieve the generated component means
mu_0 &lt;- attr(sample_mix, &quot;mu_0&quot;)
mu_1 &lt;- attr(sample_mix, &quot;mu_1&quot;)

ggplot() +
    gg_sample(sample_mix) +
    ggtitle(&quot;Sample of Mixture Distribution&quot;)

ggplot() +
    gg_sample(sample_mix) +
    gg_mix_label(list(mu_0, mu_1)) +
    facet_wrap(vars(class)) +
    ggtitle(&quot;Feature Components&quot;)

</code></pre>
<figure>
<img src="/images/sample_mix.png" title="sample-mix" alt="A sample from the mixture distributions." width="400" /><figcaption>A sample from the mixture distributions.</figcaption>
</figure>
<p>We’ve labelled the component means for each class. (There are 10 components for class 0, and 10 components for class 1.) You can see that around each of these labels is a sample from a normal distribution.</p>
<h3 id="classes-on-the-feature-space-1">Classes on the Feature Space</h3>
<p>Now we’ll compute class probabilities on the feature space.</p>
<p>First define a generating function.</p>
<pre class="r"><code>#&#39; Construct a dataframe with posterior class probabilities and the
#&#39; optimal decision boundary over a grid on the feature space
#&#39; 
#&#39; @param mean_0 numeric: the average mean of the components of the first feature
#&#39; @param sigma_0 matrix: covariance of components of the first feature
#&#39; @param w_0 numeric: vector of weights for components of the first feature
#&#39; @param mean_1 numeric: the average mean of the components of the second feature
#&#39; @param sigma_1 matrix: covariance of components of the second feature
#&#39; @param w_1 numeric: vector of weights for components of the second feature
#&#39; @param p_0 double: the prior probability of class 0
make_density_mix &lt;- function(mean_0, sigma_0, w_0,
                             mean_1, sigma_1, w_1, p_0,
                             x_min, x_max, y_min, y_max, delta = 0.05) {
    x &lt;- seq(x_min, x_max, delta)
    y &lt;- seq(y_min, y_max, delta)
    density_mix &lt;- expand.grid(x, y)
    names(density_mix) &lt;- c(&quot;x&quot;, &quot;y&quot;)
    dfun_0 &lt;- function(x) mvnfast::dmixn(matrix(x, nrow = 1),
                                         mu = mean_0,
                                         sigma = sigma_0,
                                         w = w_0)
    dfun_1 &lt;- function(x) mvnfast::dmixn(matrix(x, nrow = 1),
                                         mu = mean_1,
                                         sigma = sigma_1,
                                         w = w_1)
    optimal_mix &lt;- function(x, y) optimal_predict(c(x, y), p_0, dfun_0, dfun_1)
    density_mix &lt;-as.tibble(
        cbind(density_mix,
              t(mapply(optimal_mix,
                       density_mix$x, density_mix$y))))
    density_mix
}
</code></pre>
<p>And now compute the grid and plot.</p>
<pre class="r"><code>density_mix &lt;- make_density_mix(mu_0, sigma_0, w_0, mu_1, sigma_1, w_1, p,
                                -3, 5, -3, 5)

(ggplot() +
 gg_sample(sample_mix, classes = 0,
           alpha = 0.1) +
 gg_density(density_mix, z = p_0_xy) +
 gg_mix_label(list(mu_0, mu_1), classes = 0) +
 ggtitle(&quot;Density of Class 0&quot;)) +
(ggplot() +
 gg_sample(sample_mix, classes = 1,
           alpha = 0.1) +
 gg_density(density_mix, z = p_1_xy) +
 gg_mix_label(list(mu_0, mu_1), classes = 1) +
 ggtitle(&quot;Density of Class 1&quot;)) +
(ggplot() +
 gg_sample(sample_mix,
           alpha = 0.1) +
 geom_contour(data = density_mix,
              aes(x = x, y = y, z = p_0_xy + p_1_xy),
              color = &quot;black&quot;,
              size = 1) +
 ggtitle(&quot;Joint Density&quot;))

</code></pre>
<figure>
<img src="/images/density_mix.png" title="density-mix" alt="Contours of the feature distributions for each class." width="1000" /><figcaption>Contours of the feature distributions for each class.</figcaption>
</figure>
<h1 id="the-optimal-decision-boundary-1">The Optimal Decision Boundary</h1>
<p>And here is the optimal decision boundary for this problem. Notice how again the boundary runs through points of intersection in the two conditional distributions, and how it separates the classes of observations in the sample.</p>
<pre class="r"><code>(ggplot() +
 gg_density(density_mix, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mix, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mix)) +
(ggplot() +
 gg_sample(sample_mix, alpha = 0.25) +
 gg_optimal(density_mix))
</code></pre>
<figure>
<img src="/images/optimal_mix.png" title="optimal-mix" alt="The optimal decision boundary." width="800" /><figcaption>The optimal decision boundary.</figcaption>
</figure>
<h1 id="class-imbalance">Class Imbalance</h1>
<p>So far, we’ve only seen the case where the two classes occur about equally often. If one class has a lower probability of occuring (say class 1), then the optimal decision boundary must move toward the class 1 distribution in order to equalize the probabilities on either side. This should help illustrate why it’s important to consider class imbalance whenever you’re working on a classification problem. A large imbalance can change your decisions drastically.</p>
<p>To see this change, we will use the <code>gganimate</code> package to produce an animation showing how the optimal boundary changes as the Bernoulli parameter (the frequency of class 0) changes from 0.1 to 0.9.</p>
<h2 id="normally-distributed-features-1">Normally Distributed Features</h2>
<pre class="r"><code>## Evaluate mu_0, sigma_0, etc. again, if needed.

density_p0 &lt;-
    map_dfr(seq(0.1, 0.9, 0.005),
            function(p_0)
                make_density_mvn(mu_0, sigma_0, mu_1, sigma_1,
                                 p_0, -3, 5, -3, 5) %&gt;%
                mutate(p_0 = p_0))

anim &lt;- ggplot() +
    geom_contour(data = density_p0,
                 aes(x = x, y = y, z = p_0_xy + p_1_xy),
                 color = &quot;black&quot;,
                 size = 1,
                 alpha = 0.25) +
    gg_optimal(density_p0) +
    transition_manual(p_0) +
    ggtitle(&quot;Proportion of Class 0: {current_frame}&quot;)

anim &lt;- animate(anim, renderer = gifski_renderer(),
                width = 800, height = 800)

anim
</code></pre>
<video autoplay loop mutued playsinline>
  <source src="/images/imbalance_mvn.webm" type="video/webm">
  <source src="/images/imbalance_mvn.mp4" type="video/mp4">
</video>

<h2 id="mixture-of-normals-1">Mixture of Normals</h2>
<pre class="r"><code>density_mix_p0 &lt;-
    map_dfr(seq(0.1, 0.9, 0.005),
            function(p_0)
                make_density_mix(mu_0, sigma_0, w_0, mu_1, sigma_1, w_1,
                                 p_0, -3, 5, -3, 5) %&gt;%
                mutate(p_0 = p_0))
anim &lt;- ggplot() +
    geom_contour(data = density_mix_p0,
                 aes(x = x, y = y, z = p_0_xy + p_1_xy),
                 color = &quot;black&quot;,
                 size = 1,
                 alpha = 0.25) +
    gg_optimal(density_mix_p0) +
    transition_manual(p_0) +
    ggtitle(&quot;Proportion of Class 0: {current_frame}&quot;)

anim &lt;- animate(anim, renderer = gifski_renderer(),
                width = 800, height = 800)

anim

</code></pre>
<video autoplay loop mutued playsinline>
  <source src="/images/imbalance_mix.webm" type="video/webm">
  <source src="/images/imbalance_mix.mp4" type="video/mp4">
</video>

<h1 id="conclusion">Conclusion</h1>
<p>In this post, we reviewed <strong>decision boundaries</strong>, a way of visualizing classification rules. In particular, we looked at <strong>optimal</strong> decision boundaries, which represent the <em>best</em> solution possible to a problem given certain costs for misclassification. The rule we used in this post was the <strong>MAP</strong> estimate, which minimizes zero-one loss, where all misclassifications are equally likely.</p>
<p>In future posts, we’ll look other kinds of loss functions and how that can affect the decision rule, and also at the boundaries produced by a number of statistical learning models.</p>
<p>Hope you enjoyed it!</p>]]></description>
    <pubDate>Thu, 09 Jan 2020 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/decision/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Least Squares with the Moore-Penrose Inverse</title>
    <link>https://mathformachines.com/posts/least-squares-with-the-mp-inverse/index.html</link>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>The <strong><a href="https://en.wikipedia.org/wiki/Invertible_matrix">inverse</a></strong> of a matrix <span class="math inline">\(A\)</span> is another matrix <span class="math inline">\(A^{-1}\)</span> that has this property:</p>
<span class="math display">\[\begin{align*}
A A^{-1} &amp;= I \\
A^{-1} A &amp;= I
\end{align*}
\]</span>
<p>where <span class="math inline">\(I\)</span> is the <em>identity matrix</em>. This is a nice property for a matrix to have, because then we can work with it in equations just like we might with ordinary numbers. For instance, to solve some <a href="https://en.wikipedia.org/wiki/System_of_linear_equations">linear system of equations</a> <span class="math display">\[ A x = b \]</span> we can just multiply the inverse of <span class="math inline">\(A\)</span> to both sides <span class="math display">\[ x = A^{-1} b \]</span> and then we have some unique solution vector <span class="math inline">\(x\)</span>. Again, this is just like we would do if we were trying to solve a real-number equation like <span class="math inline">\(a x = b\)</span>.</p>
<p>Now, a matrix has an inverse whenever it is square and its rows are linearly independent. But not every system of equations we might care about will give us a matrix that satisfies these properties. The coefficient matrix <span class="math inline">\(A\)</span> would fail to be invertible if the system did not have the same number of equations as unknowns (<span class="math inline">\(A\)</span> is not square), or if the system had dependent equations (<span class="math inline">\(A\)</span> has dependent rows).</p>
<p><a href="https://en.wikipedia.org/wiki/Generalized_inverse">Generalized inverses</a> are meant to solve this problem. They are meant to solve equations like <span class="math inline">\(A x = b\)</span> in the “best way possible” when <span class="math inline">\(A^{-1}\)</span> fails to exist. There are many kinds of generalized inverses, each with its own “best way.” (They can be used to solve <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">ridge regression</a> problems, for instance.)</p>
<p>The most common is the <strong><a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose inverse</a></strong>, or sometimes just the <strong>pseudoinverse</strong>. It solves the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">least-squares</a> problem for linear systems, and therefore will give us a solution <span class="math inline">\(\hat{x}\)</span> so that <span class="math inline">\(A \hat{x}\)</span> is as close as possible in ordinary <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> to the vector <span class="math inline">\(b\)</span>.</p>
<p>The notation for the Moore-Penrose inverse is <span class="math inline">\(A^+\)</span> instead of <span class="math inline">\(A^{-1}\)</span>. If <span class="math inline">\(A\)</span> is invertible, then in fact <span class="math inline">\(A^+ = A^{-1}\)</span>, and in that case the solution to the least-squares problem is the same as the ordinary solution (<span class="math inline">\(A^+ b = A^{-1} b\)</span>). So, the MP-inverse is strictly more general than the ordinary inverse: we can always use it and it will always give us the same solution as the ordinary inverse whenever the ordinary inverse exists.</p>
<p>We will look at how we can construct the Moore-Penrose inverse using the SVD. This turns out to be an easy extension to constructing the ordinary matrix inverse with the SVD. We will then see how solving a least-squares problem is just as easy as solving an ordinary equation.</p>
<h1 id="example---system-with-an-invertible-matrix">Example - System with an Invertible Matrix</h1>
<p>First let’s recall how to solve a system whose coefficient matrix is invertible. In this case, we have the same number of equations as unknowns and the equations are all independent. Then <span class="math inline">\(A^{-1}\)</span> exists and we can find a unique solution for <span class="math inline">\(x\)</span> by multiplying <span class="math inline">\(A^{-1}\)</span> on both sides.</p>
<p>For instance, say we have</p>
<p><span class="math display">\[ \left\{\begin{align*}
x_1 - \frac{1}{2}x_2 &amp;= 1 \\
-\frac{1}{2} x_1 + x_2 &amp;= -1
\end{align*}\right. \]</span></p>
<p>Then</p>
<p><span class="math display">\[ \begin{array}{c c}
A = \begin{bmatrix}
1 &amp; -1/2 \\
-1/2 &amp; 1
\end{bmatrix},
&amp;A^{-1} = \begin{bmatrix}
4/3 &amp; 2/3 \\
2/3 &amp; 4/3
\end{bmatrix} \end{array} \]</span></p>
<p>and</p>
<p><span class="math display">\[x = A^{-1}b = \begin{bmatrix}
4/3 &amp; 2/3 \\
2/3 &amp; 4/3
\end{bmatrix} \begin{bmatrix}
1 \\ 
-1
\end{bmatrix} = \begin{bmatrix}
2/3 \\
-2/3
\end{bmatrix}
\]</span></p>
<p>So <span class="math inline">\(x_1 = \frac{2}{3}\)</span> and <span class="math inline">\(x_2 = -\frac{2}{3}\)</span>.</p>
<h1 id="constructing-inverses-with-the-svd">Constructing Inverses with the SVD</h1>
<p>The <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) gives us an intuitive way constructing an inverse matrix. We will be able to see how the geometric transforms of <span class="math inline">\(A^{-1}\)</span> undo the transforms of <span class="math inline">\(A\)</span>.</p>
<p>The SVD says that for any matrix <span class="math inline">\(A\)</span>,</p>
<p><span class="math display">\[ A = U \Sigma V^* \]</span></p>
<p>where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matricies and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix.</p>
<p>Now, if <span class="math inline">\(A\)</span> is invertible, we can use its SVD to find <span class="math inline">\(A^{-1}\)</span> like so:</p>
<p><span class="math display">\[ A^{-1} = V \Sigma^{-1} U^* \]</span></p>
<p>If we have the SVD of <span class="math inline">\(A\)</span>, we can construct its inverse by swapping the orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> and finding the inverse of <span class="math inline">\(\Sigma\)</span>. Since <span class="math inline">\(\Sigma\)</span> is diagonal, we can do this by just taking reciprocals of its diagonal entries.</p>
<h2 id="example">Example</h2>
<p>Let’s look at our earlier matrix again:</p>
<p><span class="math display">\[ A = \begin{bmatrix}
1 &amp; -1/2 \\
-1/2 &amp; 1
\end{bmatrix} \]</span></p>
<p>It has SVD</p>
<p><span class="math display">\[ A = U \Sigma V^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
3/2 &amp; 0 \\
0 &amp; 1/2
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>And so,</p>
<p><span class="math display">\[ A^{-1} = V \Sigma^{-1} U^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
2/3 &amp; 0 \\
0 &amp; 2
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>and after multiplying everything out, we get</p>
<p><span class="math display">\[ A^{-1} = \begin{bmatrix}
4/3 &amp; 2/3 \\
2/3 &amp; 4/3
\end{bmatrix} \]</span></p>
<p>just like we had before.</p>
<p>In an <a href="/posts/visualizing-linear-transformations/">earlier post</a>, we saw how we could use the SVD to visualize a matrix as a sequence of geometric transformations. Here is the matrix <span class="math inline">\(A\)</span> followed by <span class="math inline">\(A^{-1}\)</span>, acting on the unit circle:</p>
<video autoplay loop mutued playsinline>
  <source src="../../images/invertible-equation.webm" type="video/webm">
  <source src="../../images/invertible-equation.mp4" type="video/mp4">
</video>

<p>The inverse matrix <span class="math inline">\(A^{-1}\)</span> reverses exactly the action of <span class="math inline">\(A\)</span>. The matrix <span class="math inline">\(A\)</span> will map any circle to a unique ellipse, with no overlap. So, <span class="math inline">\(A^{-1}\)</span> can map ellipses back to those same circles without any ambiguity. We don’t “lose information” by applying <span class="math inline">\(A\)</span>.</p>
<h1 id="constructing-mp-inverses-with-the-svd">Constructing MP-Inverses with the SVD</h1>
<p>We can in fact do basically the same thing for <em>any</em> matrix, not just the invertible ones. The SVD always exists, so for some matrix <span class="math inline">\(A\)</span>, first write</p>
<p><span class="math display">\[ A = U \Sigma V^* \]</span></p>
<p>And then find the MP-inverse by</p>
<p><span class="math display">\[ A^+ = V \Sigma^+ U^* \]</span></p>
<p>Now, the matrix <span class="math inline">\(A\)</span> might not be invertible. If it is not square, then, to find <span class="math inline">\(\Sigma^+\)</span>, we need to take the transpose of <span class="math inline">\(\Sigma\)</span> to make sure all the dimensions are conformable in the multiplication. It <span class="math inline">\(A\)</span> is singular (dependent rows), then <span class="math inline">\(\Sigma\)</span> will have 0’s on its diagaonal, so we need to make sure only take reciprocals of the non-zero entries.</p>
<p>Summarizing, to find the Moore-Penrose inverse of a matrix <span class="math inline">\(A\)</span>:</p>
<ol>
<li>Find the Singular Value Decomposition: <span class="math inline">\(A = U \Sigma V^*\)</span> (using <a href="https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/svd">R</a> or <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html">Python</a>, if you like).</li>
<li>Find <span class="math inline">\(\Sigma^+\)</span> by transposing <span class="math inline">\(\Sigma\)</span> and taking the reciprocal of all its non-zero diagonal entries.</li>
<li>Compute <span class="math inline">\(A^+ = V \Sigma^+ U^*\)</span></li>
</ol>
<h2 id="example---an-inconsistent-system">Example - An Inconsistent System</h2>
<p>Let’s find the MP-inverse of a singular matrix. Let’s take</p>
<p><span class="math display">\[A = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>Because the rows of this matrix are linearly dependent, <span class="math inline">\(A^{-1}\)</span> does not exist. But we can still find the more general MP-inverse by following the procedure above.</p>
<p>So, first we find the SVD of <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[ A = U \Sigma V^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
2 &amp; 0 \\
0 &amp; 0
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>Then we apply the procedure above to find <span class="math inline">\(A^+\)</span>:</p>
<p><span class="math display">\[ A^+ = V \Sigma^+ U^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
1/2 &amp; 0 \\
0 &amp; 0
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>And now we multiply everything out to get:</p>
<p><span class="math display">\[ A^+ = \begin{bmatrix}
1/4 &amp; 1/4 \\
1/4 &amp; 1/4 \end{bmatrix} \]</span></p>
<p>This is the Moore-Penrose inverse of <span class="math inline">\(A\)</span>.</p>
<p>Like we did for the invertible matrix before, let’s get an idea of what <span class="math inline">\(A\)</span> and <span class="math inline">\(A^+\)</span> are doing geometrically. Here they are acting on the unit circle:</p>
<video autoplay loop mutued playsinline>
  <source src="../../images/dependent-equation.webm" type="video/webm">
  <source src="../../images/dependent-equation.mp4" type="video/mp4">
</video>

<p>Notice how <span class="math inline">\(A\)</span> now collapses the circle onto a one-dimensional space. This is a consequence of it having dependent columns. For matricies with dependent columns, its image will be of lesser dimension than the space it’s mapping into. Another way of saying this is that it has a non-trivial <a href="https://en.wikipedia.org/wiki/Kernel_(linear_algebra)">null space</a>. It “zeroes out” some of the dimensions in its domain during the transformation.</p>
<p>What if <span class="math inline">\(A\)</span> were the coefficient matrix of a system of equations? We might have:</p>
<p><span class="math display">\[ \left\{ \begin{align*}
x_1 + x_2 &amp;= b_1 \\
x_1 + x_2 &amp;= b_2
\end{align*} \right. \]</span></p>
<p>for some <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>.</p>
<p>Now, unless <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are equal, this system won’t have an exact solution for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. It will be <em>inconsistent</em>. But, with <span class="math inline">\(A^+\)</span>, we can still find values for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> that minimize the distance between <span class="math inline">\(A x\)</span> and <span class="math inline">\(b\)</span>. More specifically, let <span class="math inline">\(\hat{x} = A^{+}b\)</span>. Then <span class="math inline">\(\hat{x}\)</span> will minimize <span class="math inline">\(|| b - A x ||^2  \)</span>, the <em>squared error</em>, and <span class="math inline">\( \hat{b} = A \hat{x} = A A^{+} x \)</span> is the closest we can come to <span class="math inline">\(b\)</span>. (The vector <span class="math inline">\(b - A \hat{x}\)</span> is sometimes called the <strong><a href="https://en.wikipedia.org/wiki/Residual_(numerical_analysis)">residual</a></strong> vector.)</p>
<p>We have</p>
<p><span class="math display">\[ \hat{x} = A^{+} b = \begin{bmatrix}
1/4 (b_1 + b_2) \\
1/4 (b_1 + b_2) \end{bmatrix} \]</span></p>
<p>so <span class="math inline">\(x_1 = \frac{1}{4} (b_1 + b_2)\)</span> and <span class="math inline">\(x_2 = \frac{1}{4} (b_1 + b_2)\)</span>. And the closest we can get to <span class="math inline">\(b\)</span> is</p>
<p><span class="math display">\[ \hat{b} = A \hat{x} = \begin{bmatrix}
1/2 (b_1 + b_2) \\
1/2 (b_1 + b_2) \end{bmatrix} \]</span></p>
<p>In other words, if we have to make <span class="math inline">\(x_1 + x_2\)</span> as close as possible to two different values <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>, the best we can do is to choose <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> so as to get the average of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>.</p>
<figure>
<img src="../../images/least-squares.png" title="least-squares" alt="The vector b = (1, 3) and its orthogonal projection \hat{b} = (2, 2)." width="400" /><figcaption>The vector <span class="math inline">\(b = (1, 3)\)</span> and its orthogonal projection <span class="math inline">\(\hat{b} = (2, 2)\)</span>.</figcaption>
</figure>
<p>Or we could think about this problem geometrically. In order for there to be a solution to <span class="math inline">\(A x = b\)</span>, the vector <span class="math inline">\(b\)</span> has to reside in the image of <span class="math inline">\(A\)</span>. The image of <span class="math inline">\(A\)</span> is the span of its columns, which is all vectors like <span class="math inline">\(c(1, 1)\)</span> for a scalar <span class="math inline">\(c\)</span>. This is just the line through the origin in the picture above. But <span class="math inline">\(b = (b_1, b_2)\)</span> is not on that line if <span class="math inline">\(b_1 \neq b_2\)</span>, and so instead we minimize the distance between the two with its orthogonal projection <span class="math inline">\(\hat b\)</span>. The error or residual is the difference <span class="math inline">\(\epsilon = b - \hat{b}\)</span>.</p>]]></description>
    <pubDate>Thu, 21 Nov 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/least-squares-with-the-mp-inverse/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Understanding Eigenvalues and Singular Values</title>
    <link>https://mathformachines.com/posts/eigenvalues-and-singular-values/index.html</link>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>What are eigenvalues? What are singular values? They both describe the behavior of a matrix on a certain set of vectors. The difference is this: The eigenvectors of a matrix describe the directions of its <em>invariant</em> action. The singular vectors of a matrix describe the directions of its <em>maximum</em> action. And the corresponding eigen- and singular values describe the magnitude of that action.</p>
<p>They are defined this way. A scalar <span class="math inline">\(\lambda\)</span> is an <strong><a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors">eigenvalue</a></strong> of a linear transformation <span class="math inline">\(A\)</span> if there is a vector <span class="math inline">\(v\)</span> such that <span class="math inline">\(A v = \lambda v\)</span>, and <span class="math inline">\(v\)</span> is called an <strong>eigenvector</strong> of <span class="math inline">\(\lambda\)</span>. A scalar <span class="math inline">\(\sigma\)</span> is a <strong><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value</a></strong> of <span class="math inline">\(A\)</span> if there are (unit) vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> such that <span class="math inline">\(A v = \sigma u\)</span> and <span class="math inline">\(A^* u = \sigma v\)</span>, where <span class="math inline">\(A^*\)</span> is the <a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate transpose</a> of <span class="math inline">\(A\)</span>; the vectors <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are <strong>singular vectors</strong>. The vector <span class="math inline">\(u\)</span> is called a <strong>left</strong> singular vector and <span class="math inline">\(v\)</span> a <strong>right</strong> singular vector.</p>
<h1 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h1>
<p>That eigenvectors give the directions of invariant action is obvious from the definition. The definition says that when <span class="math inline">\(A\)</span> acts on an eigenvector, it just multiplies it by a constant, the corresponding eigenvalue. In other words, when a linear transformation acts on one of its eigenvectors, it shrinks the vector or stretches it and reverses its direction if <span class="math inline">\(\lambda\)</span> is negative, but never changes the direction otherwise. The action is invariant.</p>
<p>Take this matrix, for instance:</p>
<p><span class="math display">\[ A = \begin{bmatrix}
0 &amp; 2 \\
2 &amp; 0
\end{bmatrix} \]</span></p>
<figure>
<img src="/images/eigen-circle-1.png" title="eigen-circle-1" alt="Eigenvectors of A" width="400" /><figcaption>Eigenvectors of <span class="math inline">\(A\)</span></figcaption>
</figure>
<p>We can see how the transformation just stretches the red vector by a factor of 2, while the blue vector it stretches but also reflects over the origin.</p>
<p>And this matrix:</p>
<p><span class="math display">\[ A = \begin{bmatrix}
1 &amp; \frac{1}{3} \\
\frac{4}{3} &amp; 1
\end{bmatrix} \]</span></p>
<figure>
<img src="/images/eigen-circle-2.png" title="eigen-circle-2" alt="Eigenvectors of A" width="400" /><figcaption>Eigenvectors of <span class="math inline">\(A\)</span></figcaption>
</figure>
<p>It stretches the red vector and shrinks the blue vector, but reverses neither.</p>
<p>The point is that in every case, when a matrix acts on one of its eigenvectors, the action is always in a parallel direction.</p>
<h1 id="singular-values-and-singular-vectors">Singular Values and Singular Vectors</h1>
<p>This invariant direction does not necessarily give the transformation’s direction of <em>greatest effect</em>, however. You can see that in the previous example. But say <span class="math inline">\(\sigma_1\)</span> is the <em>largest</em> singular value of <span class="math inline">\(A\)</span> with right singular vector <span class="math inline">\(v\)</span>. Then <span class="math inline">\(v\)</span> is a solution to</p>
<p><span class="math display">\[ \operatorname*{argmax}_{x, ||x||=1} ||A x|| \]</span></p>
<p>In other words, <span class="math inline">\( ||A v|| = \sigma_1 \)</span> is at least as big as <span class="math inline">\( ||A x|| \)</span> for any other unit vector <span class="math inline">\(x\)</span>. It’s not necessarily the case that <span class="math inline">\(A v\)</span> is parallel to <span class="math inline">\(v\)</span>, though.</p>
<p>Compare the eigenvectors of the matrix in the last example to its singular vectors:</p>
<figure>
<img src="/images/singular-circle-1.png" title="singular-circle-1" alt="Singular vectors of A" width="400" /><figcaption>Singular vectors of <span class="math inline">\(A\)</span></figcaption>
</figure>
<p>The directions of maximum effect will be exactly the semi-axes of the ellipse, the ellipse which is the image of the unit circle under <span class="math inline">\(A\)</span>.</p>
<p>Let’s extend this idea to 3-dimensional space to get a better idea of what’s going on. Consider this transformation:</p>
<p><span class="math display">\[A = \begin{bmatrix}
\frac{3}{2} \, \sqrt{2} &amp; -\sqrt{2} &amp; 0 \\
\frac{3}{2} \, \sqrt{2} &amp; \sqrt{2} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>This will have the effect of transforming the unit sphere into an <a href="https://en.wikipedia.org/wiki/Ellipsoid">ellipsoid</a>:</p>
<figure>
<img src="/images/transform3d-0.png" title="transform3d-0" alt="The unit sphere transformed into an ellipsoid." width="800" /><figcaption>The unit sphere transformed into an ellipsoid.</figcaption>
</figure>
<p>Its singular values are 3, 2, and 1. You can see how they again form the semi-axes of the resulting figure.</p>
<figure>
<img src="/images/transform3d-1.png" title="transform3d-1" alt="The singular vectors as semi-axes in the ellipsoid." width="800" /><figcaption>The singular vectors as semi-axes in the ellipsoid.</figcaption>
</figure>
<h1 id="matrix-approximation-with-svd">Matrix Approximation with SVD</h1>
<p>Now, the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) will tell us what <span class="math inline">\(A\)</span>’s singular values are:</p>
<p><span class="math display">\[ A = U \Sigma V^* = 
\begin{bmatrix}
\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} &amp; 0.0 \\
\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} &amp; 0.0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
3 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>The diagonal entries of the matrix <span class="math inline">\(\Sigma\)</span> are the singular values of <span class="math inline">\(A\)</span>. We can obtain a lower-dimensional approximation to <span class="math inline">\(A\)</span> by setting one or more of its singular values to 0.</p>
<p>For instance, say we set the largest singular value, 3, to 0. We then get this matrix:</p>
<p><span class="math display">\[ A_1 = \begin{bmatrix}
\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} &amp; 0.0 \\
\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} &amp; 0.0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} = \begin{bmatrix}
0 &amp; -\frac{\sqrt{2}}{2} &amp; 0 \\
0 &amp; \frac{\sqrt{2}}{2} &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>which transforms the unit sphere like this:</p>
<figure>
<img src="/images/ellipse-2.png" title="ellipse-2" alt="The transformation with the largest singular value set to 0." width="400" /><figcaption>The transformation with the largest singular value set to 0.</figcaption>
</figure>
<p>The resulting figure now lives in a 2-dimensional space. Further, the largest singular value of <span class="math inline">\(A_1\)</span> is now 2. Set it to 0:</p>
<p><span class="math display">\[ A_2 = \begin{bmatrix}
\frac{\sqrt{2}}{2} &amp; -\frac{\sqrt{2}}{2} &amp; 0.0 \\
\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} &amp; 0.0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} = \begin{bmatrix}
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{bmatrix} \]</span></p>
<p>And we get a 1-dimensional figure, and a final largest singular value of 1:</p>
<figure>
<img src="/images/ellipse-1.png" title="ellipse-1" alt="The transformation with the two largest singular values set to 0." width="400" /><figcaption>The transformation with the two largest singular values set to 0.</figcaption>
</figure>
<p>This is the point: Each set of singular vectors will form an <a href="https://en.wikipedia.org/wiki/Orthonormal_basis">orthonormal basis</a> for some <a href="https://en.wikipedia.org/wiki/Linear_subspace">linear subspace</a> of <span class="math inline">\(\mathbb{R}^n\)</span>. A singular value and its singular vectors give the direction of maximum action among all directions orthogonal to the singular vectors of any larger singular value.</p>
<p>This has important applications. There are many problems in statistics and machine learning that come down to finding a <a href="https://en.wikipedia.org/wiki/Low-rank_approximation">low-rank approximation</a> to some matrix at hand. <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal component analysis</a> is a problem of this kind. It says: approximate some matrix <span class="math inline">\(X\)</span> of observations with a number of its uncorrelated components of maximum variance. This problem is solved by computing its singular value decomposition and setting some of its smallest singular values to 0.</p>
<figure>
<img src="/images/approximations.png" title="approximations" alt="Low-rank approximations of A." width="1000" /><figcaption>Low-rank approximations of <span class="math inline">\(A\)</span>.</figcaption>
</figure>]]></description>
    <pubDate>Fri, 15 Nov 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/eigenvalues-and-singular-values/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Visualizing Linear Transformations</title>
    <link>https://mathformachines.com/posts/visualizing-linear-transformations/index.html</link>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Say <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span> are <a href="https://en.wikipedia.org/wiki/Vector_space">vector spaces</a> with scalars in some <a href="https://en.wikipedia.org/wiki/Field_(mathematics)">field</a> <span class="math inline">\(\mathbb{F}\)</span> (the real numbers, maybe). A <strong><a href="https://en.wikipedia.org/wiki/Linear_map">linear map</a></strong> is a function <span class="math inline">\(T : V \rightarrow W \)</span> satisfying two conditions:</p>
<ul>
<li><strong>additivity</strong> <span class="math inline">\(T(x + y) = T x + T y\)</span> for all <span class="math inline">\(x, y \in V\)</span></li>
<li><strong>homogeneity</strong> <span class="math inline">\(T(c x) = c (T x)\)</span> for all <span class="math inline">\(c \in \mathbb{F} \)</span> and all <span class="math inline">\(x \in V\)</span></li>
</ul>
<p>

<p>Defining a linear map this way just ensures that anything that acts like a vector in <span class="math inline">\(V\)</span> also acts like a vector in <span class="math inline">\(W\)</span> after you map it over. It means that the map preserves all the structure of a vector space after it’s applied.</p>
<p>It’s a simple definition – which is good – but doesn’t speak much to the imagination. Since linear algebra is possibly the <a href="https://math.stackexchange.com/questions/256682/why-study-linear-algebra">most useful</a> and <a href="https://math.stackexchange.com/questions/256682/why-study-linear-algebra">most ubiquitous</a> of all the branches of mathematics, we’d like to have some intuition about what linear maps are so we have some idea of what we’re doing <a href="https://en.wikipedia.org/wiki/Linear_regression">when</a> <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">we</a> <a href="https://en.wikipedia.org/wiki/Backpropagation">use</a> <a href="https://en.wikipedia.org/wiki/Mapreduce">it</a>. Though not all vectors live there, the <a href="https://en.wikipedia.org/wiki/Euclidean_space">Euclidean plane</a> <span class="math inline">\(\mathbb{R}^2\)</span> is certainly the easiest to visualize, and the way we <a href="https://en.wikipedia.org/wiki/Euclidean_distance">measure distance</a> there is very similar to the way we <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">measure error</a> in statistics, so we can feel that our intuitions will carry over.</p>
<p>It turns out that all linear maps in <span class="math inline">\(\mathbb{R}^2\)</span> can be factored into just a few primitive geometric operations: <a href="https://en.wikipedia.org/wiki/Scaling_(geometry)">scaling</a>, <a href="https://en.wikipedia.org/wiki/Rotation_(mathematics)">rotation</a>, and <a href="https://en.wikipedia.org/wiki/Reflection_(mathematics)">reflection</a>. This isn’t the only way to factor these maps, but I think it’s the easiest to understand. (We could get by <a href="https://en.wikipedia.org/wiki/Cartan%E2%80%93Dieudonn%C3%A9_theorem">without rotations</a>, in fact.)</p>
<figure>
<img src="/images/primitives.png" title="primitives" alt="The unit circle, rotated, reflected, and scaled." /><figcaption>The unit circle, rotated, reflected, and scaled.</figcaption>
</figure>
<h1 id="three-primitive-transformations">Three Primitive Transformations</h1>
<h2 id="scaling">Scaling</h2>
<p>A (non-uniform) <strong>scaling transformation</strong> <span class="math inline">\(D\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span> is given by a <a href="https://en.wikipedia.org/wiki/Diagonal_matrix">diagonal matrix</a>:</p>
<p><span class="math display">\[Scl(d1, d2) = \begin{bmatrix}
d_1 &amp; 0   \\
0   &amp; d_2 \\
\end{bmatrix}\]</span></p>
<p>where <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> are non-negative. The transformation has the effect of stretching or shrinking a vector along each coordinate axis, and, so long as <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span> are positive, it will also preserve the <a href="https://en.wikipedia.org/wiki/Orientation_(vector_space)">orientation</a> of vectors after mapping because in this case <span class="math inline">\(\det(D) = d_1 d_2 &gt; 0\)</span>.</p>
<p>For instance, here is the effect on a vector of this matrix: <span class="math display">\[D = \begin{bmatrix}
0.75 &amp; 0 \\
0    &amp; 1.25 \\
\end{bmatrix}\]</span></p>
<figure>
<img src="/images/vector-scaled.png" title="vector-scaled" alt="A vector, scaled." width="400" /><figcaption>A vector, scaled.</figcaption>
</figure>
<p>It will shrink a vector by a factor of 0.75 along the x-axis and stretch a vector by a factor of 1.25 along the y-axis.</p>
<p>If we think about all the vectors of length 1 as being the points of the <a href="https://en.wikipedia.org/wiki/Unit_circle">unit circle</a>, then we can get an idea of how the transformation will affect any vector. We can see a scaling as a continous transformation beginning at the <a href="https://en.wikipedia.org/wiki/Identity_matrix">identity matrix</a>.</p>
<video autoplay loop mutued playsinline>
  <source src="../../images/scaling.webm" type="video/webm">
  <source src="../../images/scaling.mp4" type="video/mp4">
</video>

<p>If one of the diagonal entries is 0, then it will collapse the circle on the other axis.</p>
<p><span class="math display">\[D = \begin{bmatrix}
0 &amp; 0 \\
0 &amp; 1.25 \\
\end{bmatrix}\]</span></p>
<p>This is an example of a <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)">rank-deficient</a> matrix. It maps every vector onto the y-axis, and so its image has a dimension less than the dimension of the full space.</p>
<video autoplay loop mutued playsinline>
  <source src="../../images/collapsed.webm" type="video/webm">
  <source src="../../images/collapsed.mp4" type="video/mp4">
</video>

<h2 id="rotation">Rotation</h2>
<p>A <strong>rotation transformation</strong> <span class="math inline">\(Ref\)</span> is given by a matrix: <span class="math display">\[Ref(\theta) = \begin{bmatrix}
\cos(\theta) &amp; -\sin(\theta) \\
\sin(\theta) &amp; \cos(\theta) \\
\end{bmatrix}\]</span></p>
<p>This transformation will have the effect of rotating a vector counter-clockwise by an angle <span class="math inline">\(\theta\)</span>, when <span class="math inline">\(\theta\)</span> is positive, and clockwise by <span class="math inline">\(\theta\)</span> when <span class="math inline">\(\theta\)</span> is negative.</p>
<figure>
<img src="/images/vector-rotated.png" title="vector-rotated" alt="A vector, rotated by 3\pi/4" width="400" /><figcaption>A vector, rotated by <span class="math inline">\(3\pi/4\)</span></figcaption>
</figure>
<p>And the unit circle gets mapped onto itself.</p>
<video autoplay loop mutued playsinline>
  <source src="../../images/rotation.webm" type="video/webm">
  <source src="../../images/rotation.mp4" type="video/mp4">
</video>

<p>It shouldn’t be too hard to convince ourselves that the matrix we’ve written down is the one we want. Take some unit vector and write its coordinates like <span class="math inline">\((\cos\gamma, \sin\gamma)\)</span>. Multiply it by <span class="math inline">\(Ref(\theta)\)</span> to get <span class="math inline">\((\cos\gamma \cos\theta - \sin\gamma \sin\theta, \cos\gamma \sin\theta + \sin\gamma \cos\theta)\)</span>. But by a <a href="https://en.wikipedia.org/wiki/List_of_trigonometric_identities#Angle_sum_and_difference_identities">trigonometric identity</a>, this is exactly the vector <span class="math inline">\((\cos(\gamma + \theta), \sin(\gamma + \theta))\)</span>, which is our vector rotated by <span class="math inline">\(\theta\)</span>.</p>
<p>A rotation should preserve not only orientations, but also distances. Now, recall that the determinant for a <span class="math inline">\(2\times 2\)</span> matrix <span class="math inline">\(\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\)</span> is <span class="math inline">\(a d - b c\)</span>. So a rotation matrix will have determinant <span class="math inline">\(\cos^2(\theta) + \sin^2(\theta)\)</span>, which, by the <a href="https://en.wikipedia.org/wiki/Pythagorean_trigonometric_identity">Pythagorean identity</a>, is equal to 1. This, together with the fact that its columns are <a href="https://en.wikipedia.org/wiki/Orthonormality">orthonormal</a> means that it does preserve both. It is a kind of <a href="https://en.wikipedia.org/wiki/Orthogonal_matrix">orthogonal matrix</a>, which is a kind of <a href="https://en.wikipedia.org/wiki/Isometry">isometry</a>.</p>
<h2 id="reflection">Reflection</h2>
<p>A <strong>reflection</strong> in <span class="math inline">\(\mathbb{R}^2\)</span> can be described with matricies like: <span class="math display">\[Ref(\theta) = \begin{bmatrix}
\cos(2\theta) &amp; \sin(2\theta) \\
\sin(2\theta) &amp; -\cos(2\theta) \\
\end{bmatrix}\]</span> where the reflection is through a line crossing the origin and forming an angle <span class="math inline">\(\theta\)</span> with the x-axis.</p>
<figure>
<img src="/images/vector-reflected.png" title="vector-reflected" alt="A vector, reflected over a line at angle \pi/4." width="400" /><figcaption>A vector, reflected over a line at angle <span class="math inline">\(\pi/4\)</span>.</figcaption>
</figure>
<p>And the unit circle gets mapped onto itself.</p>
<video autoplay loop mutued playsinline>
  <source src="../../images/reflection.webm" type="video/webm">
  <source src="../../images/reflection.mp4" type="video/mp4">
</video>

<p>Note that the determinant of this matrix is -1, which means that it <em>reverses</em> orientation. But its columns are still orthonormal, and so it too is an isometry.</p>
<h1 id="decomposing-matricies-into-primitives">Decomposing Matricies into Primitives</h1>
<p>The <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) will factor any matrix <span class="math inline">\(A\)</span> having like this:</p>
<p><span class="math display">\[ A = U \Sigma V^* \]</span></p>
<p>We are working with real matricies, so <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> will both be orthogonal matrices. This means each of these will be either a reflection or a rotation, depending on the pattern of signs in its entries. The matrix <span class="math inline">\(\Sigma\)</span> is a diagonal matrix with non-negative entries, which means that it is a scaling transform. (The <span class="math inline">\(*\)</span> on the <span class="math inline">\(V\)</span> is the <a href="https://en.wikipedia.org/wiki/Conjugate_transpose">conjugate-transpose</a> operator, which just means ordinary <a href="https://en.wikipedia.org/wiki/Transpose">transpose</a> when <span class="math inline">\(V\)</span> doesn’t contain any imaginary entries. So, for us, <span class="math inline">\(V^* = V^\top\)</span>.) Now with the SVD we can rewrite any linear transformation as:</p>
<ol>
<li><span class="math inline">\(V^*\)</span>: Rotate/Reflect</li>
<li><span class="math inline">\(\Sigma\)</span>: Scale</li>
<li><span class="math inline">\(U\)</span>: Rotate/Reflect</li>
</ol>
<h2 id="example">Example</h2>
<p><span class="math display">\[\begin{bmatrix}
0.5 &amp; 1.5 \\
1.5 &amp; 0.5
\end{bmatrix} \approx \begin{bmatrix}
-0.707 &amp; -0.707 \\
-0.707 &amp; 0.707
\end{bmatrix} \begin{bmatrix}
2.0 &amp; 0.0 \\
0.0 &amp; 1.0
\end{bmatrix} \begin{bmatrix}
-0.707 &amp; -0.707 \\
0.707 &amp; -0.707
\end{bmatrix} \]</span></p>
<p>This turns out to be:</p>
<ol>
<li><span class="math inline">\(V^*\)</span>: Rotate clockwise by <span class="math inline">\(\theta = \frac{3 \pi}{4}\)</span>.</li>
<li><span class="math inline">\(\Sigma\)</span>: Scale x-coordinate by <span class="math inline">\(d_1 = 2\)</span> and y-coordinate by <span class="math inline">\(d_2 = 1\)</span>.</li>
<li><span class="math inline">\(U\)</span>: Reflect over the line with angle <span class="math inline">\(-\frac{3\pi}{8}\)</span>.</li>
</ol>
<video autoplay loop mutued playsinline>
  <source src="../../images/rot-scale-ref.webm" type="video/webm">
  <source src="../../images/rot-scale-ref.mp4" type="video/mp4">
</video>

<h2 id="example-1">Example</h2>
<p>And here is a <a href="https://en.wikipedia.org/wiki/Shear_mapping">shear transform</a>, represented as: rotation, scale, rotation.</p>
<span class="math display">\[\begin{bmatrix}
1.0 &amp; 1.0 \\
0.0 &amp; 1.0
\end{bmatrix} \approx \begin{bmatrix}
0.85 &amp; -0.53 \\
0.53 &amp; 0.85
\end{bmatrix} \begin{bmatrix}
1.62 &amp; 0.0 \\
0.0 &amp; 0.62
\end{bmatrix} \begin{bmatrix}
0.53 &amp; 0.85 \\
-0.85 &amp; 0.53
\end{bmatrix}
\]</span>
<video autoplay loop mutued playsinline>
  <source src="../../images/shear.webm" type="video/webm">
  <source src="../../images/shear.mp4" type="video/mp4">
</video>
]]></description>
    <pubDate>Tue, 12 Nov 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/visualizing-linear-transformations/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>What I'm Reading 1: Bayes and Means</title>
    <link>https://mathformachines.com/posts/bayes-and-means/index.html</link>
    <description><![CDATA[<h1 id="bayesian-aggregation">Bayesian Aggregation</h1>
<p>Yang, Y., &amp; Dunson, D. B., <em>Minimax Optimal Bayesian Aggregation</em> 2014 (<a href="https://arxiv.org/abs/1403.1345">arXiv</a>)</p>
<p>Say we have a number of estimators <span class="math inline">\(\hat f_1, \ldots, \hat f_K\)</span> derived from a number of models <span class="math inline">\(M_1, \ldots, M_K\)</span> for some regression problem <span class="math inline">\(Y = f(X) + \epsilon\)</span>, but, as is the nature of things when estimating with limited data, we don’t know which estimator represents the true model (assuming the true model is in our list). The Bayesian habit is to stick a prior on the uncertainty, compute posteriors probabilities, and then average across the unknown parameter using the posterior probabilities as weights. Since the posterior probabilities (call them <span class="math inline">\(\lambda_1, \ldots, \lambda_K\)</span>) have to sum to 1, we obtain a <em>convex combination</em> of our estimators <span class="math display">\[ \hat f = \sum_{1\leq i \leq K} \lambda_i \hat f_i \]</span> This is the approach of <a href="https://www.stat.colostate.edu/~jah/papers/statsci.pdf">Bayesian Model Averaging</a> (BMA). Yang <em>et al.</em> propose to find such combinations using a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet prior</a> on the weights <span class="math inline">\(\lambda_i\)</span>. If we remove the restriction that the weights sum to 1 and instead only ask that they have finite sum in absolute value, then we obtain <span class="math inline">\(\hat f\)</span> as a <em>linear combination</em> of <span class="math inline">\(\hat f_i\)</span>. The authors then place a Gamma prior on <span class="math inline">\(A = \sum_i |\lambda_i|\)</span> and a Dirichlet prior on <span class="math inline">\(\mu_i = \frac{|\lambda_i|}{A}\)</span>. In both the linear and the convex cases they show that the resulting estimator is minimax optimal in the sense that it will give the best worst-case predictions for a given number of observations, including the case where a sparsity restriction is placed on the number of estimators <span class="math inline">\(\hat f_i\)</span>; in other words, <span class="math inline">\(\hat f\)</span> converges to the true estimator as the number of observations increases with minimax optimal risk. The advantage to previous non-bayesian methods of linear or convex aggregation is that the sparsity parameter can be learned from the data. The Dirichlet convex combination gives good performance against Best Model selection, Majority Voting, and <a href="https://biostats.bepress.com/ucbbiostat/paper266/">SuperLearner</a>, especially when there are both a large number of observations and a large number of estimators.</p>
<p>I implemented the convex case in R for use with <a href="https://github.com/paul-buerkner/brms">brms</a>. The Dirichlet distribution has been <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Gamma_distribution">reparameterized</a> as a sum of Gamma RVs to aid in sampling. The Dirichlet concentration parameter is <span class="math inline">\(\frac{\alpha}{K^\gamma}\)</span>; the authors recommend choosing <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\gamma = 2\)</span>.</p>
<pre class="r" data-org-language="R"><code>convex_regression &lt;- function(formula, data,
                              family = &quot;gaussian&quot;,
                              ## Yang (2014) recommends alpha = 1, gamma = 2
                              alpha = 1, gamma = 2,
                              verbose = 0,
                              ...) {
  if (gamma &lt;= 1) {
    warning(paste(&quot;Parameter gamma should be greater than 1. Given:&quot;, gamma))
  }
  if (alpha &lt;= 0) {
    warning(paste(&quot;Parameter alpha should be greater than 0. Given:&quot;, alpha))
  }
  ## Set up priors.
  K &lt;- length(terms(formula))
  alpha_K &lt;- alpha / (K^gamma)
  stanvars &lt;-
    stanvar(alpha_K,
      &quot;alpha_K&quot;,
      block = &quot;data&quot;,
      scode = &quot;  real&lt;lower = 0&gt; alpha_K;  // dirichlet parameter&quot;
    ) +
    stanvar(
      name = &quot;b_raw&quot;,
      block = &quot;parameters&quot;,
      scode = &quot;  vector&lt;lower = 0&gt;[K] b_raw; &quot;
    ) +
    stanvar(
      name = &quot;b&quot;,
      block = &quot;tparameters&quot;,
      scode = &quot;  vector[K] b = b_raw / sum(b_raw);&quot;
    )
  prior &lt;- prior(&quot;target += gamma_lpdf(b_raw | alpha_K, 1)&quot;,
    class = &quot;b_raw&quot;, check = FALSE
  )
  f &lt;- update.formula(formula, . ~ . - 1)
  if (verbose &gt; 0) {
    make_stancode(f,
      prior = prior,
      data = data,
      stanvars = stanvars
    ) %&gt;% message()
  }
  fit_dir &lt;- brm(f,
    prior = prior,
    family = family,
    data = data,
    stanvars = stanvars,
    ...
  )
  fit_dir
}
</code></pre>
<p>Here is a <a href="https://gist.github.com/ryanholbrook/b5c7d44c0c7642eeee1a3034b48f29d7">gist</a> that includes an interface to <a href="https://tidymodels.github.io/parsnip/">parsnip</a>.</p>
<p>In my own experiments, I found the performance of the convex aggregator to be comparable to a <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a> SuperLearner at the cost of the lengthier training that goes with MCMC methods and the finicky convergence of sparse priors. So I would likely reserve this for when I had lots of features and lots of estimators to work through, where I presume it would show an advantage. But in that case it would definitely be on my list of things to try.</p>
<h1 id="bayesian-stacking">Bayesian Stacking</h1>
<p>Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A., <em>Using Stacking to Average Bayesian Predictive Distributions</em> (<a href="https://projecteuclid.org/euclid.ba/1516093227">pdf</a>)</p>
<p>Another approach to model combination is <a href="https://doi.org/10.1080/01621459.1996.10476733">stacking</a>. With stacking, model weights are chosen by cross-validation to minimize <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSE</a> predictive error. Now, BMA finds the aggregated model that best fits the data, while stacking finds the aggregated model that gives the best predictions. Stacking therefore is usually better when predictions are what you want. A drawback is that stacking produces models through <em>point</em> estimates. So, they don’t give you all the information of a full distribution like BMA would. Yao <em>et al.</em> propose a method of stacking that instead finds the optimal <a href="https://en.wikipedia.org/wiki/Posterior_predictive_distribution">predictive distribution</a> by convex combinations of distributions with weights chosen by some scoring rule: the authors use the minimization of KL-divergence. Hence, they choose weights <span class="math inline">\(w\)</span> empirically through <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation">LOO</a> by <span class="math display">\[ \max_w \frac{1}{n} \sum_{1\leq i \leq n} \log \sum_{1\leq k \leq K} w_k p(y_i | y_{-i}, M_k) \]</span> where <span class="math inline">\(y_1, \ldots, y_n\)</span> are the observed data and <span class="math inline">\(y_{-i}\)</span> is the data with <span class="math inline">\(y_i\)</span> left out. The following figure shows how stacking of predictive distributions gives the “best of both worlds” for BMA and point prediction stacking.</p>
<figure>
<img src="/images/stacking.png" alt="From Yao (2018)" /><figcaption>From Yao (2018)</figcaption>
</figure>
<p>They have implemented stacking for <a href="https://mc-stan.org/users/interfaces/rstan">Stan</a> models in the R package <a href="https://cran.r-project.org/web/packages/loo/vignettes/loo2-weights.html">loo</a>.</p>]]></description>
    <pubDate>Fri, 04 Oct 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/bayes-and-means/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>investmentsim - an R Package for Simulating Investment Portfolios</title>
    <link>https://mathformachines.com/posts/investmentsim/index.html</link>
    <description><![CDATA[<p>I wrote a little package recently for a project I’ve been working on. I’ve mostly been using it to help out with Monte Carlo simulations for personal finance planning. It’s a little rough at the moment, but for the adventurous it’s on Github here: <a href="https://github.com/ryanholbrook/investmentsim">investmentsim</a>. And here’s a quick tutorial on how to use it.</p>
<p>The <code>investmentsim</code> package implements a function <code>make_path</code> to simulate an investment portfolio. It supports time-varying allocation of assets, automatic rebalancing, and planned transactions. The purpose of the package is to backtest investment plans as one might do for retirement accounts. (It does not have support for taxes or fees.)</p>
<p>This example will demonstrate how to create an investment portfolio with defined allocations and transactions, and then simulate the balance of the portfolio over a period of time.</p>
<pre class="r"><code>library(tidyverse)
library(xts)
library(lubridate)
library(investmentsim)</code></pre>
<p>First let’s create a portfolio. The <code>simreturns</code> data contains an <code>xts</code> time-series with fictional yearly returns for a stock fund and a bond fund over the years 1928 to 2018.</p>
<pre class="r"><code>data(simreturns)
head(simreturns)
#&gt;            Stock.Returns Bond.Returns
#&gt; 1928-01-01    0.11867241   0.01866146
#&gt; 1929-01-01    0.04008497   0.02362385
#&gt; 1930-01-01    0.16592113   0.04912787
#&gt; 1931-01-01    0.18508859  -0.03370055
#&gt; 1932-01-01    0.05509245   0.06772749
#&gt; 1933-01-01    0.07558251   0.04195868</code></pre>
<p>An <code>asset</code> in the <code>investmentsim</code> package is a function with parameters <code>start</code> and <code>end</code> that returns the percent change in the asset over the dates from <code>start</code> to <code>end</code>. The <code>make_historical</code> function will construct an asset given a time-series of returns. This function is supposed to be used when you want to use predetermined data as opposed to something generated at runtime.</p>
<pre class="r"><code>simstock_asset &lt;- make_historical(simreturns$Stock.Returns)
simbond_asset &lt;- make_historical(simreturns$Bond.Returns)</code></pre>
<p>Next we define a portfolio with the <code>make_portfolio</code> function. It takes a list of names for the assets together with the functions defining them and a list for their initial balances. Also, let’s define a sequences of dates over which we’ll run the simulation.</p>
<pre class="r"><code>asset_names &lt;- c(&quot;Stocks&quot;, &quot;Bonds&quot;)
port &lt;- make_portfolio(asset_names,
                       c(simstock_asset,
                         simbond_asset),
                       c(2500, 2500))
dates &lt;- seq(ymd(&quot;1940-01-01&quot;), ymd(&quot;2010-01-01&quot;), by=&quot;years&quot;)</code></pre>
<p>Then we can define our desired allocations with <code>make_linear_allocation</code>. It needs a list of dates and also a list of percentages for each asset.</p>
<pre class="r"><code>alloc &lt;- make_linear_allocation_path(asset_names,
                                     c(ymd(&quot;1970-01-01&quot;),
                                       ymd(&quot;2000-01-01&quot;)),
                                     list(c(0.9, 0.1),
                                          c(0.4, 0.6)))</code></pre>
<p>It’s easiest to see how it works by looking at a graph.</p>
<pre class="r"><code>as &lt;- map(dates,
          alloc) %&gt;%
    do.call(rbind, .) %&gt;%
    xts(order.by = dates)

plot(as, ylim = c(0, 1),
     col = c(&quot;red&quot;, &quot;blue&quot;),
     main = &quot;Asset Allocation&quot;)
addLegend(&quot;topright&quot;,
          asset_names,
          col = c(&quot;red&quot;, &quot;blue&quot;),
          lty = 1, cex = 1,
          bty = &quot;o&quot;)</code></pre>
<figure>
<img src="/images/allocation.png" alt="The allocation path for the portfolio." /><figcaption>The allocation path for the portfolio.</figcaption>
</figure>
<p>You can see that it is constant before the first date given and constant after the last date, and that it linearly interpolates the allocation when moving from one date to the next.</p>
<p>Finally, we can define our desired transactions and collect everything together in a model. The <code>make_transactions_on_dates</code> function does what it sounds like it does: defines for the model a specified deposit (a positive value) or a specified withdrawal (a negative value). Within the simulation, transactions are applied at the end of the years given. So this transaction path just makes a $1000 deposit at the end of each year.</p>
<pre class="r"><code>trans &lt;- make_transactions_on_dates(rep(1000, length(dates)),
                                    dates)
model &lt;- make_model(port, alloc, trans, dates)</code></pre>
<p>Lastly, we evaluate <code>make_path</code> on the model to run the simulation.</p>
<pre class="r"><code>path &lt;- make_path(model)
c(head(path), tail(path))
#&gt;                  Stocks        Bonds        Total Transaction
#&gt; 1940-01-01     2500.000 2.500000e+03     5000.000           0
#&gt; 1941-01-01     6090.672 6.767413e+02     6767.413        1000
#&gt; 1942-01-01     7606.609 8.451788e+02     8451.788        1000
#&gt; 1943-01-01     7997.775 8.886416e+02     8886.416        1000
#&gt; 1944-01-01    11848.487 1.316499e+03    13164.986        1000
#&gt; 1945-01-01    13939.015 1.548779e+03    15487.794        1000
#&gt; 2005-01-01 11137858.729 1.670679e+07 27844646.822        1000
#&gt; 2006-01-01 12831289.074 1.924693e+07 32078222.685        1000
#&gt; 2007-01-01 14673102.513 2.200965e+07 36682756.282        1000
#&gt; 2008-01-01 16844539.341 2.526681e+07 42111348.352        1000
#&gt; 2009-01-01 16949487.079 2.542423e+07 42373717.697        1000
#&gt; 2010-01-01 20340375.373 3.051056e+07 50850938.433        1000</code></pre>
<pre class="r"><code>plot(path[,1:3],
     col = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
     main = &quot;Investment Path&quot;)
addLegend(&quot;topleft&quot;,
          c(asset_names, &quot;Total&quot;),
          col = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;),
          lty = 1, cex = 1,
          bty = &quot;o&quot;)</code></pre>
<figure>
<img src="/images/path.png" alt="The value of the portfolio over time." /><figcaption>The value of the portfolio over time.</figcaption>
</figure>
<p>We’re rich!</p>]]></description>
    <pubDate>Wed, 11 Sep 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/investmentsim/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Talk: An Introduction to Categories with Haskell and Databases</title>
    <link>https://mathformachines.com/posts/introduction-to-categories/index.html</link>
    <description><![CDATA[<p>About a month ago, I gave an introductory talk on some applications of category theory. I tried to draw connections between the category of types in Haskell and the use of categories in <a href="https://www.categoricaldata.net/">CQL</a>, a new query language founded on category theory.</p>
<p><strong>Description:</strong> Category theory is the language of structure and composition. It is the language of composeable and coherent systems. It has been applied to neural networks and chemical networks, classical mechanics and quantum. It is pervasive in functional programming. If you’ve ever used a functor or a monad or a parametric type, you have used category theory. Recently, it’s been applied to database design as well.</p>
<p>This talk will be an introduction to category theory through Haskell and database programming. We will look at how similarities between the two can be expressed in categories, and how the benefits of safety and abstraction that functional programmers enjoy can be had by database users, too.</p>
<div class="video-container"><iframe src="https://www.youtube-nocookie.com/embed/sJrVKZULiLU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div>
]]></description>
    <pubDate>Sat, 22 Jun 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/introduction-to-categories/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>A Somewhat Better Retirement Formula</title>
    <link>https://mathformachines.com/posts/retirement-formula/index.html</link>
    <description><![CDATA[<p><em>This is based off a lesson I put together for a class I teach about general mathematics. I wanted a retirement savings formula that was simple enough for an ordinary person to use on their own, but also flexible enough to account for varied goals or circumstances.</em></p>
<p><em>My formula <a href="https://www.investopedia.com/articles/personal-finance/092414/retirement-what-percentage-salary-save.asp">agrees pretty well</a> with what appear to be experts recommend, and it seems to be fairly robust in simulation. <a href="https://www.portfoliovisualizer.com/financial-goals?s=y&amp;stages=2&amp;careerYears=41&amp;mode=1&amp;initialAmount=12000&amp;years=70&amp;simulationModel=1&amp;historicalVolatility=true&amp;fullHistory=true&amp;startYear=1972&amp;endYear=2018&amp;bootstrapModel=1&amp;bootstrapMinYears=1&amp;bootstrapMaxYears=20&amp;circularBootstrap=true&amp;distribution=1&amp;dof=30&amp;meanReturn=7.0&amp;volatility=12.0&amp;sequenceStressTest=0&amp;stressTestRetirement=true&amp;inflationModel=1&amp;inflationMean=4.26&amp;inflationVolatility=3.13&amp;customIntervals=false&amp;percentileList=10%2C+25%2C+50%2C+75%2C+90&amp;returnList=0%2C+2.5%2C+5%2C+7.5%2C+10%2C+12.5&amp;asset1=TotalStockMarket&amp;allocation1_1=90&amp;asset2=TotalBond&amp;allocation2_1=10&amp;total1=100&amp;endasset1=TotalStockMarket&amp;endallocation1_1=30&amp;endasset2=TotalBond&amp;endallocation2_1=50&amp;endasset3=ShortTreasury&amp;endallocation3_1=20&amp;endtotal1=100&amp;cfname1=Saving&amp;cftype1=1&amp;cfamount1=12000&amp;cfinfadj1=true&amp;__checkbox_cfinfadj1=true&amp;cfstart1=1&amp;cffrequency1=4&amp;cfoccurs1=1&amp;cfname2=Retirement&amp;cftype2=2&amp;cfamount2=48000&amp;cfinfadj2=true&amp;__checkbox_cfinfadj2=true&amp;cfstart2=2&amp;cffrequency2=4&amp;cfoccurs2=2&amp;cftype3=2&amp;__checkbox_cfinfadj3=true&amp;cfstart3=3&amp;cffrequency3=4&amp;cfoccurs3=3">This simulation</a> <a href="/files/WithSS.pdf">(pdf)</a> based on historical data, says that a plan like this would have succeeded about 97% of the time with typical investments, and <a href="https://www.portfoliovisualizer.com/financial-goals?s=y&amp;stages=2&amp;careerYears=41&amp;mode=1&amp;initialAmount=12000&amp;years=70&amp;simulationModel=1&amp;historicalVolatility=true&amp;fullHistory=true&amp;startYear=1972&amp;endYear=2018&amp;bootstrapModel=1&amp;bootstrapMinYears=1&amp;bootstrapMaxYears=20&amp;circularBootstrap=true&amp;distribution=1&amp;dof=30&amp;meanReturn=7.0&amp;volatility=12.0&amp;sequenceStressTest=0&amp;stressTestRetirement=true&amp;inflationModel=1&amp;inflationMean=4.26&amp;inflationVolatility=3.13&amp;customIntervals=false&amp;percentileList=10%2C+25%2C+50%2C+75%2C+90&amp;returnList=0%2C+2.5%2C+5%2C+7.5%2C+10%2C+12.5&amp;asset1=TotalStockMarket&amp;allocation1_1=90&amp;asset2=TotalBond&amp;allocation2_1=10&amp;total1=100&amp;endasset1=TotalStockMarket&amp;endallocation1_1=30&amp;endasset2=TotalBond&amp;endallocation2_1=50&amp;endasset3=ShortTreasury&amp;endallocation3_1=20&amp;endtotal1=100&amp;cfname1=Saving&amp;cftype1=1&amp;cfamount1=12000&amp;cfinfadj1=true&amp;__checkbox_cfinfadj1=true&amp;cfstart1=1&amp;cffrequency1=4&amp;cfoccurs1=1&amp;cfname2=Retirement&amp;cftype2=2&amp;cfamount2=80000&amp;cfinfadj2=true&amp;__checkbox_cfinfadj2=true&amp;cfstart2=2&amp;cffrequency2=4&amp;cfoccurs2=2&amp;cftype3=2&amp;__checkbox_cfinfadj3=true&amp;cfstart3=3&amp;cffrequency3=4&amp;cfoccurs3=3">even lacking social security</a> <a href="/files/WithoutSS.pdf">(pdf)</a> would have succeeded about 76% of the time. A warning: I however am not a financial expert, so caveat emptor.</em></p>
<p>The biggest financial problem in anyone’s life is how to provide for oneself in retirement. If you retire in your 60’s, you have a good chance of living for another several decades. Even someone used to living on a modest income of $40,000 a year, could need over half a million dollars saved up to keep from running out of money. “Half a million dollars‽” you cry. “I can barely afford my student loans!” Fortunately, by taking advantage of the <a href="https://www.snopes.com/fact-check/compound-interest/">most powerful force in the universe</a> it’s easier than you think, especially if you get started early.</p>
<h1 id="the-formula">The Formula</h1>
<p>After thinking about it really hard for a while, you come up with the following goal: “I want to retire at 67 with enough savings to live for 20 years at 80% of my usual income.” What can you do to have a fair chance of meeting this goal? How much you need to save depends most of all on what age you start saving at. Under some reasonable assumptions, the percent <span class="math inline">\(p\)</span> of your income you would need to save if you started saving at age <span class="math inline">\(A\)</span> would be: <span class="math display">\[\formbox{p = 12\times0.8\left(\frac{0.03}{1.03^N - 1}\right)}\]</span> where <span class="math inline">\(N=67-A\)</span>. If you wanted to type it in to a <a href="https://en.wikipedia.org/wiki/TI-30">calculator</a>, the keypresses would likely be something like this:</p>
<pre><code>12 * 0.8 * 0.03 ÷ ( 1.03 ^ N - 1 )
</code></pre>
<p>Or you could use the fancy Javascript calculator <em>below</em>.</p>
<h2 id="example">Example</h2>
<p>For instance, say you start saving at 25. Then, assuming no prolonged periods of unemployment, you would be saving for <span class="math inline">\(N=67-25=42\)</span> years. Plugging this in to the formula, we get</p>
<p><span class="math display">\[
p = 12 \times 0.8 \left(\frac{0.03}{1.03^{42} - 1}\right) = 0.117
\]</span></p>
<p>So you would need to put about 12% of your paycheck into some kind of <a href="https://investor.vanguard.com/mutual-funds/target-retirement/">retirement fund</a> in order to meet your goal. If you make $50,000 a year, 12% of your income comes to $6000 a year, or $500 a month. Many employers, however, will offer to <a href="https://www.investopedia.com/articles/personal-finance/120315/what-good-401k-match.asp">match</a> a portion of your contributions. If your employer matches[<sup>1</sup>] 3%, then you would only need to contribute the other 9%. At $50,000 a year, this is a contribution of $375 a month.</p>
<p>[<sup>1</sup>]: Unless it is a matter of putting a roof over your head or food on the table, you should always contribute at least up to your employer match. This is an investment with a guaranteed 100% return. You’ll never do better.</p>
<h1 id="the-importance-of-starting-early">The Importance of Starting Early</h1>
<p>Consider this table:</p>
<table>
<thead>
<tr class="header">
<th>Age</th>
<th>% Needed</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>25</td>
<td>12%</td>
</tr>
<tr class="even">
<td>30</td>
<td>15%</td>
</tr>
<tr class="odd">
<td>35</td>
<td>18%</td>
</tr>
<tr class="even">
<td>40</td>
<td>24%</td>
</tr>
<tr class="odd">
<td>45</td>
<td>31%</td>
</tr>
<tr class="even">
<td>50</td>
<td>44%</td>
</tr>
<tr class="odd">
<td>55</td>
<td>68%</td>
</tr>
</tbody>
</table>
<p>If you put your contributions off until you’re 45, you would have to put back over a third of your income to maintain the lifestyle that you’re used to. Realistically, you’re going to have to make some adjustments. If you wait until you’re 55, things are going to be painful.</p>
<h1 id="assumptions">Assumptions</h1>
<p>Let’s take a look at what assumptions went into our formula and the reasons for them. Feel free to play around with the numbers, but let me try to explain why I think these are good defaults.</p>
<p>We’re assuming that you will retire at age 67, that you will need 12 years of savings in retirement, that you will be living on 80% of your working income, and that while you are working your investments will earn a 3% annual return (adjusted for inflation).</p>
<p>The goal in saving for retirement is to not have to worry about money after you’re not able to earn it any more. The penalty for failing to meet this goal is much greater than the sacrifice needed to achieve it. It is much worse to have $2500 less a month in retirement than it is to have $500 less a month while working. Failure means having to make decisions like whether you’ll pay for your medicine, buy groceries, or keep the lights on once your Social Security check comes in. We want a low chance of failure. These assumptions reflect that goal.</p>
<h2 id="retire-at-67">Retire at 67</h2>
<p>In the United States, 67 is the age when a person is <a href="https://www.ssa.gov/planners/retire/agereduction.html">eligible</a> for full Social Security benefits. <a href="https://dqydj.com/average-retirement-age-in-the-united-states/">These data</a> show that healthy people usually decide to work until 70, when you get enhanced benefits, while unhealthy people retire as soon as they can, at 62, if they can hold out for even that long. Plan for a healthy old age, but consider <a href="https://www.investopedia.com/terms/d/disability-insurance.asp">disability insurance</a>.</p>
<h2 id="years-of-savings">12 years of savings</h2>
<p>The current <a href="https://www.ssa.gov/oact/STATS/table4c6.html">life expectancy</a> for someone who makes it to age 67 is about 20 years more. Social Security, in its current state, will pay for about 8 years worth of that,[<sup>2</sup>] so you, the retiree, will have to come up with the other 12. If you want to plan for a retirement with <a href="https://www.ssa.gov/policy/docs/ssb/v70n3/v70n3p111.html">reduced benefits</a>, or if you are very averse to the risk of living to an advanced age while being very poor, or if you want to retire early, add on a few years. If you plan on dying before you reach 67, congratulations! You’re off the hook.</p>
<p>[<sup>2</sup>]: I don’t mean to say that Social Security pays out for eight years and then stops. I mean that it will pay for around 8/20 = 40% of your expenses during those 20 years.</p>
<h2 id="of-working-income">80% of working income</h2>
<p>The percentage of your income that you will need to maintain your lifestyle in retirement is called your <em>replacement ratio</em>. Generally, it will be less than your working income. Why? Though some costs will have increased (medical, perpetually), typically your financial obligations will be fewer: you don’t have to save money anymore (can’t take it with you), the kids have moved out (let us pray), the mortgage is paid off (at long last)… <a href="https://personal.vanguard.com/pdf/ISGRR.pdf">This study</a> claims your replacement ratio will usually be between 70% to 85%, depending on circumstances. If you plan on being rich and in good health, you may wish to choose the lower number; if you plan on being poor and in bad health, you may wish to choose the higher. In either case, 80% seems a cautious default.</p>
<h2 id="annual-return">3% annual return</h2>
<p><em>Question:</em> The stock market has historically had about a 7% inflation adjusted return. Even if you put half your money in bonds, you could still get well above 3% annually. So isn’t 3% way too conservative?</p>
<p><em>Answer:</em> No. Because you, as an individual, don’t get an average return; you get whatever the market gives you. If your retirement years begin with a financial crisis followed by a prolonged recession, it doesn’t matter if the market recovers ten years later, you’ve already spent all your money and you’re not dead yet; as a variation on <a href="https://en.wikiquote.org/wiki/John_Maynard_Keynes">Keynes</a>: markets can remain depressed longer than you can remain solvent. Assuming a 3% average return will give you a much better chance of avoiding the worst scenarios, and if you start saving early, isn’t much of an additional burden.</p>
<h1 id="appendix---calculator">Appendix - Calculator</h1>
<form onsubmit="return false">
  <ul class="form-group">
    <li class="d-inline-block col-5"><label>Starting Age</label></li>
    <li class="d-inline-block col-5"><input size="3" type="text" id="startage" aria-label="startage" value="25" onchange="computePercent()" /> years</li>
  </ul>

<ul class="form-group">
  <li class="d-inline-block col-5"><label>Retirement Age</label></li>
  <li class="d-inline-block col-5"><input size="3" type="text" id="retirementage" aria-label="retirementage" value="67" onchange="computePercent()" /> years</li>
</ul>

<ul class="form-group">
  <li class="d-inline-block col-5"><label>Years of Savings Needed</label></li>
  <li class="d-inline-block col-5"><input size="3" type="text" id="yearsofsavings" aria-label="yearsofsavings" value="12" onchange="computePercent()" /> years</li>
</ul>

<ul class="form-group">
  <li class="d-inline-block col-5"><label>Percent of Working Income Needed</label></li>
  <li class="d-inline-block col-5"><input size="3" type="text" id="percentofincome" aria-label="percentofincome" value="80" onchange="computePercent()" /> %</li>
</ul>

<ul class="form-group">    
  <li class="d-inline-block col-5"><label>Rate of Return</label></li>
  <li class="d-inline-block col-5"><input size="3" type="text" id="rateofreturn" aria-label="rateofreturn" value="3" onchange="computePercent()" /> % per year</li>
</ul>

    <ul>
      <div class="text-red">
      <li class="d-inline-block col-5"><label>You Need to Save:
      <li class="d-inline-block col-5" id="percenttosave"></li>
      </div> 
    </ul>
</form>

<script src="/scripts/retirement.js"></script>
]]></description>
    <pubDate>Wed, 15 May 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/retirement-formula/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Journal Review: Permitted and Forbidden Sets in STLNs</title>
    <link>https://mathformachines.com/posts/permitted-and-forbidden-sets/index.html</link>
    <description><![CDATA[<h1 id="a-model-of-associative-memory">A Model of Associative Memory</h1>
<p>How memories are encoded in neural matter is still an open question. The name for these supposed neural correlates of memory is “engram”, and papers about engrams tend to have titles like <a href="https://jflab.ca/pdfs/josselyn-et-al-2015.pdf"><em>Finding the engram</em></a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3462696/"><em>Catching the engram</em></a>, <a href="https://psycnet.apa.org/record/1952-05966-020"><em>In search of the engram</em></a>, <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2895151/"><em>Continuing the search for the engram</em></a>, which, though I’m not an expert, makes me feel like the problem isn’t well understood.</p>
<p>(Also, <a href="https://www.ncbi.nlm.nih.gov/pubmed/15450162"><em>Rite of passage of the engram</em></a> and <a href="http://www.jneurosci.org/content/34/42/14115"><em>Manipulating a cocaine engram</em></a>, making the practice of neuroscience sometimes sound like a fraternity hazing. Possibly related, while researching this post I learned that a typical experiment will usually involve things like shocking the feet of a fruit fly, sewing shut one eye of a barn owl, and shaving half the whiskers off a mouse.)</p>
<p>A popular theory is that memories are encoded as patterns of synaptic connections. Perception creates neural activity. Neural activity leaves an impression upon the brain as a pattern of modified synaptic connections (perhaps by <a href="https://en.wikipedia.org/wiki/Dendritic_spine#Importance_to_learning_and_memory">dendritic spines</a>, which become larger and more numerous to make the connection stronger). A later perception might partly activate this pattern, but this partial activation is often enough to activate the rest of the pattern, too. This is supposed to be a neural model of associative memory. (The tradition is to cite <a href="https://en.wikipedia.org/wiki/In_Search_of_Lost_Time#Memory">Proust</a> at this point; evidently, a <a href="https://en.wikipedia.org/wiki/Madeleine_(cake)">sponge cake</a> was sufficient to activate in him the neural substrate of a <a href="https://en.wikipedia.org/wiki/List_of_longest_novels">1,267,069</a> word novel. It’s remarkably illustrative, at least.)</p>
<p>Artificial neural networks are often used to model the networks of the brain. <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Feedforward networks</a> have typically been used to model the visual system, while <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">recurrent networks</a> have more often been used to model memory. When an input is applied to certain of these recurrent networks, the neural activity will always converge to a stable <a href="https://en.wikipedia.org/wiki/Steady_state">steady state</a>. This stable pattern of activity is supposed to be a memory, stored within the connections of the network.</p>
<p>Some of the most studied networks are those that are <em>symmetrically</em> connected, like the <a href="https://en.wikipedia.org/wiki/Hopfield_network">Hopfield network</a>. A network is symmetrically connected if every neuron is connected with the same weight as whatever is connected to it. A symmetrically connected network with a <em>linear</em> <a href="https://en.wikipedia.org/wiki/Activation_function">activation function</a> can, for a given set of connection weights, be activated only to a <em>single</em> stable steady state (whose values depend upon the input to the network). The drawback of these networks then is that the activity at future states will be independent of the activity at past states. Past recall cannot influence future recall.</p>
<p><a href="https://papers.nips.cc/paper/1793-permitted-and-forbidden-sets-in-symmetric-threshold-linear-networks.pdf">Hahnloser and Seung</a> present a model of associative memory in symmetrically connected networks using instead a <em>threshold-linear</em> activation function (or <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">rectified linear</a> function).</p>
<figure class="floatright"> <img src="/images/rectifier.png" alt="Graph of a rectified linear activation function." /> </figure>

<p>They show that, due to some nice properties of the rectifier, such networks can in general represent multiple patterns of stable activation even for a single input. What pattern the network will fall into upon new input, depends upon what pattern it was in before. Memories linger.</p>
<p>Their main contribution in this paper is in classifying neurons into what they call “permitted” and “forbidden” sets, which describe what sets of neurons may be activated together in a stable steady-state. They describe a method of determining what patterns of stable activity the network can achieve.</p>
<blockquote>
<p>The existence of permitted and forbidden sets suggests a new way of thinking about memory in neural networks. When an input is applied, the network must select a set of active neurons, and this selection is constrained to be one of the permitted sets. Therefore the permitted sets can be regarded as memories stored in the synaptic connections.</p>
</blockquote>
<h1 id="symmetric-threshold-linear-networks-a.k.a.-symmetric-rectifier-networks">Symmetric Threshold-Linear Networks (a.k.a. Symmetric Rectifier Networks)</h1>
<p>A threshold-linear network has the form <span class="math display">\[
\dot{x} = -x + \bigg[W x + b \bigg]_+ \tag 1 \label 1
\]</span> where <span class="math inline">\(x\)</span> is a vector with <span class="math inline">\(n\)</span> components representing neural activation, <span class="math inline">\(W\)</span> an <span class="math inline">\(n \times n\)</span> matrix representing the connection weights between neurons, <span class="math inline">\(b\)</span> is a vector representing (constant) external input, and <span class="math inline">\([\cdot]_+ = \operatorname{max}\{0, \cdot\}\)</span>, the rectifier function. Hahnloser and Seung assume the weight matrix <span class="math inline">\(W\)</span> is symmetric (meaning, neurons are connected symmetrically).</p>
<p>For a single neuron we can write <span class="math display">\[
\dot{x}_i = -x_i + \bigg[\sum_{j=1}^n w_{ij} x_j + b_i\bigg]_+ \tag 2 \label 2
\]</span> Whenever <span class="math inline">\(\sum_{j=1}^n w_{ij} x_j + b_i \leq 0\)</span> the input to the neuron is 0. Its dynamics become <span class="math inline">\(\dot x_i = -x_i\)</span> and its activation will decay exponentially to 0; it is “off”. What this means is that generally only a subset of neurons will be active at any time, and which neurons are active may change as the system evolves.</p>
<p>It helps to think about neurons as being active within “chambers” of the activation space. (These chambers can be found by considering when the expression inside <span class="math inline">\([\cdot]_+\)</span> is equal to 0.) In each chamber, some of the neurons will be active and some will be inactive. Within each chamber, the network will evolve according to that chamber’s <em>linear</em> equations: <span class="math display">\[
\dot{x} = -x + W_\sigma x + b_\sigma \tag 3 \label 3
\]</span> (Here, <span class="math inline">\(\sigma\)</span> means the set of neurons that are currently active, and <span class="math inline">\(W_\sigma\)</span> and <span class="math inline">\(b_\sigma\)</span> have entries set to 0 for those neurons not in <span class="math inline">\(\sigma\)</span>.) Whenever the system enters a new chamber, some neurons will switch on and some will switch off, and a new set of linear equations takes over. Each chamber has a set of eigenvectors given by <span class="math inline">\(W_\sigma\)</span>. These eigenvectors show straight line flows within that chamber.</p>
<p>Let’s take a look at the dynamics of a two neuron system with weight matrix <span class="math inline">\(\begin{bmatrix}0 &amp; -\frac12 \\
-\frac12 &amp; 0\end{bmatrix}\)</span>.</p>
<p>First, the rectified version. The activation space is divided into four chambers; the labels indicate which neurons are active in that chamber. Each curve represents different initialization values for the neurons; the input vector <span class="math inline">\(b\)</span> is always the same. On the right is a plot for one initialization. In this example, the network always converges to a single steady state, though in other networks there may be more than one.</p>
<img src="/images/rectified.png" alt="graphs showing dynamics of a rectifier network">

<p>Notice how the dynamics change when the system enters in innermost chamber <span class="math inline">\(\{1,2\}\)</span>. Compare this to the same system lacking the rectifier <span class="math inline">\([\cdot]_+\)</span>; it is a linear system.</p>
<img src="/images/linear.png" alt="graphs showing dynamics of a network with linear activation" />

<h1 id="three-theorems">Three Theorems</h1>
<p>The authors prove three theorems. The first gives the conditions under which a network will have a set of global, stable steady states (aka. globally asypmtotic fixed points, equilibrium points), depending on connection weights and input. These steady states, when they exist, are fixed points of activation to which the network will always converge.</p>
<p>Assuming these conditions, in the second and third theorems the authors give two possibilities for this set of steady states. The first possibility is that the network contains <em>forbidden sets</em> of neurons, neurons that may not be activated together at a steady state; in this case the network will be <em>multistable</em>: for a given input, it may converge to one of several steady states depending on initial activations. The second possibility is that there are <em>no</em> forbidden sets; in this case, for a given input, the network will always converge to the same steady state; as far as stable points go, it is just like a linear system, without the rectifier.</p>
<h2 id="theorem-1---steady-states">Theorem 1 - Steady States</h2>
<p>Again, this theorem gives the conditions under which a network may have a set of stable steady states.</p>
<p>The authors present their results in terms of the matrix <span class="math inline">\(I-W\)</span>. We can rewrite the linear system <span class="math inline">\(\ref 3\)</span> as <span class="math display">\[ \dot x = (-I + W)x + b \tag 4 \]</span> The <a href="https://en.wikipedia.org/wiki/Hurwitz_matrix#Hurwitz_stable_matrices">stability</a> of the system can be determined from the eigenvalues of the matrix <span class="math inline">\(-I + W\)</span>; specifically, the system is <a href="https://en.wikipedia.org/wiki/Lyapunov_stability">globally asymptotically stable</a> if the real parts of the matrix are all <em>negative</em>. Since <span class="math inline">\(-I + W\)</span> is symmetric and real, its eigenvalues will all be real; so, we are looking for negative eigenvalues. It is, however, usually more convenient to work with positive numbers, so instead we can look for <em>positive</em> eigenvalues of <span class="math inline">\(I - W\)</span> (or even eigenvalues of <span class="math inline">\(W\)</span> that are less than 1).</p>
<blockquote>
<p><strong>Theorem 1</strong></p>
<p>If W is symmetric, then the following conditions are equivalent:</p>
<ol>
<li>All nonnegative eigenvectors of all principal submatrices of <span class="math inline">\(I - W\)</span> have positive eigenvalues.</li>
<li>The matrix <span class="math inline">\(I - W\)</span> is copositive. That is, <span class="math inline">\(x^\top (I - W)x \gt 0\)</span> for all nonnegative <span class="math inline">\(x\)</span>, except <span class="math inline">\(x = 0\)</span>.</li>
<li>For all <span class="math inline">\(b\)</span>, the network has a nonempty set of steady states that are globally asymptotically stable.</li>
</ol>
</blockquote>
<figure class="floatright"><img src="/images/lagrange.png" alt="plot of the Lagrange function for non-negative v on the unit circle" /><figcaption>\(R(v)\) for \(\left\lVert v \right\rVert = 1\)</figcaption></figure>

<p>One of the things I liked about this paper was that they proved their results using methods from both <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyanpunov functions</a> and <a href="https://en.wikipedia.org/wiki/Quadratic_programming">quadratic programming</a>. They prove that <span class="math inline">\((1)\)</span> implies <span class="math inline">\((2)\)</span>, for instance, by minimizing <span class="math inline">\(v^\top (I - W) v\)</span> (a quadratic function) for nonnegative vectors <span class="math inline">\(v\)</span> on the unit sphere (that is, <span class="math inline">\(\left\lVert v \right\rVert = 1\)</span>). The quantity <span class="math inline">\(R(v) = v^\top (I - W) v\)</span> is equivalent to the <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotient</a>. Optimizing <span class="math inline">\(R\)</span> will find the eigenvectors of the matrix <span class="math inline">\(I - W\)</span>. Because of the rectifier, neural activations (provided they start above 0) can never fall below 0. Any steady state therefore will occur along a non-negative eigenvector. This, I think, is one of the most important insights about the effect of the rectification.</p>
<p>Here are the authors again:</p>
<blockquote>
<p>The meaning of these stability conditions is best appreciated by comparing with the analogous conditions for the purely linear network obtained by dropping the rectification from (1). In a linear network, all eigenvalues of W would have to be smaller than unity to ensure asymptotic stability. Here only nonnegative eigenvectors are able to grow without bound, due to the rectification, so that only their eigenvalues must be less than unity. All principal submatrices of W must be considered because different sets of feedback connections are active, depending on the set of neurons that are above threshold. In a linear network, <span class="math inline">\(I - W\)</span> would have to be positive definite to ensure asymptotic stability, but because of the rectification, here this condition is replaced by the weaker condition of copositivity.</p>
</blockquote>
<p>So, the tradeoff for the rectification is that we get stability for more general sets of weight matricies, but we have to analyze all <span class="math inline">\(2^n\)</span> <a href="https://en.wikipedia.org/wiki/Matrix_(mathematics)#Submatrix">principal submatrices</a> to find out if we get it.</p>
<h2 id="theorems-2-and-3---permitted-and-forbidden-sets">Theorems 2 and 3 - Permitted and Forbidden Sets</h2>
<p>These two theorems classify the permitted and forbidden sets of a network.</p>
<p>The first theorem tells us that if a network has a set of global, stable steady states, then all of the nonnegative eigenvectors of all principal submatrices of <span class="math inline">\(I-W\)</span> will have positive eigenvalues. When the system begins with positive activations, the activation will flow along time-varying superpositions of the (nonnegative) eigenvectors toward some fixed point. We might think that <em>every</em> subsystem has to have a fixed point, then. But this is not so. It could turn out that what would be the fixed point for the subsystem lies outside of its chamber, and then the dynamics will have changed before the system ever reaches it. In this case the system has a forbidden set, because the neurons in that subsystem cannot be coactivated together at a stable steady state.</p>
<blockquote>
<p><strong>Theorem 2</strong></p>
<p>If the matrix <span class="math inline">\(I - W\)</span> is copositive, then the following statements are equivalent:</p>
<ol>
<li>The matrix <span class="math inline">\(I - W\)</span> is not positive definite.</li>
<li>There exists a forbidden set.</li>
<li>The network is conditionally multistable. That is, there exists an input <span class="math inline">\(b\)</span> such that there is more than one stable steady state.</li>
</ol>
</blockquote>
<figure><img src="/images/twofp.png" alt="Plots of a three neuron system with two stable points."/><figcaption>A three neuron system with two steady states.</figcaption></figure>

<p>They prove that (2) implies (3) by examining a Lyapunov function <span class="math inline">\(V(x) = \frac12 x^\top (I - W) x - b^\top x\)</span>. They argue as follows: a forbidden set implies the existence of a negative eigenvalue of <span class="math inline">\(I - W\)</span> in the corresponding active submatrix. The function <span class="math inline">\(V\)</span> therefore forms a saddle. The system can be initially activated on either side of the saddle, and will descend to a different minimum on each side. These are two different stable steady states.</p>
<figure><img src="/images/multistable.png" alt="3D plot of Lyapunov function and a contour plot with line given by a positive eigenvector"><figcaption>The Lyapunov function for a two neuron system with connection weights equal to 2. On the right, a line in the direction of an eigenvector with positive eigenvalue is in red.</figcaption></figure>

<blockquote>
<p><strong>Theorem 3</strong> If <span class="math inline">\(W\)</span> is symmetric, then the following conditions are equivalent:</p>
<ol>
<li>The matrix <span class="math inline">\(I - W\)</span> is positive definite.</li>
<li>All sets are permitted.</li>
<li>For all <span class="math inline">\(b\)</span> there is a unique steady state, and it is stable.</li>
</ol>
</blockquote>
<p>A linear system, like <span class="math inline">\(\ref 3\)</span>, will have a global steady state if <span class="math inline">\(I-W\)</span> is positive definite (all eigenvalues are positive). So, in a rectified system if <em>all</em> the neurons may be activated together at a stable steady state, the system behaves much like a linear system in regard to its steady states. Rectified systems are more interesting when they have some forbidden sets.</p>
<p>If I am understanding the paper correctly, we could characterize permitted and forbidden sets like this:</p>
<table>
<tbody>
<tr class="odd">
<td>permitted set</td>
<td>forbidden set</td>
</tr>
<tr class="even">
<td>principal submatrix with only positive eigenvalues</td>
<td>principal submatrix with a negative eigenvalue</td>
</tr>
<tr class="odd">
<td>neurons that can be coactivated at a stable steady state</td>
<td>neurons that cannot be coactivated at a stable steady state</td>
</tr>
<tr class="even">
<td>positive eigenvectors and positive eigenvalues</td>
<td>eigenvectors with negative components that give negative eigenvalues</td>
</tr>
</tbody>
</table>
<p>Finally, they show with the <a href="https://en.wikipedia.org/wiki/Min-max_theorem#Cauchy_interlacing_theorem">interlacing theorem</a> that the sets of neurons that may be coactivated together at stable states are constant in some sense throughout the system, for the reason that eigenvalues of a submatrix have to be contained in the radius of eigenvalues of the parent matrix.</p>
<blockquote>
<p><strong>Theorem 4</strong></p>
<p>Any subset of a permitted set is permitted. Any superset of a forbidden set is forbidden.</p>
</blockquote>
<p>Here for instance are the permitted sets for a network of ten neurons with randomly generated weights.</p>
<figure><img src="/images/permitted.png" alt="Diagram of permitted sets for a ten neuron network."/><figcaption>Permitted sets for a ten neuron network.</figcaption></figure>

<p>(This only shows “maximal” permitted sets; that is, those permitted sets not contained in any other permitted set.)</p>
<p>And this shows the steady state of the topmost permitted set with each neuron receiving an input of 1.</p>
<figure><img src="/images/steadystate.png"/><figcaption>Left: Neural activations. Right: Steady states.</figcaption></figure>

<p>And here is a (different) network transitioning through stable states as inputs and activations vary.</p>
<video controls loop src="/images/stability.mp4"></video>

<h1 id="conclusion">Conclusion</h1>
<p>If a connection pattern in a network is a memory, then multistability allows the brain to store memories much more efficiently. Patterns of activation can overlap within a network. One neuron can partake of several memories, much like a single gene can be implicated in the expression of a multitude of traits or behaviors. I imagine that whatever process the brain uses for memory storage, it must make a tradeoff between robustness and efficiency. It wants to minimize the cost of storing memories and so should use as few neurons as possible to do so, yet the death of a single neuron shouldn’t disrupt the system as a whole. The model of overlapping patterns seems to me like a plausible solution.</p>
<p>(I decided to read this paper after becoming interested in <a href="http://www.personal.psu.edu/cpc16/">Carina</a> <a href="https://www.quantamagazine.org/mathematician-carina-curto-thinks-like-a-physicist-to-solve-neuroscience-problems-20180619/">Curto</a>’s work on <a href="http://sites.psu.edu/mathneurolab/ctln/">combinatorial threshold networks</a>. She and her collaborators have extended the ideas presented here to more general threshold networks that can display various kind of dynamic behavior. I hope I can review some of her work in the future.)</p>
<h1 id="appendix---computing-permitted-sets-in-julia">Appendix - Computing Permitted Sets in Julia</h1>
<pre class="julia"><code>using Combinatorics
using LinearAlgebra

&quot;&quot;&quot;Determine whether the list `l1` is a numerical translation of the
list `l2`. The function will return `true` when `l1 == k+.l2` for some `k` 
modulo `n+1`.&quot;&quot;&quot;
function istranslation(l1, l2, n::Int)
    any([l1 == map(x -&gt; mod(x+i, n+1), l2) for i in 1:n])
end

&quot;&quot;&quot;Returns a maximal set of lists from `lists` that are unique up to translation.&quot;&quot;&quot;
function removetranslations(lists, n::Int)
    ls = []
    for l in lists
        if !any(map(x-&gt;istranslation(l, x, n), ls))
            push!(ls, l)
        end
    end
    return ls
end

&quot;&quot;&quot;Returns a set of lists from `lists` that are not properly contained in 
any other list.&quot;&quot;&quot;
function removesubsets(lists)
    isproper(a, b) = issubset(a, b) &amp;&amp; a != b
    ls = []
    for a in lists
        if !any(map(b -&gt; isproper(a, b), lists))
            push!(ls, a)
        end
    end
    return ls
end

&quot;&quot;&quot;Determines whether a matrix `A` represents a permitted set of neurons. `A` 
should be of the form `I-W`, where `W` is the weight matrix.&quot;&quot;&quot;
function ispermitted(A)
    all(map(x -&gt; x&gt;0, eigvals(A)))
end

&quot;&quot;&quot;Returns a matrix `P` of all permitted sets represented by a matrix
`A` of the form `I-W`. If neuron `j` is contained in permitted set
`i`, then `P[i,j] == 1`; otherwise, `P[i,j] == 0`. Each permitted set
is unique up to translation, and is not contained in any other
permitted set in `P`.&quot;&quot;&quot;
function permittedparents(A)
    ps = []
    n = length(A[:,1])
    idxs = removetranslations(powerset(1:n), n)
    filter!(!isempty, idxs)
    for idx in idxs
        submatrix = A[idx, idx]
        if ispermitted(submatrix)
            push!(ps, idx) 
        end
    end
    ps = removesubsets(ps)
    P = zeros(length(ps), n)
    for (i, pp) in enumerate(ps)
        for j in pp
            P[i, j] = 1
        end
    end
    return P
end
</code></pre>]]></description>
    <pubDate>Mon, 08 Apr 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/permitted-and-forbidden-sets/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Change of Basis for Vectors and Covectors</title>
    <link>https://mathformachines.com/posts/change-of-basis-for-vectors-and-covectors/index.html</link>
    <description><![CDATA[<blockquote>
<p>We share a philosophy about linear algebra: we think basis-free, we write basis-free, but when the chips are down we close the office door and compute with matrices like fury.</p>
</blockquote>
<p><a href="https://mathoverflow.net/questions/11669/what-is-the-difference-between-matrix-theory-and-linear-algebra/19923">Irving Kaplansky</a></p>
<p>Often, the first step in analyzing a problem is to <em>transform</em> it into something more amenable to our analysis. We would like the <em>representation</em> of our problem to reflect as naturally as possible whatever features of it we are most interested in. We might normalize data through a scaling transform, for instance, to eliminate spurious differences among like quantities. Or we might rotate data to align some of its salient dimensions with the coordinate axes, simplifying computations. Many matrix decompositions take the form <span class="math inline">\(M = BNA\)</span>. When <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span> are non-singular, we can think of <span class="math inline">\(N\)</span> as being a simpler representation of <span class="math inline">\(M\)</span> under coordinate transforms <span class="math inline">\(B\)</span> and <span class="math inline">\(A\)</span>. The <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix">spectral decomposition</a> and the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> are of this form.</p>
<p>All of these kinds of coordinate transformations are <em>linear</em> transformations. Linear coordinate transformations come about from operations on basis vectors that leave any vectors represented by them <a href="https://en.wikipedia.org/wiki/Active_and_passive_transformation">unchanged</a>. They are, in other words, a change of basis.</p>
<p>This post came about from my frustration at not finding simple formulas for these transformations with simple explanations to go along with them. So here, I tried to give a simple exposition of coordinate transformations for vectors in vector spaces along with transformations of their cousins, the covectors in the dual space. I’ll get into matrices and some applications in a future post.</p>
<h1 id="vectors">Vectors</h1>
<h2 id="example---part-1">Example - Part 1</h2>
<p>Let’s go through an example to see how it works. (I’ll assume the field is <span class="math inline">\(\mathbb{R}\)</span> throughout.)</p>
<p>Let <span class="math inline">\(e\)</span> be the standard basis in <span class="math inline">\(\mathbb{R}^2\)</span> and let <span class="math inline">\(e&#39;\)</span> be another basis where <span class="math display">\[
\begin{array}{cc}
e&#39;_1 = \frac12 e_1, &amp; e&#39;_2 = -e_1 + 2 e_2
\end{array}
\]</span> So we have written the basis <span class="math inline">\(e&#39;\)</span> in terms of the standard basis <span class="math inline">\(e\)</span>. As vectors they look like this:</p>
<figure><img src="/images/bases.jpg" alt="Drawing of the basis vectors e_1, e_2, and e'_1, e'_2"/></figure>

<p>And each will induce its own coordinate system, indicating the angle and orientation of each axis, and each axis’ unit of measure.</p>
<figure><img src="/images/coordinates.jpg" alt="Drawing of basis vectors and the coordinates they induce."></figure>

<p>We can write any vector in <span class="math inline">\(\mathbb{R}^2\)</span> as a linear combination of these basis elements.</p>
<p><span class="math display">\[\begin{array}{cc}
v = e_1 + 2 e_2, &amp; w&#39; = 5 e&#39;_1 + \frac12 e&#39;_2
\end{array}\]</span></p>
<figure><img src="/images/vectors.jpg" alt="Drawing of vectors in two coordinate systems."></figure>

<p>We call the coefficients on the basis elements the <strong>coordinates</strong> of the vector in that basis. So, in basis <span class="math inline">\(e\)</span> the vector <span class="math inline">\(v\)</span> has coordinates <span class="math inline">\((1, 2)\)</span>, and in basis <span class="math inline">\(e&#39;\)</span> the vector <span class="math inline">\(w&#39;\)</span> has coordinates <span class="math inline">\((5, \frac12)\)</span>.</p>
<h2 id="formulas">Formulas</h2>
<p>The choice of basis is just a choice of representation. The vector itself should stay the same. So the question is – how can we rewrite a vector in a <em>different</em> basis without changing the vector itself?</p>
<p>Let’s establish some notation. First, whenever we are talking about a vector in the abstract, let’s write <span class="math inline">\(\mathbf{v}\)</span>, and whenever we are talking about a vector represented in some basis let’s write <span class="math inline">\(v\)</span>. So the same vector <span class="math inline">\(\mathbf{v}\)</span> might have two different basis representations <span class="math inline">\(v\)</span> and <span class="math inline">\(v&#39;\)</span>, which nevertheless all stand for the same vector: <span class="math inline">\(\mathbf{v} = v = v&#39;\)</span>. However, when we write <span class="math inline">\(e\)</span> for a basis, we mean a list of vectors <span class="math inline">\(e_i\)</span> that form a basis in some vector space <span class="math inline">\(V\)</span>. So, <span class="math inline">\(v = v&#39;\)</span> always, but in general <span class="math inline">\(e \neq e&#39;\)</span>.</p>
<p>Our basis elements let’s index with subscripts (like <span class="math inline">\(e_1\)</span>), and coordinates let’s index with superscripts (like <span class="math inline">\(v^1\)</span>). This will help us keep track of which one we’re working with. Also, let’s write basis elements as row vectors, and coordinates as column vectors. This way we can write a vector as a matrix product of the basis elements and the coordinates:</p>
<p><span class="math display">\[v = \begin{bmatrix} e_1 &amp; e_2 \end{bmatrix}\begin{bmatrix}v^1 \\
v^2\end{bmatrix} = v^1 e_1 + v^2 e_2
\]</span></p>
<p>Now we can also write the transformation given above of <span class="math inline">\(e\)</span> into <span class="math inline">\(e&#39;\)</span> using matrix multiplication:</p>
<p><span class="math display">\[e&#39; = \begin{bmatrix}e&#39;_1&amp; e&#39;_2\end{bmatrix} = \begin{bmatrix}e_1&amp;  e_2\end{bmatrix}\begin{bmatrix}
\frac12 &amp; -1 \\
0 &amp; 2 
\end{bmatrix} = \begin{bmatrix}\frac12 e_1 &amp; -e_1 + 2 e_2\end{bmatrix}\]</span></p>
<p>The <span class="math inline">\(2 \times 2\)</span> matrix used in that transformation is called the <strong>transformation matrix</strong> from the basis <span class="math inline">\(e\)</span> to the basis <span class="math inline">\(e&#39;\)</span>.</p>
<p>The general formula is</p>
<p><span class="math display">\[\formbox{e&#39; = e A}\]</span></p>
<p>where <span class="math inline">\(A\)</span> is the transformation matrix. We can use this <em>same</em> matrix to transform coordinate vectors, but we shouldn’t necessarily expect that we can use the same formula. The bases and the coordinates are playing different roles here: the basis elements are vectors that describe the coordinate system, while the coordinates are scalars that describe a vector’s position in that system.</p>
<p>Let’s think about how this should work. Generally we write <span class="math inline">\(v = v^1 e_1 + v^2 e_2 \cdots + v^n e_n\)</span>. If we make some new basis <span class="math inline">\(e&#39;\)</span> by multiplying all the <span class="math inline">\(e_i\)</span>’s by 2, say, and <em>also</em> multiplied all the <span class="math inline">\(v_j\)</span>’s by 2, then we would end up with a vector <em>four times</em> the size of the original. Instead, we should have multiplied all the <span class="math inline">\(v_j\)</span>’s by <span class="math inline">\(\frac12\)</span>, the inverse of 2, and then we would have <span class="math inline">\(v&#39; = v\)</span>, as needed. The vector must be the same in either basis.</p>
<p>So, if we change the <span class="math inline">\(e\)</span>’s by some factor then, the <span class="math inline">\(v\)</span>’s need to change in the <em>inverse</em> manner to maintain equality. This suggests that to change <span class="math inline">\(v\)</span> into a representation <span class="math inline">\(v&#39;\)</span> in basis <span class="math inline">\(e&#39;\)</span> we should use instead</p>
<p><span class="math display">\[\formbox{v&#39; = A^{-1} v}\]</span></p>
<p>(We’ll prove it a little bit later.)</p>
<p>The fact that basis elements change in one way (<span class="math inline">\(e&#39; = e A\)</span>) while coordinates change in the inverse way (<span class="math inline">\(v&#39; = A^{-1} v\)</span>), is why we sometimes call the basis elements <strong>covariant</strong> and the vector coordinates <strong>contravariant</strong>, and distinguish them with the position of their indices.</p>
<h2 id="example---part-2">Example - Part 2</h2>
<p>Let’s go back to our example. Using our formula, we get</p>
<p><span class="math display">\[
v&#39; = \begin{bmatrix}2 &amp; 1 \\
0 &amp; \frac12 \end{bmatrix} \begin{bmatrix}1 \\
2\end{bmatrix} = \begin{bmatrix}4 \\
1\end{bmatrix}
\]</span></p>
<p>But what about <span class="math inline">\(w&#39;\)</span>? Well, since its representation is in <span class="math inline">\(e&#39;\)</span>, to convert in the opposite direction, to <span class="math inline">\(e\)</span>, we need to use the transformation that’s the inverse of <span class="math inline">\(A^{-1}\)</span>, namely, <span class="math inline">\(A\)</span>.</p>
<p><span class="math display">\[
w = \begin{bmatrix}\frac12 &amp; -1 \\
0 &amp; 1 \end{bmatrix} \begin{bmatrix}5 \\
\frac12\end{bmatrix} = \begin{bmatrix}2 \\
1\end{bmatrix}
\]</span></p>
<p>And now we have:</p>
<figure><img src="/images/transformed.jpg" alt="Drawing of vectors v, v', w, and w'." /></figure>

<p>Each vector is unchanged after a change of basis.</p>
<h1 id="covectors">Covectors</h1>
<p>Recall the <a href="https://en.wikipedia.org/wiki/Inner_product_space">inner product</a> on a vector space.</p>
<p>We might ask, given some vector <span class="math inline">\(v\)</span> how does an inner product vary as we range over vectors <span class="math inline">\(w\)</span>? In this case, we could think of <span class="math inline">\(\langle v, \cdot\rangle\)</span> as a function of vectors in <span class="math inline">\(V\)</span> whose outputs are scalars. In fact, these sorts of functions themselves form a vector space, called the <strong>dual space</strong> of <span class="math inline">\(V\)</span>, which we write <span class="math inline">\(V^*\)</span>. The members of <span class="math inline">\(V^*\)</span> are called <strong>linear functionals</strong> or <strong>covectors</strong>. The covector given by <span class="math inline">\(\langle v, \cdot\rangle\)</span> we denote <span class="math inline">\(v^*\)</span>.</p>
<p>We’ve been working with vectors in <span class="math inline">\(\mathbb{R}^n\)</span>, and in <span class="math inline">\(\mathbb{R}^n\)</span> the (canonical) inner product is the <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a>. This means that if we denote the covectors in <span class="math inline">\(V^*\)</span> as <em>rows</em> and the vectors in <span class="math inline">\(V\)</span> as <em>columns</em> (as usual), then we can write</p>
<p><span class="math display">\[
v^*(w) = \begin{bmatrix} v_1 &amp; \cdots &amp; v_n\end{bmatrix}\begin{bmatrix}w^1 \\
\vdots\\
w^n\end{bmatrix} = v_1 w^1 + \cdots + v_n w^n
\]</span></p>
<p>So, the covectors are functions <span class="math inline">\(\mathbb{R}^n \to \mathbb{R}\)</span>, but we can do computations with them just like we do with vectors, using matrix multiplication. We still write the indices of the row vectors as subscripts and the indices of the column vectors as superscripts.</p>
<p>If we can think about vectors in <span class="math inline">\(\mathbb{R}^n\)</span> as arrows, how should we think about covectors? To simplify things, let’s restrict our attention to the two-dimensional case. Now, consider the action of a covector <span class="math inline">\(v^*\)</span> on some unknown vector <span class="math inline">\(w = \begin{bmatrix}x&amp; y\end{bmatrix}^\top\)</span> in <span class="math inline">\(\mathbb{R}^2\)</span>:</p>
<p><span class="math display">\[
v^*(w) = v_1 x + v_2 y
\]</span></p>
<p>Now if we look at the level sets of this function, <span class="math inline">\(v_1 x + v_2 y = k\)</span>, it should start to look familiar…</p>
<p>It’s a family of <a href="https://en.wikipedia.org/wiki/Linear_equation#Two-point_form">lines</a>!</p>
<p>And to find out the value of <span class="math inline">\(v^*(w)\)</span> we just count how many lines of <span class="math inline">\(v^*\)</span> the vector <span class="math inline">\(w\)</span> passes through (including maybe “fractional” valued lines – <span class="math inline">\(k\)</span> doesn’t have to just be an integer). More generally, the covectors of <span class="math inline">\(\mathbb{R}^n\)</span> can be thought of as <a href="https://en.wikipedia.org/wiki/Hyperplane">hyperplanes</a>, and the value of <span class="math inline">\(v^*(w)\)</span> can be determined by how many hyperplanes of <span class="math inline">\(v^*\)</span> the vector <span class="math inline">\(w\)</span> passes through. And furthermore, the vector <span class="math inline">\(v\)</span> will be the <a href="https://en.wikipedia.org/wiki/Normal_(geometry)">normal</a> vector to the hyperplanes given by <span class="math inline">\(v^*\)</span>, that is, they are perpendicular.</p>
<h2 id="example---part-3">Example - Part 3</h2>
<p>In the standard basis, let <span class="math inline">\(v^*\)</span> be given by <span class="math inline">\(\begin{bmatrix}1 &amp; 2\end{bmatrix}\)</span>. Its family of lines will then be <span class="math inline">\(x + 2 y = k\)</span>. Now let <span class="math inline">\(w\)</span> be given by <span class="math inline">\(\begin{bmatrix}2 &amp; 1\end{bmatrix}\)</span>, and count how many lines <span class="math inline">\(w\)</span> crosses through:</p>
<figure><img src="/images/covectors.jpg" alt="Left: Drawing of v and v^*. Right: Drawing of v^* and w." /></figure>

<p>It’s exactly the same as <span class="math inline">\(v^*(w) = 2 + 2(1) = 4\)</span>! I think that’s pretty cool.</p>
<h2 id="the-dual-basis">The Dual Basis</h2>
<p>Okay, so what about bases in <span class="math inline">\(V^*\)</span>? We’d like to have a basis for <span class="math inline">\(V^*\)</span> that is the “best fit” for whatever basis we have in <span class="math inline">\(V\)</span>. This turns out to be the basis given by: <span class="math display">\[
e^i(e_j) =
\begin{cases}
  1 &amp; \text{if } i = j\\
  0 &amp; \text{if } i \ne j
\end{cases}\]</span></p>
<p>where <span class="math inline">\((e_j)\)</span> is a basis in <span class="math inline">\(V\)</span>. Or sometimes people write instead <span class="math inline">\(e^i(e_j) = \delta^i_j\)</span>, where <span class="math inline">\(\delta^i_j\)</span> is the <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a>. We call this basis <span class="math inline">\((e^i)\)</span> the <strong>dual basis</strong> of <span class="math inline">\((e_j)\)</span>. You can see that a basis and its dual have a kind of “bi-orthogonality” property that turns out to be very convenient.</p>
<p>Let’s look at formulas for changing bases now. If we have a vector <span class="math inline">\(v\)</span> in <span class="math inline">\(V\)</span> written as a column, how can we find the corresponding vector <span class="math inline">\(v^*\)</span> in <span class="math inline">\(V^*\)</span>? The obvious thing to do would be to take the transposition of <span class="math inline">\(v\)</span>. This will not always work. Recall the definitions of <span class="math inline">\(v, v&#39;, w\)</span> and <span class="math inline">\(w&#39;\)</span> from the <em>first section</em>, and consider:</p>
<p><span class="math display">\[v^\top v = \begin{bmatrix} 1 &amp; 2\end{bmatrix}\begin{bmatrix}1 \\
2\end{bmatrix} = 1 + 4 = 5\]</span></p>
<p><span class="math display">\[v&#39;^\top v&#39; = \begin{bmatrix} 4 &amp; 1\end{bmatrix}\begin{bmatrix}4 \\
1\end{bmatrix} = 16 + 1 = 17\]</span></p>
<p>This is no good. We get two different values for <span class="math inline">\(\bar v^*(\bar w)\)</span> depending on which basis we use, but the values of a function on a vector space shouldn’t depend on the basis. The trouble is that the dual of <span class="math inline">\((e&#39;_i)\)</span> isn’t the transpose of those basis vectors (they don’t satisfy the bi-orthogonality property), so the duals of those vectors represented in it won’t be the transposes of those vectors either.</p>
<p>This <em>will</em> be true for <a href="https://en.wikipedia.org/wiki/Orthonormality">orthonormal</a> bases, however. The standard basis <span class="math inline">\((e_i)\)</span> <em>is</em> orthonormal, and the duals of the vectors represented in it will in fact be those transposes.</p>
<p><span class="math display">\[ \formbox{v^* = v^\top \text{for any vector } v \text{ written in an orthonormal basis.}} \]</span></p>
<h2 id="formulas-1">Formulas</h2>
<p>The next question is, if we perform a change of basis in <span class="math inline">\(V\)</span>, what is the corresponding change in <span class="math inline">\(V^*\)</span>? Let’s use the same reasoning that we did before. For a vector <span class="math inline">\(w\)</span> in <span class="math inline">\(\mathbb{R}\)</span> and a covector <span class="math inline">\(v^*\)</span>, we have</p>
<p><span class="math display">\[
v^*(w) = v_1 w^1 + \cdots + v_n w^n
\]</span></p>
<p>And so, like before, if we change the values of the <span class="math inline">\(w_j\)</span>’s, the values of the <span class="math inline">\(v^i\)</span>’s should change in the inverse manner to preserve equality. But <span class="math inline">\(w\)</span> changes as <span class="math inline">\(w&#39; = A^{-1} w\)</span>, so <span class="math inline">\(v^*\)</span> must change as <span class="math inline">\(v&#39;^* = v^* A\)</span>. And its basis (the dual basis) must change as <em>its</em> inverse: <span class="math inline">\(e&#39;^* = A^{-1} e^*\)</span>.</p>
<p><span class="math display">\[\formbox{\begin{align}
e&#39;^* &amp;= A^{-1} e^*\\
v&#39;^* &amp;= v^* A
\end{align}}\]</span></p>
<p>Notice that this time the basis vectors are playing the <strong>contravariant</strong> part, while the coordinates are playing the <strong>covariant</strong> part with respect to the original vector space.</p>
<h2 id="example---part-4">Example - Part 4</h2>
<p>Lets continue our example. Since <span class="math inline">\(e\)</span> is the standard basis, it is orthonormal, and we can therefore find the duals of <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> by taking transposes. We can then apply our formula to find the duals of <span class="math inline">\(v&#39;^*\)</span> and <span class="math inline">\(w&#39;^*\)</span>.</p>
<span class="math display">\[\begin{align}
v&#39;^*(x, y) &amp;= \begin{bmatrix}1 &amp; 2\end{bmatrix}\begin{bmatrix}\frac12 &amp; -1\\
0 &amp; 2\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= \begin{bmatrix}\frac12 &amp; 3\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= \frac12 x + 3y\\
\\
w&#39;^*(x, y) &amp;= \begin{bmatrix}2 &amp; 1\end{bmatrix}\begin{bmatrix}\frac12 &amp; -1\\
0 &amp; 2\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= \begin{bmatrix}1 &amp; 0\end{bmatrix}\begin{bmatrix}x\\
y\end{bmatrix}\\
&amp;= 2x + 2y
\end{align}
\]</span>
<p>The duals too are unchanged after a change of basis.</p>
<figure><img src="/images/covectors2.jpg" alt="Left: Drawing of $v^*$ and $w^*$. Right: Drawing of $v'^*$ and $w'^*$." /></figure>

<h1 id="summary-of-formulas">Summary of Formulas</h1>
<p><span class="math display">\[\formbox{\begin{array}{llr} 
e&#39;   &amp;= e A      &amp; &amp;(1)\\ 
v&#39;   &amp;= A^{-1} v &amp; &amp;(2)\\
e&#39;^* &amp;= A^{-1} e &amp; &amp;(3)\\
v&#39;^* &amp;= v A      &amp; &amp;(4)
\end{array} }\]</span></p>
<p>Suppose <span class="math inline">\((1)\)</span>, that <span class="math inline">\(e&#39; = e A\)</span>, where <span class="math inline">\(A\)</span> is a non-singular matrix.</p>
<p><strong>Proof of (2):</strong> We know <span class="math inline">\(e v = e&#39; v&#39;\)</span>. Now</p>
<p><span class="math display">\[
e&#39; v&#39; = e v = e A A^{-1} v = e&#39; (A^{-1} v)
\]</span></p>
<p>But then it must be that <span class="math inline">\(v&#39; = A^{-1} v\)</span> since basis representations are unique.</p>
<p><strong>Proof of (4):</strong> We also know <span class="math inline">\(v&#39;^* w&#39; = v^* w\)</span> for all vectors <span class="math inline">\(w\)</span>. But then</p>
<p><span class="math display">\[
v&#39;^* w&#39; = v^* w = v^* w = v^* A A^{-1} w = (v^* A) w&#39;
\]</span></p>
<p>for all <span class="math inline">\(w&#39;\)</span>. So, <span class="math inline">\(v&#39;^* = v^* A\)</span>.</p>
<p><strong>Proof of (3):</strong> Lastly,</p>
<p><span class="math display">\[
v&#39;^* e&#39;^* = v^* e^* = v^* A A^{-1} e^* = v&#39;^* A^{-1} e^*
\]</span></p>
<p>for all <span class="math inline">\(w&#39;\)</span>. So, <span class="math inline">\(e&#39;^* = A^{-1}e^*\)</span>. <strong>QED</strong></p>]]></description>
    <pubDate>Mon, 18 Mar 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/change-of-basis-for-vectors-and-covectors/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>

    </channel>
</rss>
