<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Math for Machines</title>
        <link>https://mathformachines.com</link>
        <description><![CDATA[A blog about data science and machine learning, with a lot of math.]]></description>
        <atom:link href="https://mathformachines.com/feeds/data-science.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Fri, 04 Oct 2019 00:00:00 UT</lastBuildDate>
        <item>
    <title>What I'm Reading 1: Bayes and Means</title>
    <link>https://mathformachines.com/posts/bayes-and-means/index.html</link>
    <description><![CDATA[<h1 id="bayesian-aggregation">Bayesian Aggregation</h1>
<p>Yang, Y., &amp; Dunson, D. B., <em>Minimax Optimal Bayesian Aggregation</em> 2014 (<a href="https://arxiv.org/abs/1403.1345">arXiv</a>)</p>
<p>Say we have a number of estimators <span class="math inline">\(\hat f_1, \ldots, \hat f_K\)</span> derived from a number of models <span class="math inline">\(M_1, \ldots, M_K\)</span> for some regression problem <span class="math inline">\(Y = f(X) + \epsilon\)</span>, but, as is the nature of things when estimating with limited data, we don’t know which estimator represents the true model (assuming the true model is in our list). The Bayesian habit is to stick a prior on the uncertainty, compute posteriors probabilities, and then average across the unknown parameter using the posterior probabilities as weights. Since the posterior probabilities (call them <span class="math inline">\(\lambda_1, \ldots, \lambda_K\)</span>) have to sum to 1, we obtain a <em>convex combination</em> of our estimators <span class="math display">\[ \hat f = \sum_{1\leq i \leq K} \lambda_i \hat f_i \]</span> This is the approach of <a href="https://www.stat.colostate.edu/~jah/papers/statsci.pdf">Bayesian Model Averaging</a> (BMA). Yang <em>et al.</em> propose to find such combinations using a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet prior</a> on the weights <span class="math inline">\(\lambda_i\)</span>. If we remove the restriction that the weights sum to 1 and instead only ask that they have finite sum in absolute value, then we obtain <span class="math inline">\(\hat f\)</span> as a <em>linear combination</em> of <span class="math inline">\(\hat f_i\)</span>. The authors then place a Gamma prior on <span class="math inline">\(A = \sum_i |\lambda_i|\)</span> and a Dirichlet prior on <span class="math inline">\(\mu_i = \frac{|\lambda_i|}{A}\)</span>. In both the linear and the convex cases they show that the resulting estimator is minimax optimal in the sense that it will give the best worst-case predictions for a given number of observations, including the case where a sparsity restriction is placed on the number of estimators <span class="math inline">\(\hat f_i\)</span>; in other words, <span class="math inline">\(\hat f\)</span> converges to the true estimator as the number of observations increases with minimax optimal risk. The advantage to previous non-bayesian methods of linear or convex aggregation is that the sparsity parameter can be learned from the data. The Dirichlet convex combination gives good performance against Best Model selection, Majority Voting, and <a href="https://biostats.bepress.com/ucbbiostat/paper266/">SuperLearner</a>, especially when there are both a large number of observations and a large number of estimators.</p>
<p>I implemented the convex case in R for use with <a href="https://github.com/paul-buerkner/brms">brms</a>. The Dirichlet distribution has been <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution#Gamma_distribution">reparameterized</a> as a sum of Gamma RVs to aid in sampling. The Dirichlet concentration parameter is <span class="math inline">\(\frac{\alpha}{K^\gamma}\)</span>; the authors recommend choosing <span class="math inline">\(\alpha = 1\)</span> and <span class="math inline">\(\gamma = 2\)</span>.</p>
<pre class="r" data-org-language="R"><code>convex_regression &lt;- function(formula, data,
                              family = &quot;gaussian&quot;,
                              ## Yang (2014) recommends alpha = 1, gamma = 2
                              alpha = 1, gamma = 2,
                              verbose = 0,
                              ...) {
  if (gamma &lt;= 1) {
    warning(paste(&quot;Parameter gamma should be greater than 1. Given:&quot;, gamma))
  }
  if (alpha &lt;= 0) {
    warning(paste(&quot;Parameter alpha should be greater than 0. Given:&quot;, alpha))
  }
  ## Set up priors.
  K &lt;- length(terms(formula))
  alpha_K &lt;- alpha / (K^gamma)
  stanvars &lt;-
    stanvar(alpha_K,
      &quot;alpha_K&quot;,
      block = &quot;data&quot;,
      scode = &quot;  real&lt;lower = 0&gt; alpha_K;  // dirichlet parameter&quot;
    ) +
    stanvar(
      name = &quot;b_raw&quot;,
      block = &quot;parameters&quot;,
      scode = &quot;  vector&lt;lower = 0&gt;[K] b_raw; &quot;
    ) +
    stanvar(
      name = &quot;b&quot;,
      block = &quot;tparameters&quot;,
      scode = &quot;  vector[K] b = b_raw / sum(b_raw);&quot;
    )
  prior &lt;- prior(&quot;target += gamma_lpdf(b_raw | alpha_K, 1)&quot;,
    class = &quot;b_raw&quot;, check = FALSE
  )
  f &lt;- update.formula(formula, . ~ . - 1)
  if (verbose &gt; 0) {
    make_stancode(f,
      prior = prior,
      data = data,
      stanvars = stanvars
    ) %&gt;% message()
  }
  fit_dir &lt;- brm(f,
    prior = prior,
    family = family,
    data = data,
    stanvars = stanvars,
    ...
  )
  fit_dir
}
</code></pre>
<p>Here is a <a href="https://gist.github.com/ryanholbrook/b5c7d44c0c7642eeee1a3034b48f29d7">gist</a> that includes an interface to <a href="https://tidymodels.github.io/parsnip/">parsnip</a>.</p>
<p>In my own experiments, I found the performance of the convex aggregator to be comparable to a <a href="https://en.wikipedia.org/wiki/Lasso_(statistics)">LASSO</a> SuperLearner at the cost of the lengthier training that goes with MCMC methods and the finicky convergence of sparse priors. So I would likely reserve this for when I had lots of features and lots of estimators to work through, where I presume it would show an advantage. But in that case it would definitely be on my list of things to try.</p>
<h1 id="bayesian-stacking">Bayesian Stacking</h1>
<p>Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A., <em>Using Stacking to Average Bayesian Predictive Distributions</em> (<a href="https://projecteuclid.org/euclid.ba/1516093227">pdf</a>)</p>
<p>Another approach to model combination is <a href="https://doi.org/10.1080/01621459.1996.10476733">stacking</a>. With stacking, model weights are chosen by cross-validation to minimize <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">RMSE</a> predictive error. Now, BMA finds the aggregated model that best fits the data, while stacking finds the aggregated model that gives the best predictions. Stacking therefore is usually better when predictions are what you want. A drawback is that stacking produces models through <em>point</em> estimates. So, they don’t give you all the information of a full distribution like BMA would. Yao <em>et al.</em> propose a method of stacking that instead finds the optimal <a href="https://en.wikipedia.org/wiki/Posterior_predictive_distribution">predictive distribution</a> by convex combinations of distributions with weights chosen by some scoring rule: the authors use the minimization of KL-divergence. Hence, they choose weights <span class="math inline">\(w\)</span> empirically through <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#Leave-one-out_cross-validation">LOO</a> by <span class="math display">\[ \max_w \frac{1}{n} \sum_{1\leq i \leq n} \log \sum_{1\leq k \leq K} w_k p(y_i | y_{-i}, M_k) \]</span> where <span class="math inline">\(y_1, \ldots, y_n\)</span> are the observed data and <span class="math inline">\(y_{-i}\)</span> is the data with <span class="math inline">\(y_i\)</span> left out. The following figure shows how stacking of predictive distributions gives the “best of both worlds” for BMA and point prediction stacking.</p>
<figure>
<img src="/images/stacking.png" alt="From Yao (2018)" /><figcaption>From Yao (2018)</figcaption>
</figure>
<p>They have implemented stacking for <a href="https://mc-stan.org/users/interfaces/rstan">Stan</a> models in the R package <a href="https://cran.r-project.org/web/packages/loo/vignettes/loo2-weights.html">loo</a>.</p>]]></description>
    <pubDate>Fri, 04 Oct 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/bayes-and-means/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Bayesian Topic Modeling</title>
    <link>https://mathformachines.com/posts/bayesian-topic-modeling/index.html</link>
    <description><![CDATA[<p>Imagine we have some collection of documents. They could be novels, or tweets, or financial reports—just any collection of text. We want an algorithm that can discover what they are about, and we would like our algorithm to do it automatically, without any hints. (That is, we want our algorithm to be <em>unsupervised</em>.) We will look at several models that probabilistically assign words to topics using <a href="https://en.wikipedia.org/wiki/Bayes&#39;_theorem">Bayes’ Theorem</a>. They are all <a href="https://en.wikipedia.org/wiki/Bayesian_network">Bayesian Graphical Models</a>.</p>
<p>The basic problem in statistics is to infer some unobservable value from observable instances of it. In our case, we want to infer the <em>topics</em> of a document from the actual words in the document. We want to be able to infer that our document is about “colors” if we observe “red” and “green” and “blue”.</p>
<p>Bayes’ Theorem allows us to do this. It allows us to infer probabilities concerning the unobserved value from the observations that we can make. It allows us to reason backwards in some sense. So, when constructing a Bayesian model, it is helpful to <em>think</em> backwards. Instead of first asking how words are distributed to topics and topics to documents, we will ask how we could <em>generate</em> a document if we already knew these distributions. To construct our model, we will first reason from the unknown values to the known values so that we know how to do the converse when the time comes.</p>
<h1 id="some-simple-generative-examples">Some Simple Generative Examples</h1>
<p>In all of our models, we are going make a simplfying assumption. We will assume that all of the words in a document occur independently of whatever words came before or come after; that is, a document will just be a “bag of words.” We’ll see that even with ignoring word-order, we can still infer pretty accurately what a document might be about.</p>
<p>Let’s start with a very simple example: 1 document with 1 topic and 2 words in our vocabulary.</p>
<p>(Some definitions: The “vocabulary” is just the set of unique words that occur in all of the documents together, the “corpus.” We’ll refer to a word in the vocabulary as just a “word” and some instance of a word in a document as a “token.”)</p>
<p>Let’s say our two words are “blue” and “red”, and that the probability of any given word (any token) being “red” is <span class="math inline">\(\phi\)</span>: <span class="math inline">\(P(W = red) = \phi\)</span>. This is the same as saying our random variable of tokens <span class="math inline">\(W\)</span> has a Bernoulli distribution with parameter <span class="math inline">\(\phi\)</span>: <span class="math inline">\(W \sim Bernoulli(\phi)\)</span>.</p>
<p>The distribution looks like this:</p>
<pre class="ipython" data-exports="both"><code>x = [0, 1]
pmf = st.bernoulli.pmf(x, 0.3)

plt.stem(x, pmf)
plt.xticks([0,1])
plt.ylim(0,1)
plt.xlim(-0.5, 1.5)
</code></pre>
<div class="RESULTS drawer">
<pre class="example"><code>(-0.5, 1.5)
</code></pre>
</div>
<figure><img src="/images/bernoulli.png" alt="Bernoulli pmf" /></figure>

<p>Here, 1 represents “red” and 0 represents “blue” (or not-“red”).</p>
<p>And here is how we could generate a document with this model:</p>
<pre class="ipython" data-exports="both"><code>coding = {0 : &quot;blue&quot;, 1 : &quot;red&quot;}
W = 50  # number of tokens in the document
tokens = st.bernoulli.rvs(0.3, size = W)  # choose the tokens
print(&#39; &#39;.join(str(w) for w in [coding[i] for i in tokens]))
</code></pre>
<pre class="example"><code>blue blue red blue red blue blue red red blue blue blue blue blue blue blue blue red blue blue blue blue blue blue blue blue blue red red blue blue blue blue blue blue red blue blue red blue red blue red blue blue blue blue blue red blue
</code></pre>
<h2 id="unigram-model">Unigram Model</h2>
<p>For the general model, we will also choose the distribution of words within the topic randomly. That is, we will assign a probability distribution to <span class="math inline">\(\phi\)</span>.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta distribution</a> is a natural choice. Since its support is <span class="math inline">\([0,1]\)</span> it can represent randomly chosen probabilities (values between 0 and 1). It is also conceptually convenient being the <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate prior</a> of the Bernoulli distribution, which allows us to make a more explicit connection between its parameters and the parameters of its Bernoulli distribution.</p>
<p>The model is now:</p>
<p><span class="math inline">\(\phi \sim Beta(\beta_0, \beta_1)\)</span></p>
<p><span class="math inline">\(W \sim Bernoulli(\phi)\)</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are the “shape parameters” of the beta distribution. We can think of them as the assumed counts of each word, or the “pseudo-counts.” Let’s see how different values of these parameters affect the shape of the distribution.</p>
<pre class="ipython" data-exports="both"><code>beta_0 = [0.8, 1, 2, 10]
beta_1 = [0.8, 1, 2, 10]

x = np.array(np.linspace(0, 1, 1000))

f, axarr = plt.subplots(len(beta_0), len(beta_1), sharex=&#39;all&#39;, sharey=&#39;none&#39;)

for i in range(len(beta_0)):
    for j in range(len(beta_1)):
        a = beta_0[i]
        b = beta_1[j]
        y = st.beta(a, b).pdf(x)
        axarr[i, j].plot(x, y)
        axarr[i, j].axes.yaxis.set_ticklabels([])
        axarr[i, j].set_title(r&#39;$\beta_0 =$ &#39; + str(a) + r&#39;, $\beta_1 =$ &#39; + str(b))

f.subplots_adjust(hspace=0.3)
f.suptitle(r&#39;Beta Distributions for $\theta$&#39;, fontsize=20)
</code></pre>
<pre class="example"><code>Text(0.5, 0.98, &#39;Beta Distributions for $\\theta$&#39;)
</code></pre>
<figure><img src="/images/beta.png" alt="a grid of six beta pdfs for various parameters" /></figure>

<p>Values near 0 will favor “blue” and values near 1 will favor “red”. We can choose <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to generate the kinds of documents we like. (The notation is a bit backwards here: <span class="math inline">\(\beta_0\)</span> is the <em>pseudo-count</em> for “red”, whose probability is toward 1, on the right of the graph. So <span class="math inline">\(\beta_0 &gt; \beta_1\)</span> means more “red”s, and vice versa.)</p>
<p>Let’s generate some documents with this expanded model. We’ll set <span class="math inline">\(\beta_0 = 0.8\)</span> and <span class="math inline">\(\beta_1 = 0.8\)</span>. We would expect most of our documents to favor one word or the other, but overall to occur equally often.</p>
<pre class="ipython" data-exports="both"><code>beta_0 = 0.8
beta_1 = 0.8

thetas = st.beta.rvs(beta_0, beta_1, size = 6)

W = 10  # number of tokens in each document

for t in thetas:
    print(&#39;Theta: &#39;, t)
    tokens = st.bernoulli.rvs(t, size = W)
    print(&#39;Document: &#39; + &#39; &#39;.join(str(w) for w in [coding[i] for i in tokens]) + &#39;\n&#39;)
</code></pre>
<pre class="example"><code>Theta:  0.2376299911870814
Document: blue red blue blue red red blue blue blue blue

Theta:  0.768902025579346
Document: red red red red blue red red red blue red

Theta:  0.6339386112711662
Document: red blue red blue red blue blue blue red blue

Theta:  0.889248394241369
Document: red red red blue red red red red red red

Theta:  0.7522981849896823
Document: red red red red blue blue red red red red

Theta:  0.18416659985533126
Document: blue red red blue blue blue red red blue blue
</code></pre>
<p>(We could also assign a distribution to W, the number of tokens in each document. (Blei 2003) uses a Poisson distribution.)</p>
<p>Let’s look at a couple more.</p>
<h2 id="mixture-of-unigrams">Mixture of Unigrams</h2>
<p>Here, we’ll also choose a single topic for each document, from among two. To simplify things, we’ll also assume the topics generate distinct words and that the proportions of words in topics are similar, that is, that they have the same shape parameters. We’ll see later that is a good assumption when using inference models.</p>
<p>Distribution of topics to documents: <span class="math inline">\(\theta \sim Beta(\alpha_0, \alpha_1)\)</span></p>
<p>Distribution of words to Topic 0: <span class="math inline">\(\phi_0 \sim Beta(\beta_0, \beta_1)\)</span></p>
<p>Distribution of words to Topic 1: <span class="math inline">\(\phi_1 \sim Beta(\beta_0, \beta_1)\)</span></p>
<p>The topics: <span class="math inline">\(T \sim Bernoulli(\theta)\)</span></p>
<p>Words from Topic 0: <span class="math inline">\(W_1 \sim Bernoulli(\phi_0)\)</span></p>
<p>Words from Topic 1: <span class="math inline">\(W_2 \sim Bernoulli(\phi_1)\)</span></p>
<pre class="ipython" data-exports="both"><code>coding_0 = {0:&#39;blue&#39;, 1:&#39;red&#39;}  # words in topic 0
coding_1 = {0:&#39;dogs&#39;, 1:&#39;cats&#39;}  # words in topic 1

D = 15  # number of documents in corpus
W = 10  # number of tokens in each document

alpha_0, alpha_1 = 1, 1.5
beta_0, beta_1 = 0.8, 0.8

theta = st.beta.rvs(alpha_0, alpha_1, size = 1)[0]  # choose a distribution of topics to documents
phi_0 = st.beta.rvs(beta_0, beta_1, size = 1)[0] # choose distribution of words in topic 0
phi_1 = st.beta.rvs(beta_0, beta_1, size = 1)[0] # choose distribution of words in topic 1

topics = st.bernoulli.rvs(theta, size = D)  # choose a topic for each document

print(&#39;Theta: {:.3f}  Phi_0: {:.3f}  Phi_1: {:.3f}&#39;.format(theta, phi_0, phi_1))
for i in range(D):
    if topics[i] == 0:
        tokens = st.bernoulli.rvs(phi_0, size = W)
        print(&#39;Document: &#39; + &#39; &#39;.join(str(w) 
              for w in [coding_0[i] for i in tokens]))
    else:
        tokens = st.bernoulli.rvs(phi_1, size = W)
        print(&#39;Document: &#39; + &#39; &#39;.join(str(w) 
              for w in [coding_1[i] for i in tokens]))
</code></pre>
<pre class="example"><code>Theta: 0.114  Phi_0: 0.973  Phi_1: 0.637
Document: red red red red red red red red red red
Document: red red red blue red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: dogs dogs cats cats cats cats cats dogs cats dogs
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
Document: red red blue red red red red red red red
Document: red red red red red red red red red red
Document: red red red red red red red red red red
</code></pre>
<h2 id="latent-dirichlet-allocation">Latent Dirichlet Allocation</h2>
<p>This time, instead of choosing a single topic for each document, we’ll choose a topic for each word. This will make our model much more flexible and its behavior more realistic.</p>
<p>Distribution of topics <strong>within</strong> documents: <span class="math inline">\(\theta \sim Beta(\alpha_0, \alpha_1)\)</span></p>
<p>Distribution of words to Topic 0: <span class="math inline">\(\phi_0 \sim Beta(\beta_0, \beta_1)\)</span></p>
<p>Distribution of words to Topic 1: <span class="math inline">\(\phi_1 \sim Beta(\beta_0, \beta_1)\)</span></p>
<p>The topics: <span class="math inline">\(T \sim Bernoulli(\theta)\)</span></p>
<p>Words from Topic 0: <span class="math inline">\(W_1 \sim Bernoulli(\phi_0)\)</span></p>
<p>Words from Topic 1: <span class="math inline">\(W_2 \sim Bernoulli(\phi_1)\)</span></p>
<pre class="ipython" data-exports="both"><code>coding_0 = {0:&#39;blue&#39;, 1:&#39;red&#39;}  # words in topic 0
coding_1 = {0:&#39;dogs&#39;, 1:&#39;cats&#39;}  # words in topic 1

D = 15
W = 10  # number of tokens in each document

alpha_0, alpha_1 = 1, 1.5
beta_0, beta_1 = 0.8, 0.8

theta = st.beta.rvs(alpha_0, alpha_1, size = 1)[0]  # choose a distribution of topics to documents
phi_0 = st.beta.rvs(beta_0, beta_1, size = 1)[0]  # choose distribution of words in topic 0
phi_1 = st.beta.rvs(beta_0, beta_1, size = 1)[0]  # choose distribution of words in topic 1

print(&#39;Theta: {:.3f}  Phi_0: {:.3f}  Phi_1: {:.3f}&#39;.format(theta, phi_0, phi_1))
for i in range(D):
    print(&#39;Document: &#39;, end=&#39;&#39;)
    topics = st.bernoulli.rvs(theta, size=W)  # choose topics for each word
    for j in range(W):
        if topics[j] == 0:
            token = st.bernoulli.rvs(phi_0, size=1)[0]  # choose a word from topic 0
            print(coding_0[token], end=&#39; &#39;)
        else:
            token = st.bernoulli.rvs(phi_1, size=1)[0]  # choose a word from topic 1
            print(coding_1[token], end=&#39; &#39;)
    print() 
</code></pre>
<pre class="example"><code>Theta: 0.384  Phi_0: 0.127  Phi_1: 0.028
Document: dogs blue blue blue dogs blue dogs blue blue blue 
Document: blue dogs blue blue dogs dogs dogs dogs blue cats 
Document: blue dogs blue blue blue dogs red dogs blue blue 
Document: dogs dogs red dogs dogs blue dogs blue blue blue 
Document: blue dogs dogs blue blue dogs red dogs dogs red 
Document: dogs blue blue red dogs blue dogs blue blue blue 
Document: blue blue blue dogs blue dogs blue dogs dogs blue 
Document: dogs red dogs red dogs blue dogs dogs blue blue 
Document: dogs dogs blue dogs blue dogs blue blue blue dogs 
Document: dogs blue blue blue blue red blue blue dogs dogs 
Document: dogs dogs blue red dogs dogs blue blue blue blue 
Document: blue blue blue red dogs blue blue blue blue red 
Document: blue blue blue dogs blue dogs red dogs blue dogs 
Document: dogs blue blue dogs dogs dogs blue dogs dogs blue 
Document: dogs dogs dogs red blue dogs red dogs dogs dogs 
</code></pre>
<h1 id="the-dirichlet-distribution">The Dirichlet Distribution</h1>
<p>Before we go on, we need to generalize our model a bit to be able to handle arbitrary numbers of words and topics, instead of being limited to just two. The multivariate generalization of the Bernoulli distribution is the <a href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical distribution</a>, which simply gives a probability to each of some number of categories. The generalization of the beta distribution is a little trickier. It is called the <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a>. And just like samples from the beta distribution will give parameters for a Bernoulli RV, samples from the Dirichlet distribution will give parameters for the categorical RV.</p>
<p>Let’s recall the two requirements for some set of <span class="math inline">\(p\)</span>’s to be probability parameters to a categorical distribution. First, they have to sum to 1: <span class="math inline">\(p_0 + p_1 + \cdots + p_v = 1\)</span>. This means they form a <a href="https://en.wikipedia.org/wiki/Hyperplane">hyperplane</a> in <span class="math inline">\(v\)</span>-dimensional space. Second, they all have to be non-negative: <span class="math inline">\(p_i \geq 0\)</span>. This means they all lie in the first quadrant (or <a href="https://en.wikipedia.org/wiki/Orthant">orthant</a>, more precisely). The geometric object that satisfies these two requirements is a <a href="https://en.wikipedia.org/wiki/Simplex#The_standard_simplex">simplex</a>. In the case of two variables it will be a line-segment and in the case of three variables it will be a triangle.</p>
<p>As sampled from the distribution, these values will form <a href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system">barycentric coordinates</a> on the simplex. This just means that the coordinates tell you how far the point is from the center of the simplex, instead of how far it is from the origin, like with Cartesian coordinates.</p>
<p>The 3-dimensional Dirichlet returns barycentric coordinates on the 2-simplex, a triangle. We can visualize the surface of the Dirichlet pdf as existing over a triangle; that is, its domain is the simplex.</p>
<pre class="ipython" data-exports="both"><code>import simplex_plots as sp
# from https://gist.github.com/tboggs/8778945

alphas = [[0.999, 0.999, 0.999], [1, 2, 1], [1, 2, 3], 
          [2, 0.999, 1], [10, 3, 4], [0.999, 1, 1]]

fig = plt.figure(figsize=(12, 8))
fig.suptitle(&#39;The Dirichlet Distribution&#39;, fontsize=16)
for i, a in enumerate(alphas):
    plt.subplot(2, len(alphas)/2, i + 1)
    sp.draw_pdf_contours(sp.Dirichlet(a), border=True, cmap=&#39;Blues&#39;)
    title = r&#39;$\alpha = $ = ({0[0]:.3f}, {0[1]:.3f}, {0[2]:.3f})&#39;.format(a)
    plt.title(title, fontdict={&#39;fontsize&#39;: 14})
</code></pre>
<figure><img src="/images/dirichlet.png" alt="various dirichlet pdfs" /></figure>

<p>Each corner of the triangle will favor a particular category (a word or a topic), just like either side of the domain of the beta distribution favored a category.</p>
<p>As in the upper left picture, whenever all of the entries in <span class="math inline">\(\alpha\)</span> are equal, we call the distribution “symmetric,” and whenever they are all less then 1, we call the distribution “sparse.” Distributions that are both symmetric and sparse are often used as priors when inferring a topic model, symmetry because we don’t <em>a priori</em> have any reason to favor one unknown category over another, and sparsity to encourage our categories to be distinct.</p>
<p>Now let’s start developing our models.</p>
<h1 id="the-full-model">The Full Model</h1>
<h2 id="data-preparation">Data Preparation</h2>
<p>First we’ll make up a corpus and put it into an encoding that our models can use. To simplify things, we’ll let all of our documents have the same number of tokens and flatten the encoded data structure.</p>
<pre class="ipython" data-exports="both"><code>from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    &#39;Red blue green. Green blue blue? Red, red, blue, yellow.&#39;,
    &#39;Car light red stop. Stop car. Car drive green, yellow.&#39;,
    &#39;Car engine gas stop! Battery engine drive, car. Electric, gas.&#39;,
    &#39;Watt, volt, volt, amp. Battery, watt, volt, electric volt charge. &#39;,
]

tokenizer = CountVectorizer(lowercase=True).build_analyzer()
encoder = LabelEncoder()

corpus_tokenized = np.array([tokenizer(doc) for doc in corpus])  # assign a number to each word
encoder.fit(corpus_tokenized.ravel())
vocab = list(encoder.classes_)  # the vocabulary

# The number of documents and their length
D, W = corpus_tokenized.shape
# The number of words in the vocabulary
V = len(vocab)

# Flatten and encode the corpus, and create an index.
data = corpus_tokenized.ravel()
data = encoder.transform(data)
data_index = np.repeat(np.arange(D), W)
</code></pre>
<p>Now a couple of diagnostic functions.</p>
<pre class="ipython" data-exports="both"><code>def print_top_words(vocab, phis, n):
    &#39;&#39;&#39;Prints the top words occuring within a topic.&#39;&#39;&#39;
    for i, p in enumerate(phis):
        z = list(zip(vocab, p))
        z.sort(key = lambda x: x[1], reverse=True)
        z = z[0:n]

        for word, percent in z:
            print(f&#39;Topic: {i:2}  Word: {word:10}  Percent: {percent:0.3f}&#39;)

        print()

def print_corpus_topics(corpus_tokenized, zs):
    &#39;&#39;&#39;Prints the corpus together with the topic assigned to each word.&#39;&#39;&#39;
    for d in range(zs.shape[0]):  # the document index
        for w in range(zs.shape[1]):  # the word index
            print(f&#39;({corpus_tokenized[d, w]}, {zs[d, w]})&#39;, end=&#39; &#39;)
        print(&#39;\n&#39;)
</code></pre>
<h2 id="the-unigram-model">The Unigram Model</h2>
<p>In this model, words from every document are drawn from a single categorical distribution.</p>
<p>Distribution of words in a document: <span class="math inline">\(\phi \sim Dir(\vec{\beta})\)</span>, where <span class="math inline">\(\vec{\beta}\)</span> is a vector of shape parameters</p>
<p>Distribution of tokens: <span class="math inline">\(W \sim Cat(\vec{\phi})\)</span></p>
<p><a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov-Chain Monte Carlo</a> is a technique for sampling a model to discover its posterior parameters statistically. When models become complex, it is often the case that analytic solutions for the parameters are intractable. We will use the <a href="https://docs.pymc.io/">PyMC3</a> package.</p>
<p>First we describe the model.</p>
<pre class="ipython" data-exports="both"><code># Pseudo-counts for each vocab word occuring in the documents.
beta = np.ones(V)

with pm.Model() as unigram_model:

    # Distribution of word-types in the corpus.
    phi = pm.Dirichlet(&#39;phi&#39;, a = beta)

    # The distribution of words.
    w = pm.Categorical(&#39;w&#39;, p = phi, observed = data)
</code></pre>
<p>Next we sample the model to create the posterior distribution.</p>
<pre class="ipython" data-exports="both"><code>with unigram_model:
    draw = 5000
    unigram_trace = pm.sample(5000, tune=1000, chains=4, progressbar=False)
</code></pre>
<p>And now we can see what the model determined the proportion of each word in the corpus was.</p>
<pre class="ipython" data-exports="both"><code>print_top_words(vocab, [unigram_trace.get_values(&#39;phi&#39;)[draw-1]], len(vocab))
</code></pre>
<pre class="example"><code>Topic:  0  Word: red         Percent: 0.150
Topic:  0  Word: watt        Percent: 0.145
Topic:  0  Word: car         Percent: 0.117
Topic:  0  Word: green       Percent: 0.080
Topic:  0  Word: battery     Percent: 0.073
Topic:  0  Word: volt        Percent: 0.068
Topic:  0  Word: yellow      Percent: 0.067
Topic:  0  Word: drive       Percent: 0.059
Topic:  0  Word: electric    Percent: 0.054
Topic:  0  Word: gas         Percent: 0.053
Topic:  0  Word: stop        Percent: 0.048
Topic:  0  Word: blue        Percent: 0.030
Topic:  0  Word: engine      Percent: 0.025
Topic:  0  Word: charge      Percent: 0.021
Topic:  0  Word: light       Percent: 0.011
Topic:  0  Word: amp         Percent: 0.002
</code></pre>
<h2 id="mixture-of-unigrams-naive-bayes">Mixture of Unigrams (Naive Bayes)</h2>
<p>In this model, each document is assigned a topic and each topic has its own distribution of words.</p>
<p>Distribution of topics to documents: <span class="math inline">\(\vec{\theta} \sim Dirichlet(\vec{\alpha})\)</span></p>
<p>Distribution of words to topics: <span class="math inline">\(\vec{\phi} \sim Dirichlet(\vec{\beta})\)</span></p>
<p>The topics: <span class="math inline">\(T \sim Categorical(\vec{\theta})\)</span></p>
<p>The tokens: <span class="math inline">\(W \sim Categorical(\vec{\phi})\)</span></p>
<pre class="ipython" data-exports="both"><code># Number of topics    
K = 3

# Pseudo-counts for topics and words.
alpha = np.ones(K)*0.8
beta = np.ones(V)*0.8

with pm.Model() as naive_model:
    # Global topic distribution
    theta = pm.Dirichlet(&quot;theta&quot;, a=alpha)

    # Word distributions for K topics
    phi = pm.Dirichlet(&quot;phi&quot;, a=beta, shape=(K, V))

    # Topic of documents
    z = pm.Categorical(&quot;z&quot;, p=theta, shape=D)

    # Words in documents
    p = phi[z][data_index]
    w = pm.Categorical(&quot;w&quot;, p=p, observed=data)
</code></pre>
<pre class="ipython" data-exports="both"><code>with naive_model:
    draw = 5000
    naive_trace = pm.sample(draw, tune=1000, chains=4, progressbar=False)
</code></pre>
<pre class="ipython" data-exports="both"><code>print_top_words(vocab, naive_trace[&#39;phi&#39;][draw-1], 5)
</code></pre>
<pre class="example"><code>Topic:  0  Word: drive       Percent: 0.177
Topic:  0  Word: car         Percent: 0.166
Topic:  0  Word: red         Percent: 0.126
Topic:  0  Word: blue        Percent: 0.108
Topic:  0  Word: green       Percent: 0.086

Topic:  1  Word: car         Percent: 0.238
Topic:  1  Word: green       Percent: 0.192
Topic:  1  Word: watt        Percent: 0.180
Topic:  1  Word: blue        Percent: 0.070
Topic:  1  Word: red         Percent: 0.045

Topic:  2  Word: volt        Percent: 0.161
Topic:  2  Word: car         Percent: 0.123
Topic:  2  Word: engine      Percent: 0.113
Topic:  2  Word: electric    Percent: 0.094
Topic:  2  Word: gas         Percent: 0.081
</code></pre>
<h2 id="latent-dirichlet-allocation-1">Latent Dirichlet Allocation</h2>
<p>In this model, each word is assigned a topic and topics are distributed varyingly within each document.</p>
<p>Distribution of topics within documents: <span class="math inline">\(\vec{\theta} \sim Dirichlet(\vec{\alpha})\)</span></p>
<p>Distribution of words to topics: <span class="math inline">\(\vec{\phi} \sim Dirichlet(\vec{\beta})\)</span></p>
<p>The topics: <span class="math inline">\(T \sim Categorical(\vec{\theta})\)</span></p>
<p>The tokens: <span class="math inline">\(W \sim Categorical(\vec{\phi})\)</span></p>
<pre class="ipython" data-exports="both"><code># Number of topics    
K = 3

# Pseudo-counts. Sparse to encourage separation.
alpha = np.ones((1, K))*0.5
beta = np.ones((1, V))*0.5

with pm.Model() as lda_model:
    # Distribution of topics within each document
    theta = pm.Dirichlet(&quot;theta&quot;, a=alpha, shape=(D, K))

    # Distribution of words within each topic
    phi = pm.Dirichlet(&quot;phi&quot;, a=beta, shape=(K, V))

    # The topic for each word
    z = pm.Categorical(&quot;z&quot;, p=theta, shape=(W, D))

    # Words in documents
    p = phi[z].reshape((D*W, V))
    w = pm.Categorical(&quot;w&quot;, p=p, observed=data)
</code></pre>
<pre class="ipython" data-exports="both"><code>with lda_model:
    draw = 5000
    lda_trace = pm.sample(draw, tune=1000, chains=4, progressbar=False)

print_top_words(tokens, lda_trace.get_values(&#39;phi&#39;)[draw-1], 4)
</code></pre>
<p>At the cost of some complexity, we can rewrite our model to handle a corpus with documents of varying lengths.</p>
<pre class="ipython" data-exports="both"><code>alpha = np.ones([D, K])*0.5  # prior weights for the topics in each document (pseudo-counts)
beta  = np.ones([K, V])*0.5  # prior weights for the vocab words in each topic (pseudo-counts)

sequence_data = np.reshape(np.array(data), (D,W))
N = np.repeat(W, D)  # this model needs a list of document lengths

with pm.Model() as sequence_model:

    # distribution of the topics occuring in a particular document
    theta   = pm.Dirichlet(&#39;theta&#39;, a=alpha, shape=(D, K))

    # distribution of the vocab words occuring in a particular topic
    phi     = pm.Dirichlet(&#39;phi&#39;, a=beta, shape=(K, V))

    # the topic for a particular word in a particular document: shape = (D, N[d])
    # theta[d] is the vector of category probabilities for each topic in 
    # document d.
    z = [pm.Categorical(&#39;z_{}&#39;.format(d), p = theta[d], shape=N[d])
          for d in range(D)]

    # the word occuring at position n, in a particular document d: shape = (D, N[d]) 
    # z[d] is the vector of topics for document d
    # z[d][n] is the topic for word n in document d
    # phi[z[d][n]] is the distribution of words for topic z[d][n]
    # [d][n] is the n-th word observed in document d
    w = [pm.Categorical(&#39;w_{}_{}&#39;.format(d, n), p=phi[z[d][n]],
                        observed = sequence_data[d][n])
         for d in range(D) for n in range(N[d])]

with sequence_model:
    draw = 5000
    sequence_trace = pm.sample(draw, tune=1000, chains=4, progressbar=False)

print_top_words(tokens, sequence_trace.get_values(&#39;phi&#39;)[4999], 4)
</code></pre>
<p>And here we can see what topic the model assigned to each token in the corpus.</p>
<pre class="ipython" data-exports="both"><code>zs = [sequence_trace.get_values(&#39;z_{}&#39;.format(d))[draw-1] for d in range(D)]
zs = np.array(zs)

print_corpus_topics(corpus_tokenized, zs)
</code></pre>
<pre class="example"><code>(red, 2) (blue, 0) (green, 0) (green, 0) (blue, 0) (blue, 0) (red, 2) (red, 0) (blue, 0) (yellow, 0) 

(car, 1) (light, 2) (red, 1) (stop, 1) (stop, 1) (car, 1) (car, 1) (drive, 1) (green, 2) (yellow, 1) 

(car, 1) (engine, 1) (gas, 1) (stop, 1) (battery, 1) (engine, 1) (drive, 1) (car, 0) (electric, 1) (gas, 1) 

(watt, 0) (volt, 0) (volt, 0) (amp, 1) (battery, 1) (watt, 0) (volt, 0) (electric, 0) (volt, 0) (charge, 0) 

</code></pre>
<p>Since we chose to distribute words among three topics, we can examine the distributions of these topics to each document on a simplex. Below, each triangle represents a document and each corner represents a topic. Whenever the sampled points cluster at a corner, that means our model decided that that document was predominantly about the corresponding topic.</p>
<pre class="ipython" data-exports="both"><code>with sequence_model:
    pps = pm.sample_posterior_predictive(sequence_trace, vars=[theta], samples=1000, progressbar=False)

var = pps[&#39;theta&#39;]
thetas = sequence_trace[&#39;theta&#39;][4999]
nthetas = thetas.shape[0]

blue = sns.color_palette(&#39;Blues_r&#39;)[0]
fig = plt.figure()
fig.suptitle(&#39;Distribution of Topics to Documents&#39;, fontsize=16)
for i, ts in enumerate(thetas):
    plt.subplot(2, nthetas/2, i + 1)
    sp.plot_points(var[:,i], color=blue, marker=&#39;o&#39;, alpha=0.1, markersize=3)
    title = r&#39;$\theta_{0}$ = ({1[0]:.3f}, {1[1]:.3f}, {1[2]:.3f})&#39;.format(i,ts)
    plt.title(title, fontdict={&#39;fontsize&#39;: 14})
</code></pre>
<figure><img src="/images/distribution.png" alt="random sample of documents in the dirichlet model"/></figure>

<p>That’s all for now!</p>
<h1 id="references">References</h1>
<p>Blei, David M, Andrew Y Ng and Michael I Jordan. 2003. “Latent dirichlet allocation.” Journal of machine Learning research.</p>
<p><a href="https://stackoverflow.com/questions/31473459/pymc3-how-to-implement-latent-dirichlet-allocation" class="uri">https://stackoverflow.com/questions/31473459/pymc3-how-to-implement-latent-dirichlet-allocation</a></p>
<p><a href="https://github.com/junpenglao/Planet_Sakaar_Data_Science/blob/master/PyMC3QnA/discourse_2314.ipynb" class="uri">https://github.com/junpenglao/Planet_Sakaar_Data_Science/blob/master/PyMC3QnA/discourse_2314.ipynb</a></p>]]></description>
    <pubDate>Wed, 30 Jan 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/bayesian-topic-modeling/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>
<item>
    <title>Optimal Decision Boundaries</title>
    <link>https://mathformachines.com/posts/decision/index.html</link>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Over the next few posts, we will investigate <em>decision boundaries</em>. A decision boundary is a graphical representation of the solution to a classification problem. Decision boundaries can help us to understand what kind of solution might be appropriate for a problem. They can also help us to understand the how various machine learning classifiers arrive at a solution.</p>
<p>In this post, we will look at a problem’s <em>optimal</em> decision boundary, which we can find when we know exactly how our data was generated. The optimal decision boundary represents the “best” solution possible for that problem. Consequently, by looking at the complexity of this boundary and at how much error it produces, we can get an idea of the inherent difficulty of the problem.</p>
<p>Unless we have generated the data ourselves, we won’t usually be able to find the optimal boundary. Instead, we approximate it using a classifier. A good machine learning classifier tries to approximate the optimal boundary for a problem as closely as possible.</p>
<p>In future posts, we will look at the approximating boundary created by various classification algorithms. We will investigate the strategy the classifier uses to create this boundary and how this boundary evolves as the classifier is trained on more and more data. There are many classification algorithms available to a data scientist – regression, discriminant analysis, decision trees, neural networks, to name a few – and it is important to understand which algorithm is appropritate for the problem at hand. Decision boundaries can help us to do this.</p>
<video autoplay loop mutued playsinline controls>
  <source src="/images/rf_mix.mp4" type="video/mp4">
  <source src="/images/rf_mix.webm" type="video/webm">
  <source src="/images/rf_mix.ogv" type="video/ogg">
</video>

<h1 id="optimal-boundaries">Optimal Boundaries</h1>
<p>A classification problem asks: given some observations of a thing, what is the best way to assign that thing to a class based on some of its features? For instance, we might want to predict whether a person will like a movie or not based on some data we have about them, the “features” of that person.</p>
<p>A solution to the classification problem is a rule that partitions the features and assigns each all the features of a partition to the same class. The “boundary” of this partitioning is the <strong>decision boundary</strong> of the rule.</p>
<p>It might be that two observations have exactly the same features, but are assigned to different classes. (Two things that look the same in the ways we’ve observed might differ in ways we haven’t observed.) In terms of probabilities this means both <span class="math display">\[P(C = 0 \mid X) \gt 0\]</span> and <span class="math display">\[P(C = 1 \mid X) \gt 0\]</span>. In other words, we might not be able with full certainty to classify an observation. We could however assign the observation to its <em>most probable</em> class. This gives us the decision rule <span class="math display">\[ \hat{C} = \operatorname*{argmax}_c P(C = c \mid X) \]</span></p>
<p>The boundary that this rule produces is the <strong>optimal decision boundary</strong>. It is the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">MAP estimate</a> of the class label, and it is the rule that minimizes classification error under the <a href="https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function">zero-one loss function</a>. We will look at error and loss more in a future post.</p>
<p>We will consider <em>binary</em> classification problems, meaning, there will only be two possible classes, 0 or 1. For a binary classification problem, the optimal boundary occurs at those points where each class is equally probable: <span class="math display">\[ P(C = 0 \mid X) = P(C = 1 \mid X) \]</span></p>
<h1 id="prepare-r">Prepare R</h1>
<p>We will use R to do our analysis. We’ll have a chance to try out <code>gganimate</code> and <code>patchwork</code>, a couple of newer packages that <a href="https://www.data-imaginist.com/">Thomas Lin Pedersen</a> has been working on; they are really nice.</p>
<p>Here we’ll define some functions to produce plots of our examples. All of these assume a classification problem where our response is binary, <span class="math inline">\(C \in \{0, 1\}\)</span>, and is predicted by two continuous features, <span class="math inline">\((X, Y)\)</span>.</p>
<p>Briefly, they are</p>
<ol>
<li><code>gg_sample</code> :: creates a layer for a sample of the features colored by class.</li>
<li><code>gg_density</code> :: creates a layer of contour plots for feature densities within each class.</li>
<li><code>gg_optimal</code> :: creates a layer showing an optimal decision boundary.</li>
<li><code>gg_mix_label</code> :: creates a layer labelling components in a mixture distribution.</li>
</ol>
<p>

<pre class="r"><code>library(magrittr)
library(tidyverse)
library(ggplot2)
library(gganimate)
library(patchwork)

theme_set(theme_linedraw() +
          theme(plot.title = element_text(size = 20),
                legend.position = &quot;none&quot;,
                axis.text.x = element_blank(),
                axis.text.y = element_blank(),
                axis.title.x = element_blank(),
                axis.title.y = element_blank(),
                aspect.ratio = 1))

#&#39; Make a sample layer
#&#39;
#&#39; @param data data.frame: a sample with continuous features `x` and `y`
#&#39; grouped by factor `class`
#&#39; @param classes (optional) a vector of which levels of `class` to
#&#39; plot; default is to plot data from all classes
gg_sample &lt;- function(data, classes = NULL, size = 3, alpha = 0.5, ...) {
    if (is.null(classes)) {
        subdata &lt;- data
    } else {
        subdata &lt;- filter(data, class %in% classes)
    }
    list(geom_point(data = subdata,
                    aes(x, y,
                        color = factor(class),
                        shape = factor(class)),
                    size = size,
                    alpha = alpha,
                    ...),
         scale_colour_discrete(drop = TRUE,
                               limits = levels(factor(data$class))))
}

#&#39; Make a density layer
#&#39;
#&#39; @param data data.frame: a data grid of features `x` and `y` with contours `z`
#&#39; @param data character: the name of the contour column 
gg_density &lt;- function(data, z, size = 1, color = &quot;black&quot;, alpha = 1, ...) {
    z &lt;- ensym(z)
    geom_contour(data = data,
                 aes(x, y, z = !!z),
                 size = size,
                 color = color,
                 alpha = alpha,
                 ...)
}

#&#39; Make an optimal boundary layer
#&#39;
#&#39; @param data data.frame: a data grid of features `x` and `y` with a column with
#&#39; the `optimal` boundary contours
#&#39; @param breaks numeric: which contour levels of `optimal` to plot
gg_optimal &lt;- function(data, breaks = c(0), ...) {
    gg_density(data, z = optimal, breaks = breaks, ...)
}

#&#39; Make a layer of component labels for a mixture distribution with two classes
#&#39;
#&#39; @param mus list(data.frame): the means for components of each class; every row
#&#39; is a mean, each column is a coordinate
#&#39; @param classes (optional) a vector of which levels of class to plot
gg_mix_label &lt;- function(mus, classes = NULL, size = 10, ...) {
    ns &lt;- map_int(mus, nrow)
    component &lt;- do.call(c, map(ns, seq_len))
    class &lt;- do.call(c, map2(0:(length(ns) - 1), ns, rep.int))
    mu_all &lt;- do.call(rbind, mus)
    data &lt;- cbind(mu_all, component, class) %&gt;%
        set_colnames(c(&quot;x&quot;, &quot;y&quot;, &quot;component&quot;, &quot;class&quot;)) %&gt;%
        as_tibble()
    if (is.null(classes)) {
        subdata &lt;- data
    } else {
        subdata &lt;- filter(data, class %in% classes)
    }    
    list(shadowtext::geom_shadowtext(data = subdata,
                                     mapping = aes(x, y,
                                                   label = component,
                                                   color = factor(class)),
                                     size = size,
                                     ...),
         scale_colour_discrete(drop = TRUE,
                               limits = levels(factor(data$class))))
}

</code></pre>
<h1 id="decision-boundaries-for-continuous-features">Decision Boundaries for Continuous Features</h1>
<p>Decision boundaries are most easily visualized whenever we have <em>continuous</em> features, most especially when we have <em>two</em> continuous features, because then the decision boundary will exist in a plane.</p>
<p>With two continuous features, the feature space will form a plane, and a decision boundary in this feature space is a set of one or more curves that divide the plane into distinct regions. Inside of a region, all observations will be assigned to the same class.</p>
<p>As mentioned above, whenever we know exactly how our data was generated, we can produce the optimal decision boundary. Though this won’t usually be possible in practice, investigating the optimal boundaries produced from simulated data can still help us to understand their properties.</p>
<p>We will look at the optimal boundary for a binary classification problem on a with features on a couple of common distributions: a multivariate normal distribution and a mixture of normal distributions.</p>
<h2 id="normally-distributed-features">Normally Distributed Features</h2>
<p>In a binary classification problem, whenever the features for each class jointly have a multivariate normal distribution, the optimal decision boundary is relatively simple. We will start our investigation here.</p>
<p>With two features, the feature space is a plane. It can be shown that the optimal decision boundary in this case will either be a line or a <a href="https://en.wikipedia.org/wiki/Conic_section">conic section</a> (that is, an ellipse, a parabola, or a hyperbola). With higher dimesional feature spaces, the decision boundary will form a <a href="https://en.wikipedia.org/wiki/Hyperplane">hyperplane</a> or a <a href="https://en.wikipedia.org/wiki/Quadric">quadric surface</a>.</p>
<p>We will consider classification problems with two classes, <span class="math inline">\(C = {0, 1}\)</span>, and two features, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Each class will be Bernoulli distributed and the features for each class will be distributed normally. Specifically,</p>
<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><span class="math inline">\( C \sim \operatorname{Bernoulli}(p) \)</span></td>
</tr>
<tr class="even">
<td>Features for Class 0</td>
<td><span class="math inline">\( (X, Y) \mid C = 0 \sim \operatorname{Normal}(\mu_0, \Sigma_0) \)</span></td>
</tr>
<tr class="odd">
<td>Features for Class 1</td>
<td><span class="math inline">\( (X, Y) \mid C = 1 \sim \operatorname{Normal}(\mu_0, \Sigma_1) \)</span></td>
</tr>
</tbody>
</table>
<p>Our goal is to produce two kinds of visualizations: one, of a sample from these distributions, and two, the contours of the class-conditional densities for each feature. We’ll use the <code>mvnfast</code> package to help us with computations on the joint MVN.</p>
<h3 id="samples">Samples</h3>
<p>Let’s choose some values for our parameters. We’ll start with the case when the classes occur equally often. For our features, we’ll choose means so that there is some significant overlap between the two classes, and covariance matrices so that the distributions have a nice elliptical shape.</p>
<pre class="r"><code>p &lt;- 0.5
mu_0 &lt;- c(0, 2)
sigma_0 &lt;- matrix(c(1, 0.3, 0.3, 1), nrow = 2)
mu_1 &lt;- c(2, 0)
sigma_1 &lt;- matrix(c(1, -0.3, -0.3, 1), nrow = 2)
</code></pre>
<p>Now we’ll write a function to create a dataframe containing a sample of classified features from our distribution.</p>
<pre class="r"><code>#&#39; Generate normally distributed feature samples for a binary
#&#39; classification problem
#&#39;
#&#39; @param n integer: the size of the sample
#&#39; @param mean_0 vector: the mean vector of the first class
#&#39; @param sigma_0 matrix: the 2x2 covariance matrix of the first class
#&#39; @param mean_1 vector: the mean vector of the second class
#&#39; @param sigma_1 matrix: the 2x2 covariance matrix of the second class
#&#39; @param p_0 double: the prior probability of class 0
make_mvn_sample &lt;- function(n, mu_0, sigma_0, mu_1, sigma_1, p_0) {
    n_0 &lt;- rbinom(1, n, p_0)
    n_1 &lt;- n - n_0
    sample_mvn &lt;- as_tibble(
        rbind(mvnfast::rmvn(n_0,
                            mu = mu_0,
                            sigma = sigma_0),
              mvnfast::rmvn(n_1,
                            mu = mu_1,
                            sigma = sigma_1)))
    sample_mvn[1:n_0, 3] &lt;- 0
    sample_mvn[(n_0 + 1):(n_0 + n_1), 3] &lt;- 1
    sample_mvn &lt;- sample_mvn[sample(nrow(sample_mvn)), ]
    colnames(sample_mvn) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;class&quot;)
    sample_mvn
}

</code></pre>
<p>Finally, we’ll create a sample of 4000 points and plot the result.</p>
<pre class="r"><code>n &lt;- 4000
set.seed(31415)
sample_mvn &lt;- make_mvn_sample(n,
                              mu_0, sigma_0,
                              mu_1, sigma_1,
                              p)

ggplot() +
    gg_sample(sample_mvn) +
    coord_fixed()
</code></pre>
<figure>
<img src="/images/sample_mvn.png" title="sample-mvn" alt="A sample of the feature distributions for each class." width="400" /><figcaption>A sample of the feature distributions for each class.</figcaption>
</figure>
<p>It should be apparent that because of the overlap in these distributions, any decision rule will necessarily misclassify some observations fairly often.</p>
<h3 id="classes-on-the-feature-space">Classes on the Feature Space</h3>
<p>Next, we will produce some contour plots of our feature distributions. Let’s write a function to generate class probabilities at any observation <span class="math inline">\((x, y)\)</span> in the feature space; we will model the optimal decision boundary as those points where the posterior probabilities of the two classes are equal, that is, where <span class="math display">\[ P(X, Y \mid C = 0) P(C = 0) - P(X, Y \mid C = 1) P(C = 1) = 0 \]</span></p>
<pre class="r"><code>#&#39; Make an optimal prediction at a point from two class distributions
#&#39;
#&#39; @param x vector: input
#&#39; @param p_0 double: prior probability of class 0
#&#39; @param dfun_0 function(x): density of features of class 0
#&#39; @param dfun_1 function(x): density of features of class 1
optimal_predict &lt;- function(x, p_0, dfun_0, dfun_1) {
    ## Prior probability of class 1
    p_1 &lt;- 1 - p_0
    ## Conditional probability of (x, y) given class 0
    p_x_0 &lt;- dfun_0(x)
    ## Conditional probability of (x, y) given class 1
    p_x_1 &lt;- dfun_1(x)
    ## Conditional probability of class 0 given (x, y)
    p_0_xy &lt;- p_x_0 * p_0
    ## Conditional probability of class 1 given (x, y)
    p_1_xy &lt;- p_x_1 * p_1
    optimal &lt;- p_1_xy - p_0_xy
    class &lt;- ifelse(optimal &gt; 0, 1, 0)
    result &lt;- c(p_0_xy, p_1_xy, optimal, class)
    names(result) &lt;- c(&quot;p_0_xy&quot;, &quot;p_1_xy&quot;, &quot;optimal&quot;, &quot;class&quot;)
    result
}

#&#39; Construct a dataframe with posterior class probabilities and the
#&#39; optimal decision boundary over a grid on the feature space
#&#39; 
#&#39; @param mean_0 vector: the mean vector of the first class
#&#39; @param sigma_0 matrix: the 2x2 covariance matrix of the first class
#&#39; @param mean_1 vector: the mean vector of the second class
#&#39; @param sigma_1 matrix: the 2x2 covariance matrix of the second class
#&#39; @param p_0 double: the prior probability of class 0
make_density_mvn &lt;- function(mean_0, sigma_0, mean_1, sigma_1, p_0,
                             x_min, x_max, y_min, y_max, delta = 0.05) {
    x &lt;- seq(x_min, x_max, delta)
    y &lt;- seq(y_min, y_max, delta)
    density_mvn &lt;- expand.grid(x, y)
    names(density_mvn) &lt;- c(&quot;x&quot;, &quot;y&quot;)
    dfun_0 &lt;- function(x) mvnfast::dmvn(x, mu_0, sigma_0)
    dfun_1 &lt;- function(x) mvnfast::dmvn(x, mu_1, sigma_1)
    optimal_mvn &lt;- function(x, y) optimal_predict(c(x, y), p_0, dfun_0, dfun_1)
    density_mvn &lt;-as.tibble(
        cbind(density_mvn,
              t(mapply(optimal_mvn,
                       density_mvn$x, density_mvn$y))))
    density_mvn
}

</code></pre>
<p>Now we can generate a grid of points and compute posterior class probabilities over that grid. By plotting these probabilities, we can get describe both the conditional feature distributions for each class as well as the joint feature distribution.</p>
<pre class="r"><code>density_mvn &lt;- make_density_mvn(mu_0, sigma_0, mu_1, sigma_1, p,
                                -3, 5, -3, 5)

(ggplot() +
 gg_sample(sample_mvn, alpha = 0.1) +
 gg_density(density_mvn, z = p_0_xy) +
 gg_density(density_mvn, z = p_1_xy) +
 ggtitle(&quot;Conditional Distributions&quot;)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.1) +
 geom_contour(data = density_mvn,
              aes(x = x, y = y, z = p_0_xy + p_1_xy),
              size = 1,
              color = &quot;black&quot;) +
 ggtitle(&quot;Joint Distribution&quot;))

</code></pre>
<figure>
<img src="/images/density_mvn.png" title="density-mvn" alt="Contours of the feature distributions for each class." width="800" /><figcaption>Contours of the feature distributions for each class.</figcaption>
</figure>
<h3 id="the-optimal-decision-boundary">The Optimal Decision Boundary</h3>
<p>Now let’s add a plot for the optimal decision boundary for this problem.</p>
<pre class="r"><code>(ggplot() +
 gg_density(density_mvn, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mvn, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mvn)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.25) +
 gg_optimal(density_mvn)) +
plot_annotation(&quot;The Optimal Decision Boundary&quot;)

</code></pre>
<figure>
<img src="/images/optimal_mvn.png" title="optimal-mvn" alt="The optimal decision boundary" width="800" /><figcaption>The optimal decision boundary</figcaption>
</figure>
<p>Notice how the boundary runs through the points where the contours of the two conditional distributions intersect. These points of intersection are where the classes have equal posterior probability.</p>
<h2 id="mixture-of-normals">Mixture of Normals</h2>
<p>The features of each class might also be modeled as a <em>mixture</em> of normal distributions. This means that each observation in a class will come from one of <em>several</em> normal distributions; in our case, the distributions from a class will be joined by a common hyperparameter, their mean.</p>
<p>In description, at least, the problem is still relatively simple. The possible decision boundaries produced, however, can be quite complex. This is a much more difficult problem than the one we saw before.</p>
<p>For our examples, we will generate the data as follows:</p>
<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><span class="math inline">\( C \sim Bernoulli(p) \)</span></td>
</tr>
<tr class="even">
<td>Mean of Means for Class 0</td>
<td><span class="math inline">\( \nu_0 \sim Normal((0, 1), I) \)</span></td>
</tr>
<tr class="odd">
<td>Mean of Means for Class 1</td>
<td><span class="math inline">\( \nu_0 \sim Normal((1, 0), I) \)</span></td>
</tr>
<tr class="even">
<td>Means of Components for Class 0</td>
<td><span class="math inline">\( \mu_{0, i=1, \ldots, n_0} \sim Normal(\nu_0, I) \)</span></td>
</tr>
<tr class="odd">
<td>Means of Components for Class 1</td>
<td><span class="math inline">\( \mu_{1, i=1, \ldots, n_1} \sim Normal(\nu_1, I) \)</span></td>
</tr>
<tr class="even">
<td>Features for Class 0</td>
<td><span class="math inline">\( (X, Y) \mid C = 0 \sim w_{0, 1} Normal(\mu_{0, 1}, \Sigma_0) + \cdots + w_{0, l_0} Normal(\mu_{0, 0}, \Sigma_0) \)</span></td>
</tr>
<tr class="odd">
<td>Features for Class 1</td>
<td><span class="math inline">\( (X, Y) \mid C = 1 \sim w_{1, 1} Normal(\mu_{1, 1}, \Sigma_1) + \cdots + w_{1, l_1} Normal(\mu_{1, l_1}, \Sigma_1) \)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(n_0\)</span> is the number of components for class 0, <span class="math inline">\(w_{0, i}\)</span> are the weights on each component, <span class="math inline">\(\Sigma_0 = \frac{1}{2 * l_0} I\)</span>, and <span class="math inline">\(I\)</span> is the identity matrix; similarly for class 1.</p>
<p>This is a bit awful, but we are basically doing this:</p>
<p>For each class, define the distribution of the features <span class="math inline">\((X, Y)\)</span> by</p>
<ol>
<li>Choosing the number of components to go in the mixture.</li>
<li>Choosing a mean for each component by sampling from a normal distribution.</li>
</ol>
<p>Then, to get a sample: Get an observation by</p>
<ol>
<li>Choosing a class, 0 or 1.</li>
<li>Choosing a component from that class, a normal distribution.</li>
<li>Sample the observation from that component.</li>
</ol>
<h3 id="samples-1">Samples</h3>
<p>The computations for the mixture of MVNs are fairly similar to the ones we did before. First let’s define a sampling function. This function just implements the above steps.</p>
<pre class="r"><code>#&#39; Generate normally distributed feature samples for a binary
#&#39; classification problem
#&#39;
#&#39; @param n integer: the size of the sample
#&#39; @param nu_0 numeric: the average mean of the components of the first feature
#&#39; @param sigma_0 matrix: covariance of components of the first feature
#&#39; @param n_0 integer: class frequency of first feature in the sample
#&#39; @param w_0 numeric: vector of weights for components of the first feature
#&#39; @param mean_1 numeric: the average mean of the components of the second feature
#&#39; @param sigma_1 matrix: covariance of components of the second feature
#&#39; @param n_1 integer: class frequency of second feature in the sample
#&#39; @param w_1 numeric: vector of weights for components of the second feature
#&#39; @param p_0 double: the prior probability of class 0
make_mix_sample &lt;- function(n,
                            nu_0, tau_0, n_0, sigma_0, w_0,
                            nu_1, tau_1, n_1, sigma_1, w_1,
                            p_0) {
    ## Number of Components for Each Class
    l_0 &lt;- length(w_0)
    l_1 &lt;- length(w_1)
    ## Sample the Component Means
    mu_0 &lt;- mvnfast::rmvn(n = l_0,
                          mu = nu_0, sigma = tau_0)
    mu_1 &lt;- mvnfast::rmvn(n = l_1,
                          mu = nu_1, sigma = tau_1)
    ## Class Frequency in the Sample
    n_0 &lt;- rbinom(1, n, p_0)
    n_1 &lt;- n - n_0
    ## Sample the Features
    f_0 &lt;- mvnfast::rmixn(n = n_0,
                          mu = mu_0, sigma = sigma_0, w = w_0,
                          retInd = TRUE)
    c_0 &lt;- attr(f_0, &quot;index&quot;)
    f_1 &lt;- mvnfast::rmixn(n = n_1,
                          mu = mu_1, sigma = sigma_1, w = w_1,
                          retInd = TRUE)
    c_1 &lt;- attr(f_1, &quot;index&quot;)
    sample_mix &lt;- as.data.frame(rbind(f_0, f_1))
    sample_mix[, 3] &lt;- c(c_0, c_1)
    ## Define Classes
    sample_mix[1:n_0, 4] &lt;- 0
    sample_mix[(n_0 + 1):(n_0 + n_1), 4] &lt;- 1
    sample_mix &lt;- sample_mix[sample(nrow(sample_mix)), ]
    names(sample_mix) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;component&quot;, &quot;class&quot;)
    ## Store Component Means
    attr(sample_mix, &quot;mu_0&quot;) &lt;- mu_0
    attr(sample_mix, &quot;mu_1&quot;) &lt;- mu_1
    sample_mix
}

</code></pre>
<p>Now we’ll define the parameters, construct a sample, and look at the result.</p>
<pre class="r"><code>
## Bernoulli parameter for class distribution
p = 0.5
## Mean of component means
nu_0 = c(0, 1)
nu_1 = c(1, 0)
## Covariance for component means
tau_0 = matrix(c(1, 0, 0, 1), nrow = 2)
tau_1 = matrix(c(1, 0, 0, 1), nrow = 2)
## Number of components for each class
n_0 &lt;- 10
n_1 &lt;- 10
## Covariance for each class
sigma_0 &lt;- replicate(n_0, matrix(c(1, 0, 0, 1), 2) / n_0 * 2,
                     simplify = FALSE)
sigma_1 &lt;- replicate(n_1, matrix(c(1, 0, 0, 1), 2) / n_1 * 2,
                     simplify = FALSE)
## Weights of mixture components
w_0 &lt;- rep(1 / n_0, n_0)
w_1 &lt;- rep(1 / n_1, n_1)

## Sample size
n &lt;- 4000
set.seed(31)
sample_mix &lt;- make_mix_sample(n,
                              nu_0, tau_0, n_0, sigma_0, w_0,
                              nu_1, tau_1, n_1, sigma_1, w_1,
                              p)
## Retrieve the generated component means
mu_0 &lt;- attr(sample_mix, &quot;mu_0&quot;)
mu_1 &lt;- attr(sample_mix, &quot;mu_1&quot;)

ggplot() +
    gg_sample(sample_mix) +
    ggtitle(&quot;Sample of Mixture Distribution&quot;)

ggplot() +
    gg_sample(sample_mix) +
    gg_mix_label(list(mu_0, mu_1)) +
    facet_wrap(vars(class)) +
    ggtitle(&quot;Feature Components&quot;)

</code></pre>
<figure>
<img src="/images/sample_mix.png" title="sample-mix" alt="A sample from the mixture distributions." width="400" /><figcaption>A sample from the mixture distributions.</figcaption>
</figure>
<p>We’ve labelled the component means for each class. (There are 10 components for class 0, and 10 components for class 1.) You can see that around each of these labels is a sample from a normal distribution.</p>
<h3 id="classes-on-the-feature-space-1">Classes on the Feature Space</h3>
<p>Now we’ll compute class probabilities on the feature space.</p>
<p>First define a generating function.</p>
<pre class="r"><code>#&#39; Construct a dataframe with posterior class probabilities and the
#&#39; optimal decision boundary over a grid on the feature space
#&#39; 
#&#39; @param mean_0 numeric: the average mean of the components of the first feature
#&#39; @param sigma_0 matrix: covariance of components of the first feature
#&#39; @param w_0 numeric: vector of weights for components of the first feature
#&#39; @param mean_1 numeric: the average mean of the components of the second feature
#&#39; @param sigma_1 matrix: covariance of components of the second feature
#&#39; @param w_1 numeric: vector of weights for components of the second feature
#&#39; @param p_0 double: the prior probability of class 0
make_density_mix &lt;- function(mean_0, sigma_0, w_0,
                             mean_1, sigma_1, w_1, p_0,
                             x_min, x_max, y_min, y_max, delta = 0.05) {
    x &lt;- seq(x_min, x_max, delta)
    y &lt;- seq(y_min, y_max, delta)
    density_mix &lt;- expand.grid(x, y)
    names(density_mix) &lt;- c(&quot;x&quot;, &quot;y&quot;)
    dfun_0 &lt;- function(x) mvnfast::dmixn(matrix(x, nrow = 1),
                                         mu = mean_0,
                                         sigma = sigma_0,
                                         w = w_0)
    dfun_1 &lt;- function(x) mvnfast::dmixn(matrix(x, nrow = 1),
                                         mu = mean_1,
                                         sigma = sigma_1,
                                         w = w_1)
    optimal_mix &lt;- function(x, y) optimal_predict(c(x, y), p_0, dfun_0, dfun_1)
    density_mix &lt;-as.tibble(
        cbind(density_mix,
              t(mapply(optimal_mix,
                       density_mix$x, density_mix$y))))
    density_mix
}
</code></pre>
<p>And now compute the grid and plot.</p>
<pre class="r"><code>density_mix &lt;- make_density_mix(mu_0, sigma_0, w_0, mu_1, sigma_1, w_1, p,
                                -3, 5, -3, 5)

(ggplot() +
 gg_sample(sample_mix, classes = 0,
           alpha = 0.1) +
 gg_density(density_mix, z = p_0_xy) +
 gg_mix_label(list(mu_0, mu_1), classes = 0) +
 ggtitle(&quot;Density of Class 0&quot;)) +
(ggplot() +
 gg_sample(sample_mix, classes = 1,
           alpha = 0.1) +
 gg_density(density_mix, z = p_1_xy) +
 gg_mix_label(list(mu_0, mu_1), classes = 1) +
 ggtitle(&quot;Density of Class 1&quot;)) +
(ggplot() +
 gg_sample(sample_mix,
           alpha = 0.1) +
 geom_contour(data = density_mix,
              aes(x = x, y = y, z = p_0_xy + p_1_xy),
              color = &quot;black&quot;,
              size = 1) +
 ggtitle(&quot;Joint Density&quot;))

</code></pre>
<figure>
<img src="/images/density_mix.png" title="density-mix" alt="Contours of the feature distributions for each class." width="1000" /><figcaption>Contours of the feature distributions for each class.</figcaption>
</figure>
<h1 id="the-optimal-decision-boundary-1">The Optimal Decision Boundary</h1>
<p>And here is the optimal decision boundary for this problem. Notice how again the boundary runs through points of intersection in the two conditional distributions, and how it separates the classes of observations in the sample.</p>
<pre class="r"><code>(ggplot() +
 gg_density(density_mix, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mix, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mix)) +
(ggplot() +
 gg_sample(sample_mix, alpha = 0.25) +
 gg_optimal(density_mix))
</code></pre>
<figure>
<img src="/images/optimal_mix.png" title="optimal-mix" alt="The optimal decision boundary." width="800" /><figcaption>The optimal decision boundary.</figcaption>
</figure>
<h1 id="class-imbalance">Class Imbalance</h1>
<p>So far, we’ve only seen the case where the two classes occur about equally often. If one class has a lower probability of occuring (say class 1), then the optimal decision boundary must move toward the class 1 distribution in order to equalize the probabilities on either side. This should help illustrate why it’s important to consider class imbalance whenever you’re working on a classification problem. A large imbalance can change your decisions drastically.</p>
<p>To see this change, we will use the <code>gganimate</code> package to produce an animation showing how the optimal boundary changes as the Bernoulli parameter (the frequency of class 0) changes from 0.1 to 0.9.</p>
<h2 id="normally-distributed-features-1">Normally Distributed Features</h2>
<pre class="r"><code>## Evaluate mu_0, sigma_0, etc. again, if needed.

density_p0 &lt;-
    map_dfr(seq(0.1, 0.9, 0.005),
            function(p_0)
                make_density_mvn(mu_0, sigma_0, mu_1, sigma_1,
                                 p_0, -3, 5, -3, 5) %&gt;%
                mutate(p_0 = p_0))

anim &lt;- ggplot() +
    geom_contour(data = density_p0,
                 aes(x = x, y = y, z = p_0_xy + p_1_xy),
                 color = &quot;black&quot;,
                 size = 1,
                 alpha = 0.25) +
    gg_optimal(density_p0) +
    transition_manual(p_0) +
    ggtitle(&quot;Proportion of Class 0: {current_frame}&quot;)

anim &lt;- animate(anim, renderer = gifski_renderer(),
                width = 800, height = 800)

anim
</code></pre>
<video autoplay loop mutued playsinline controls>
  <source src="/images/imbalance_mvn.webm" type="video/webm">
  <source src="/images/imbalance_mvn.mp4" type="video/mp4">
  <source src="/images/imbalance_mvn.ogv" type="video/ogg">
</video>

<h2 id="mixture-of-normals-1">Mixture of Normals</h2>
<pre class="r"><code>density_mix_p0 &lt;-
    map_dfr(seq(0.1, 0.9, 0.005),
            function(p_0)
                make_density_mix(mu_0, sigma_0, w_0, mu_1, sigma_1, w_1,
                                 p_0, -3, 5, -3, 5) %&gt;%
                mutate(p_0 = p_0))
anim &lt;- ggplot() +
    geom_contour(data = density_mix_p0,
                 aes(x = x, y = y, z = p_0_xy + p_1_xy),
                 color = &quot;black&quot;,
                 size = 1,
                 alpha = 0.25) +
    gg_optimal(density_mix_p0) +
    transition_manual(p_0) +
    ggtitle(&quot;Proportion of Class 0: {current_frame}&quot;)

anim &lt;- animate(anim, renderer = gifski_renderer(),
                width = 800, height = 800)

anim

</code></pre>
<video autoplay loop mutued playsinline>
  <source src="/images/imbalance_mix.webm" type="video/webm">
  <source src="/images/imbalance_mix.mp4" type="video/mp4">
  <source src="/images/imbalance_mix.ogg" type="video/ogg">
</video>

<h1 id="conclusion">Conclusion</h1>
<p>In this post, we reviewed <strong>decision boundaries</strong>, a way of visualizing classification rules. In particular, we looked at <strong>optimal</strong> decision boundaries, which represent the <em>best</em> solution possible to a problem given certain costs for misclassification. The rule we used in this post was the <strong>MAP</strong> estimate, which minimizes zero-one loss, where all misclassifications are equally likely.</p>
<p>In future posts, we’ll look other kinds of loss functions and how that can affect the decision rule, and also at the boundaries produced by a number of statistical learning models.</p>
<p>Hope you enjoyed it!</p>]]></description>
    <pubDate>Thu, 09 Jan 2020 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/decision/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>

    </channel>
</rss>
