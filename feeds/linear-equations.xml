<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Math for Machines</title>
        <link>https://mathformachines.com</link>
        <description><![CDATA[A blog about data science and machine learning, with a lot of math.]]></description>
        <atom:link href="https://mathformachines.com/feeds/linear-equations.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Thu, 21 Nov 2019 00:00:00 UT</lastBuildDate>
        <item>
    <title>Least Squares with the Moore-Penrose Inverse</title>
    <link>https://mathformachines.com/posts/least-squares-with-the-mp-inverse/index.html</link>
    <description><![CDATA[<h1 id="introduction">Introduction</h1>
<p>The <strong><a href="https://en.wikipedia.org/wiki/Invertible_matrix">inverse</a></strong> of a matrix <span class="math inline">\(A\)</span> is another matrix <span class="math inline">\(A^{-1}\)</span> that has this property:</p>
<span class="math display">\[\begin{align*}
A A^{-1} &amp;= I \\
A^{-1} A &amp;= I
\end{align*}
\]</span>
<p>where <span class="math inline">\(I\)</span> is the <em>identity matrix</em>. This is a nice property for a matrix to have, because then we can work with it in equations just like we might with ordinary numbers. For instance, to solve some <a href="https://en.wikipedia.org/wiki/System_of_linear_equations">linear system of equations</a> <span class="math display">\[ A x = b \]</span> we can just multiply the inverse of <span class="math inline">\(A\)</span> to both sides <span class="math display">\[ x = A^{-1} b \]</span> and then we have some unique solution vector <span class="math inline">\(x\)</span>. Again, this is just like we would do if we were trying to solve a real-number equation like <span class="math inline">\(a x = b\)</span>.</p>
<p>Now, a matrix has an inverse whenever it is square and its rows are linearly independent. But not every system of equations we might care about will give us a matrix that satisfies these properties. The coefficient matrix <span class="math inline">\(A\)</span> would fail to be invertible if the system did not have the same number of equations as unknowns (<span class="math inline">\(A\)</span> is not square), or if the system had dependent equations (<span class="math inline">\(A\)</span> has dependent rows).</p>
<p><a href="https://en.wikipedia.org/wiki/Generalized_inverse">Generalized inverses</a> are meant to solve this problem. They are meant to solve equations like <span class="math inline">\(A x = b\)</span> in the “best way possible” when <span class="math inline">\(A^{-1}\)</span> fails to exist. There are many kinds of generalized inverses, each with its own “best way.” (They can be used to solve <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">ridge regression</a> problems, for instance.)</p>
<p>The most common is the <strong><a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse">Moore-Penrose inverse</a></strong>, or sometimes just the <strong>pseudoinverse</strong>. It solves the <a href="https://en.wikipedia.org/wiki/Ordinary_least_squares">least-squares</a> problem for linear systems, and therefore will give us a solution <span class="math inline">\(\hat{x}\)</span> so that <span class="math inline">\(A \hat{x}\)</span> is as close as possible in ordinary <a href="https://en.wikipedia.org/wiki/Euclidean_distance">Euclidean distance</a> to the vector <span class="math inline">\(b\)</span>.</p>
<p>The notation for the Moore-Penrose inverse is <span class="math inline">\(A^+\)</span> instead of <span class="math inline">\(A^{-1}\)</span>. If <span class="math inline">\(A\)</span> is invertible, then in fact <span class="math inline">\(A^+ = A^{-1}\)</span>, and in that case the solution to the least-squares problem is the same as the ordinary solution (<span class="math inline">\(A^+ b = A^{-1} b\)</span>). So, the MP-inverse is strictly more general than the ordinary inverse: we can always use it and it will always give us the same solution as the ordinary inverse whenever the ordinary inverse exists.</p>
<p>We will look at how we can construct the Moore-Penrose inverse using the SVD. This turns out to be an easy extension to constructing the ordinary matrix inverse with the SVD. We will then see how solving a least-squares problem is just as easy as solving an ordinary equation.</p>
<h1 id="example---system-with-an-invertible-matrix">Example - System with an Invertible Matrix</h1>
<p>First let’s recall how to solve a system whose coefficient matrix is invertible. In this case, we have the same number of equations as unknowns and the equations are all independent. Then <span class="math inline">\(A^{-1}\)</span> exists and we can find a unique solution for <span class="math inline">\(x\)</span> by multiplying <span class="math inline">\(A^{-1}\)</span> on both sides.</p>
<p>For instance, say we have</p>
<p><span class="math display">\[ \left\{\begin{align*}
x_1 - \frac{1}{2}x_2 &amp;= 1 \\
-\frac{1}{2} x_1 + x_2 &amp;= -1
\end{align*}\right. \]</span></p>
<p>Then</p>
<p><span class="math display">\[ \begin{array}{c c}
A = \begin{bmatrix}
1 &amp; -1/2 \\
-1/2 &amp; 1
\end{bmatrix},
&amp;A^{-1} = \begin{bmatrix}
4/3 &amp; 2/3 \\
2/3 &amp; 4/3
\end{bmatrix} \end{array} \]</span></p>
<p>and</p>
<p><span class="math display">\[x = A^{-1}b = \begin{bmatrix}
4/3 &amp; 2/3 \\
2/3 &amp; 4/3
\end{bmatrix} \begin{bmatrix}
1 \\ 
-1
\end{bmatrix} = \begin{bmatrix}
2/3 \\
-2/3
\end{bmatrix}
\]</span></p>
<p>So <span class="math inline">\(x_1 = \frac{2}{3}\)</span> and <span class="math inline">\(x_2 = -\frac{2}{3}\)</span>.</p>
<h1 id="constructing-inverses-with-the-svd">Constructing Inverses with the SVD</h1>
<p>The <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) gives us an intuitive way constructing an inverse matrix. We will be able to see how the geometric transforms of <span class="math inline">\(A^{-1}\)</span> undo the transforms of <span class="math inline">\(A\)</span>.</p>
<p>The SVD says that for any matrix <span class="math inline">\(A\)</span>,</p>
<p><span class="math display">\[ A = U \Sigma V^* \]</span></p>
<p>where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are orthogonal matricies and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix.</p>
<p>Now, if <span class="math inline">\(A\)</span> is invertible, we can use its SVD to find <span class="math inline">\(A^{-1}\)</span> like so:</p>
<p><span class="math display">\[ A^{-1} = V \Sigma^{-1} U^* \]</span></p>
<p>If we have the SVD of <span class="math inline">\(A\)</span>, we can construct its inverse by swapping the orthogonal matrices <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> and finding the inverse of <span class="math inline">\(\Sigma\)</span>. Since <span class="math inline">\(\Sigma\)</span> is diagonal, we can do this by just taking reciprocals of its diagonal entries.</p>
<h2 id="example">Example</h2>
<p>Let’s look at our earlier matrix again:</p>
<p><span class="math display">\[ A = \begin{bmatrix}
1 &amp; -1/2 \\
-1/2 &amp; 1
\end{bmatrix} \]</span></p>
<p>It has SVD</p>
<p><span class="math display">\[ A = U \Sigma V^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
3/2 &amp; 0 \\
0 &amp; 1/2
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>And so,</p>
<p><span class="math display">\[ A^{-1} = V \Sigma^{-1} U^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
2/3 &amp; 0 \\
0 &amp; 2
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>and after multiplying everything out, we get</p>
<p><span class="math display">\[ A^{-1} = \begin{bmatrix}
4/3 &amp; 2/3 \\
2/3 &amp; 4/3
\end{bmatrix} \]</span></p>
<p>just like we had before.</p>
<p>In an <a href="/posts/visualizing-linear-transformations/">earlier post</a>, we saw how we could use the SVD to visualize a matrix as a sequence of geometric transformations. Here is the matrix <span class="math inline">\(A\)</span> followed by <span class="math inline">\(A^{-1}\)</span>, acting on the unit circle:</p>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/invertible-equation.webm" type="video/webm">
  <source src="../../images/invertible-equation.mp4" type="video/mp4">
  <source src="../../images/invertible-equation.webm" type="video/ogg">
</video>

<p>The inverse matrix <span class="math inline">\(A^{-1}\)</span> reverses exactly the action of <span class="math inline">\(A\)</span>. The matrix <span class="math inline">\(A\)</span> will map any circle to a unique ellipse, with no overlap. So, <span class="math inline">\(A^{-1}\)</span> can map ellipses back to those same circles without any ambiguity. We don’t “lose information” by applying <span class="math inline">\(A\)</span>.</p>
<h1 id="constructing-mp-inverses-with-the-svd">Constructing MP-Inverses with the SVD</h1>
<p>We can in fact do basically the same thing for <em>any</em> matrix, not just the invertible ones. The SVD always exists, so for some matrix <span class="math inline">\(A\)</span>, first write</p>
<p><span class="math display">\[ A = U \Sigma V^* \]</span></p>
<p>And then find the MP-inverse by</p>
<p><span class="math display">\[ A^+ = V \Sigma^+ U^* \]</span></p>
<p>Now, the matrix <span class="math inline">\(A\)</span> might not be invertible. If it is not square, then, to find <span class="math inline">\(\Sigma^+\)</span>, we need to take the transpose of <span class="math inline">\(\Sigma\)</span> to make sure all the dimensions are conformable in the multiplication. It <span class="math inline">\(A\)</span> is singular (dependent rows), then <span class="math inline">\(\Sigma\)</span> will have 0’s on its diagaonal, so we need to make sure only take reciprocals of the non-zero entries.</p>
<p>Summarizing, to find the Moore-Penrose inverse of a matrix <span class="math inline">\(A\)</span>:</p>
<ol>
<li>Find the Singular Value Decomposition: <span class="math inline">\(A = U \Sigma V^*\)</span> (using <a href="https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/svd">R</a> or <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html">Python</a>, if you like).</li>
<li>Find <span class="math inline">\(\Sigma^+\)</span> by transposing <span class="math inline">\(\Sigma\)</span> and taking the reciprocal of all its non-zero diagonal entries.</li>
<li>Compute <span class="math inline">\(A^+ = V \Sigma^+ U^*\)</span></li>
</ol>
<h2 id="example---an-inconsistent-system">Example - An Inconsistent System</h2>
<p>Let’s find the MP-inverse of a singular matrix. Let’s take</p>
<p><span class="math display">\[A = \begin{bmatrix}
1 &amp; 1 \\
1 &amp; 1
\end{bmatrix}
\]</span></p>
<p>Because the rows of this matrix are linearly dependent, <span class="math inline">\(A^{-1}\)</span> does not exist. But we can still find the more general MP-inverse by following the procedure above.</p>
<p>So, first we find the SVD of <span class="math inline">\(A\)</span>:</p>
<p><span class="math display">\[ A = U \Sigma V^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
2 &amp; 0 \\
0 &amp; 0
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>Then we apply the procedure above to find <span class="math inline">\(A^+\)</span>:</p>
<p><span class="math display">\[ A^+ = V \Sigma^+ U^* = \begin{bmatrix}
\sqrt{2}/2 &amp; -\sqrt{2}/2 \\
\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \begin{bmatrix}
1/2 &amp; 0 \\
0 &amp; 0
\end{bmatrix} \begin{bmatrix}
\sqrt{2}/2 &amp; \sqrt{2}/2 \\
-\sqrt{2}/2 &amp; \sqrt{2}/2
\end{bmatrix} \]</span></p>
<p>And now we multiply everything out to get:</p>
<p><span class="math display">\[ A^+ = \begin{bmatrix}
1/4 &amp; 1/4 \\
1/4 &amp; 1/4 \end{bmatrix} \]</span></p>
<p>This is the Moore-Penrose inverse of <span class="math inline">\(A\)</span>.</p>
<p>Like we did for the invertible matrix before, let’s get an idea of what <span class="math inline">\(A\)</span> and <span class="math inline">\(A^+\)</span> are doing geometrically. Here they are acting on the unit circle:</p>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/dependent-equation.webm" type="video/webm">
  <source src="../../images/dependent-equation.mp4" type="video/mp4">
  <source src="../../images/dependent-equation.ogg" type="video/ogg">
</video>

<p>Notice how <span class="math inline">\(A\)</span> now collapses the circle onto a one-dimensional space. This is a consequence of it having dependent columns. For matricies with dependent columns, its image will be of lesser dimension than the space it’s mapping into. Another way of saying this is that it has a non-trivial <a href="https://en.wikipedia.org/wiki/Kernel_(linear_algebra)">null space</a>. It “zeroes out” some of the dimensions in its domain during the transformation.</p>
<p>What if <span class="math inline">\(A\)</span> were the coefficient matrix of a system of equations? We might have:</p>
<p><span class="math display">\[ \left\{ \begin{align*}
x_1 + x_2 &amp;= b_1 \\
x_1 + x_2 &amp;= b_2
\end{align*} \right. \]</span></p>
<p>for some <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>.</p>
<p>Now, unless <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span> are equal, this system won’t have an exact solution for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>. It will be <em>inconsistent</em>. But, with <span class="math inline">\(A^+\)</span>, we can still find values for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> that minimize the distance between <span class="math inline">\(A x\)</span> and <span class="math inline">\(b\)</span>. More specifically, let <span class="math inline">\(\hat{x} = A^{+}b\)</span>. Then <span class="math inline">\(\hat{x}\)</span> will minimize <span class="math inline">\(|| b - A x ||^2  \)</span>, the <em>squared error</em>, and <span class="math inline">\( \hat{b} = A \hat{x} = A A^{+} x \)</span> is the closest we can come to <span class="math inline">\(b\)</span>. (The vector <span class="math inline">\(b - A \hat{x}\)</span> is sometimes called the <strong><a href="https://en.wikipedia.org/wiki/Residual_(numerical_analysis)">residual</a></strong> vector.)</p>
<p>We have</p>
<p><span class="math display">\[ \hat{x} = A^{+} b = \begin{bmatrix}
1/4 (b_1 + b_2) \\
1/4 (b_1 + b_2) \end{bmatrix} \]</span></p>
<p>so <span class="math inline">\(x_1 = \frac{1}{4} (b_1 + b_2)\)</span> and <span class="math inline">\(x_2 = \frac{1}{4} (b_1 + b_2)\)</span>. And the closest we can get to <span class="math inline">\(b\)</span> is</p>
<p><span class="math display">\[ \hat{b} = A \hat{x} = \begin{bmatrix}
1/2 (b_1 + b_2) \\
1/2 (b_1 + b_2) \end{bmatrix} \]</span></p>
<p>In other words, if we have to make <span class="math inline">\(x_1 + x_2\)</span> as close as possible to two different values <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>, the best we can do is to choose <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> so as to get the average of <span class="math inline">\(b_1\)</span> and <span class="math inline">\(b_2\)</span>.</p>
<figure>
<img src="../../images/least-squares.png" title="least-squares" alt="The vector b = (1, 3) and its orthogonal projection \hat{b} = (2, 2)." width="400" /><figcaption>The vector <span class="math inline">\(b = (1, 3)\)</span> and its orthogonal projection <span class="math inline">\(\hat{b} = (2, 2)\)</span>.</figcaption>
</figure>
<p>Or we could think about this problem geometrically. In order for there to be a solution to <span class="math inline">\(A x = b\)</span>, the vector <span class="math inline">\(b\)</span> has to reside in the image of <span class="math inline">\(A\)</span>. The image of <span class="math inline">\(A\)</span> is the span of its columns, which is all vectors like <span class="math inline">\(c(1, 1)\)</span> for a scalar <span class="math inline">\(c\)</span>. This is just the line through the origin in the picture above. But <span class="math inline">\(b = (b_1, b_2)\)</span> is not on that line if <span class="math inline">\(b_1 \neq b_2\)</span>, and so instead we minimize the distance between the two with its orthogonal projection <span class="math inline">\(\hat b\)</span>. The error or residual is the difference <span class="math inline">\(\epsilon = b - \hat{b}\)</span>.</p>]]></description>
    <pubDate>Thu, 21 Nov 2019 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/least-squares-with-the-mp-inverse/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>

    </channel>
</rss>
