<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Math for Machines</title>
        <link>https://mathformachines.com</link>
        <description><![CDATA[A blog about data science and machine learning, with a lot of math.]]></description>
        <atom:link href="https://mathformachines.com/feeds/tpus.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Fri, 10 Jul 2020 00:00:00 UT</lastBuildDate>
        <item>
    <title>Getting Started with TPUs on Kaggle</title>
    <link>https://mathformachines.com/posts/getting-started-with-tpus/index.html</link>
    <description><![CDATA[<p><a href="https://www.kaggle.com/c/tpu-getting-started/overview/faq"><strong>Petals to the Metal</strong></a> is Kaggleâ€™s newest <a href="https://www.kaggle.com/c/tpu-getting-started/overview/faq">Getting Started</a> competition, highlighting <strong>Tensor Processing Units</strong>. A TPU is an accelerator developed by Google especially for machine learning. They can be used in TensorFlow much like GPUs, and are quite powerful. I had the pleasure of writing a tutorial to go along with the competition. Check it out!</p>
<p><a href="https://www.kaggle.com/ryanholbrook/create-your-first-submission">Getting Started with TPUs</a></p>]]></description>
    <pubDate>Fri, 10 Jul 2020 00:00:00 UT</pubDate>
    <guid>https://mathformachines.com/posts/getting-started-with-tpus/index.html</guid>
    <dc:creator>Ryan Holbrook</dc:creator>
</item>

    </channel>
</rss>
