---
title: "Naive Bayes Classifiers"
published: February 9, 2020
tags: R, classification, decision-boundaries, data-science, naive-bayes
usetoc: true
---

We saw in the post on [[file:./posts/decision/][optimal decision boundaries]] that the optimal boundary (under [[https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function][zero-one loss]]) is produced by a rule that assigns to an observation the /most probable/ class $c$ given the observed features $X$:

\[ \hat{C} = \operatorname*{argmax}_c P(C = c \mid X) \]

Recall that [[https://en.wikipedia.org/wiki/Bayes%27_theorem][Bayes' theorem]] tells us that this probability \(P(C = c \mid X)\) is proportionate to \(P(X \mid C = c) P(C = c)\). To estimate this optimal classification rule, therefore, a classifier will often attempt to estimate either the maximum of \(P(X \mid C = c) P(C = c)\) (a [[https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation][MAP]] estimate) or only \(P(X \mid C = c)\) (an [[https://en.wikipedia.org/wiki/Maximum_likelihood_estimation][MLE]] estimate). These estimates converge as the sample size grows.

One kind of classifier will make these estimations by modeling the class-conditional distributions for the features (that is, \(P(X \mid C = c)\)) as [[https://en.wikipedia.org/wiki/Normal_distribution][Normal (or Gaussian) distributions]] \(Normal(\mu_c, \Sigma_c)\) and then using [[https://en.wikipedia.org/wiki/Plug-in_principle][plug-in]] estimates for \(P(X)\) and the parameters \(\mu_c\) and \(\Sigma_c\). This technique is called *Gaussian discriminant analysis* (GDA).

* Naive Bayes Classifiers

The first kind of classifier of this type we will consider is the *naive Bayes* classifier. A naive Bayes classifier, in addition to assuming a distribution for \(P(X \mid C = c)\), also assumes that the features are [[https://en.wikipedia.org/wiki/Conditional_independence][conditionally independent]]. If $X$ is a vector of two features, $X = (X_1, Y_1)$, this means we can write
\[ P(X \mid C = c) = P(X_1 \mid C = c) P(X_2 \mid C = c) \]

This makes computing \(P(X \mid C = c)\) especially easy.

A naive Bayes classifier can model \(P(X_i \mid C = c)\) with a variety of distributions. When the features are binary, it might make sense to use a [[https://en.wikipedia.org/wiki/Bernoulli_distribution][Bernoulli distribution]], for instance. Our features will be real-valued, though, and we will model the features with normal distributions.

Considered as a method of GDA, this means we are modeling the class conditional distributions with [[https://en.wikipedia.org/wiki/Multivariate_normal_distribution][MVNs]] \(Normal(\mu_c | \Sigma_c)\) where each [[https://en.wikipedia.org/wiki/Covariance_matrix][covariance matrix]] is [[https://en.wikipedia.org/wiki/Diagonal_matrix][diagonal]]. (Recall that the off-diagonal entries of $\Sigma_c$ express the covariance between two features, while the diagonal entries of the matrix expresses the variance of individual features. Since we are assuming the features are independent, [[https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_and_independence][they must also be uncorrelated]]; hence, only diagonal entries can be non-zero.)

** Example 1 - Independent Features

Let's first fit a Naive bayes classifier to a data set where the data is actually generated exactly how the NB classifier assumes it will be. Our model will be

|----------------------+-------------------------------------------------------------------|
| Classes              | \(C \sim \operatorname{Bernoulli}(p)\)                            |
| Features for Class 0 | \((X, Y) \mid C = 0 \sim \operatorname{Normal}(\mu_0, \Sigma_0)\) |
| Features for Class 1 | \((X, Y) \mid C = 1 \sim \operatorname{Normal}(\mu_0, \Sigma_1)\) |
|----------------------+-------------------------------------------------------------------|

where 

|-------------------------------------------------------------|
| \(p = 0.5\)                                                 |
| \(\mu_0 = (0, 2)\)                                          |
| \(\Sigma_0 = \begin{bmatrix}1 & 0 \\ 0 & 1.5\end{bmatrix}\) |
| \(\mu_1 = (2, 0)\)                                          |
| \(\Sigma_1 = \begin{bmatrix}2 & 0 \\ 0 & 1\end{bmatrix}\)   |
|-------------------------------------------------------------|

First let's define the parameters and generate a sample of 4000 points and then also plot the optimal boundary. (All the necessary functions were defined in the previous post. You can find the code here: )

#+begin_src r
p <- 0.5
mu_0 <- c(0, 2)
sigma_0 <- matrix(c(1, 0, 0, 1.5), nrow = 2)
mu_1 <- c(2, 0)
sigma_1 <- matrix(c(2, 0, 0, 1), nrow = 2)

n <- 4000
set.seed(31415)
sample_mvn <- make_mvn_sample(n,
                              mu_0, sigma_0,
                              mu_1, sigma_1,
                              p)
density_mvn <- make_density_mvn(mu_0, sigma_0,
                                mu_1, sigma_1,
                                p,
                                -3, 5, -3, 5)


(ggplot() +
 gg_density(density_mvn, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mvn, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mvn)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.25) +
 gg_optimal(density_mvn)) +
plot_annotation("The Optimal Decision Boundary")
#+end_src

We see as before that the optimal boundary runs through points of intersection of the contours. The fact that our features are independent means that the contours can "spread out" only horizontally or vertically. I mean that the major-axis of the ellipse drawn has to be either horizontal or vertical. A diagonal spread would mean that the features were correlated and not independent.

Now let's look at how the classifier fits on this data.

#+begin_src r
fit_mvn_nb <- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mvn)
pred_mvn_nb <- predict(fit_mvn_nb, newdata = density_mvn[, c("x", "y")], type = "prob")
density_mvn_nb <- cbind(density_mvn, "fitted" = pred_mvn_nb[, "1"] - 0.5)

gg_plot_boundary(density_mvn_nb, sample_mvn, title = "Naive Bayes")

anim <- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_nb)
anim_save("/home/jovyan/work/bayeserror/nb_mvn_perfect.gif")
,#+end_src r

So, we can see that the model fits the optimal boundary quite well.


Here is a confusion matrix. Accurate classification almost 99% of the time.

,#+begin_src r
density_mvn_nb[, "assigned"] <- ifelse(density_mvn_nb$fitted > 0, 1, 0)

caret::confusionMatrix(factor(density_mvn_nb$class),
                       factor(density_mvn_nb$assigned))
#+end_src


** Example 2 - Dependent Features

What happens when the features are correlated within each class? Let's have our data model now be

|-----------------------------------------------------------------|
| \(p = 0.5\)                                                     |
| \(\mu_0 = (0, 2)\)                                              |
| \(\Sigma_0 = \begin{bmatrix}1 & 0.5 \\ 0.5 & 1.5\end{bmatrix}\) |
| \(\mu_1 = (2, 0)\)                                              |
| \(\Sigma_1 = \begin{bmatrix}2 & -0.5 \\ -0.5 & 1\end{bmatrix}\) |
|-----------------------------------------------------------------|

Note that the covariance matrices now have non-zero off-diagonal entries: the features are correlated.

#+begin_src r
p <- 0.5
mu_0 <- c(0, 2)
sigma_0 <- matrix(c(2, -0.5, -0.5, 1), nrow = 2)
mu_1 <- c(2, 0)
sigma_1 <- matrix(c(1, 0.5, 0.5, 1.5), nrow = 2)

n <- 4000
set.seed(31415)
sample_mvn <- make_mvn_sample(n,
                              mu_0, sigma_0,
                              mu_1, sigma_1,
                              p)
density_mvn <- make_density_mvn(mu_0, sigma_0,
                                mu_1, sigma_1,
                                p,
                                -3, 5, -3, 5)


(ggplot() +
 gg_density(density_mvn, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mvn, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mvn)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.25) +
 gg_optimal(density_mvn)) +
plot_annotation("The Optimal Decision Boundary")
#+end_src

The optimal boundary is not too different.

#+begin_src r
fit_mvn_nb <- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mvn)
pred_mvn_nb <- predict(fit_mvn_nb, newdata = density_mvn[, c("x", "y")], type = "prob")
density_mvn_nb <- cbind(density_mvn, "fitted" = pred_mvn_nb[, "1"] - 0.5)
gg_plot_boundary(density_mvn_nb, sample_mvn, title = "Naive Bayes")

anim <- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_nb)

anim_save("/home/jovyan/work/bayeserror/nb_mvn_dependent.gif")
#+end_src

But the naive Bayes classifier isn't able to get a as exact of a fit this time.

Here is a confusion matrix. Now only accurate about 86% of the time.

#+begin_src r
density_mvn_nb[, "assigned"] <- ifelse(density_mvn_nb$fitted > 0, 1, 0)

caret::confusionMatrix(factor(density_mvn_nb$class),
                       factor(density_mvn_nb$assigned))
#+end_src

** Example 3 - The Misspecified Model

How badly does the model degrade as the features depart from independence? To investigate, let's see how the fitted boundary changes as we vary the dependence structure in each class.

#+begin_src r

#+end_src


#+begin_src r
fit_mvn_nb <- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mvn)
pred_mvn_nb <- predict(fit_mvn_nb, newdata = density_mvn[, c("x", "y")], type = "prob")
density_mvn_nb <- cbind(density_mvn, "fitted" = pred_mvn_nb[, "1"] - 0.5)
gg_plot_boundary(density_mvn_nb, sample_mvn, title = "Naive Bayes")

fit_mix_nb <- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mix, kernel = TRUE)
pred_mix_nb <- predict(fit_mix_nb, newdata = density_mix[, c("x", "y")], type = "prob")
density_mix_nb <- cbind(density_mix, "fitted" = pred_mix_nb[, "1"] - 0.5)

gg_plot_boundary(density_mix_nb, sample_mix, title = "Naive Bayes")


fit_and_predict_nb <- function(sample, density) {
    fit_nb <- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample)
    pred_nb <- predict(fit_nb, newdata = density[, c("x", "y")], type = "prob")
    density_nb <- cbind(density, "fitted" = pred_nb[, "1"])
    density_nb
}

anim <- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_nb)
anim_save("/home/jovyan/work/bayeserror/nb_mvn.gif")

anim <- animate_boundary(sample_mixn, density_mix, 100, fit_and_predict_nb)
anim_save("/home/jovyan/work/bayeserror/nb_mix.gif")
#+end_src

* Linear Discriminant Analysis

#+begin_src r

fit_lda <- MASS::lda(class ~ x + y, data = density_mvn)
pred_lda <- predict(fit_lda, newdata = density_mvn)
density_lda <- cbind(density_mvn, "fitted" = pred_lda$posterior[, "1"] - 0.5)
#+end_src

* Quadratic Discriminant Analysis

#+begin_src r

fit_mvn_qda <- MASS::qda(class ~ x + y, data = sample_mvn)
pred_mvn_qda <- predict(fit_mvn_qda, newdata = density_mvn)
density_mvn_qda <- cbind(density_mvn, "fitted" = pred_mvn_qda$posterior[, "1"] - 0.5)
gg_plot_boundary(density_mvn_qda, sample_mvn, title = "QDA")

fit_mvn_qda <- MASS::qda(class ~ x + y, data = sample_mvn)
pred_mvn_qda <- predict(fit_mvn_qda, newdata = density_mvn)
density_mvn_qda <- cbind(density_mvn, "fitted" = pred_mvn_qda$posterior[, "1"] - 0.5)
gg_plot_boundary(density_mvn_qda, sample_mvn, title = "QDA")


fit_and_predict_qda <- function(sample, density) {
    fit_qda <- MASS::qda(class ~ x + y, data = sample)
    pred_qda <- predict(fit_qda, newdata = density)
    density_qda <- cbind(density, "fitted" = pred_qda$posterior[, "1"])
    density_qda
}

anim_mvn_qda <- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_qda)

anim_save("/home/jovyan/work/bayeserror/qda_mvn.gif")

anim_mix_qda <- animate_boundary(sample_mix, density_mix, 10, fit_and_predict_qda)

anim_save("/home/jovyan/work/bayeserror/qda_mix.gif")
#+end_src

* Conclusion

All of these classifiers involved trade-offs.
