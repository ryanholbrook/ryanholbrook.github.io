
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Naive Bayes Classifiers</title>
    <link href="https://fonts.googleapis.com/css?family=Merriweather:400&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro&display=swap" rel="stylesheet"> 
    <link href="https://unpkg.com/primer/build/build.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/style.css" />
      <!-- Syntax highlighting -->
  <link rel="stylesheet" href="../../css/github.css" />
  <script src="../../scripts/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class="bg-gray-dark">
    <div class="container-xxxl h-100 my-3">
      <div class="box-shadow-large bg-gray">

        <header id="header" class="bg-white">

  <div class="p-4" style="text-align: center">

    <a class="title" href="../../">
      Math for Machines
    </a>

  </div>
  
  <nav class="UnderlineNav UnderlineNav--right px-2 border-top">
    <div class="UnderlineNav-body">
      <a class="UnderlineNav-item " href="../../about/">
        <span>About</span>
      </a>
      <a class="UnderlineNav-item " href="../../archive/">
        <span>Archive</span>
      </a>
    </div>
  </nav>


  <!-- Scripts -->
  <!-- Enable MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic:true } },
    });
    MathJax.Hub.Config({
        TeX: { extensions: ["color.js"] }
    });
    MathJax.Hub.Config({
        TeX: {
            Macros: {
                formbox: ["\\bbox[15px, border:1px solid Gray]{#1}", 1],
            }
        }
    });
  </script>
  <script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133546767-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133546767-1', { 'optimize_id': 'GTM-T3XD3JM'});
  </script>

</header>

        
        <div id="holy">

          <div id="left" class="mr-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Recent Posts</h1>
                <ul>
    
        <li>
          <a href="../../posts/naive-bayes/">Naive Bayes Classifiers</a>
        </li>
    
        <li>
          <a href="../../posts/decision/">Optimal Decision Boundaries</a>
        </li>
    
        <li>
          <a href="../../posts/least-squares-with-the-mp-inverse/">Least Squares with the Moore-Penrose Inverse</a>
        </li>
    
        <li>
          <a href="../../posts/eigenvalues-and-singular-values/">Understanding Eigenvalues and Singular Values</a>
        </li>
    
        <li>
          <a href="../../posts/visualizing-linear-transformations/">Visualizing Linear Transformations</a>
        </li>
    
        <li>
          <a href="../../posts/bayes-and-means/">What I'm Reading 1: Bayes and Means</a>
        </li>
    
        <li>
          <a href="../../posts/investmentsim/">investmentsim - an R Package for Simulating Investment Portfolios</a>
        </li>
    
        <li>
          <a href="../../posts/introduction-to-categories/">Talk: An Introduction to Categories with Haskell and Databases</a>
        </li>
    
        <li>
          <a href="../../posts/retirement-formula/">A Somewhat Better Retirement Formula</a>
        </li>
    
        <li>
          <a href="../../posts/permitted-and-forbidden-sets/">Journal Review: Permitted and Forbidden Sets in STLNs</a>
        </li>
    
</ul>

              </div>
            </div>
          </div>
            
          <div class="my-2">
          <div role="main" id="main">
  <div class="rounded-2 box-shadow-medium pb-3 px-4 bg-white">
    <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../../posts/naive-bayes/">Naive Bayes Classifiers</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../../tags/R/">R</a>, <a href="../../tags/classification/">classification</a>, <a href="../../tags/decision-boundaries/">decision-boundaries</a>, <a href="../../tags/data-science/">data-science</a>, <a href="../../tags/naive-bayes/">naive-bayes</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: February  9, 2020
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#naive-bayes-classifiers">Naive Bayes Classifiers</a><ul class="incremental">
<li><a href="#example-1---independent-features">Example 1 - Independent Features</a></li>
<li><a href="#example-2---dependent-features">Example 2 - Dependent Features</a></li>
<li><a href="#example-3---the-misspecified-model">Example 3 - The Misspecified Model</a></li>
</ul></li>
<li><a href="#linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
<li><a href="#quadratic-discriminant-analysis">Quadratic Discriminant Analysis</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <p>We saw in the post on <a href="./posts/decision/">optimal decision boundaries</a> that the optimal boundary (under <a href="https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function">zero-one loss</a>) is produced by a rule that assigns to an observation the <em>most probable</em> class <span class="math inline">\(c\)</span> given the observed features <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[ \hat{C} = \operatorname*{argmax}_c P(C = c \mid X) \]</span></p>
<p>Recall that <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a> tells us that this probability <span class="math inline">\(P(C = c \mid X)\)</span> is proportionate to <span class="math inline">\(P(X \mid C = c) P(C = c)\)</span>. To estimate this optimal classification rule, therefore, a classifier will often attempt to estimate either the maximum of <span class="math inline">\(P(X \mid C = c) P(C = c)\)</span> (a <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">MAP</a> estimate) or only <span class="math inline">\(P(X \mid C = c)\)</span> (an <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">MLE</a> estimate). These estimates converge as the sample size grows.</p>
<p>One kind of classifier will make these estimations by modeling the class-conditional distributions for the features (that is, <span class="math inline">\(P(X \mid C = c)\)</span>) as <a href="https://en.wikipedia.org/wiki/Normal_distribution">Normal (or Gaussian) distributions</a> <span class="math inline">\(Normal(\mu_c, \Sigma_c)\)</span> and then using <a href="https://en.wikipedia.org/wiki/Plug-in_principle">plug-in</a> estimates for <span class="math inline">\(P(X)\)</span> and the parameters <span class="math inline">\(\mu_c\)</span> and <span class="math inline">\(\Sigma_c\)</span>. This technique is called <strong>Gaussian discriminant analysis</strong> (GDA).</p>
<h1 id="naive-bayes-classifiers">Naive Bayes Classifiers</h1>
<p>The first kind of classifier of this type we will consider is the <strong>naive Bayes</strong> classifier. A naive Bayes classifier, in addition to assuming a distribution for <span class="math inline">\(P(X \mid C = c)\)</span>, also assumes that the features are <a href="https://en.wikipedia.org/wiki/Conditional_independence">conditionally independent</a>. If <span class="math inline">\(X\)</span> is a vector of two features, <span class="math inline">\(X = (X_1, Y_1)\)</span>, this means we can write <span class="math display">\[ P(X \mid C = c) = P(X_1 \mid C = c) P(X_2 \mid C = c) \]</span></p>
<p>This makes computing <span class="math inline">\(P(X \mid C = c)\)</span> especially easy.</p>
<p>A naive Bayes classifier can model <span class="math inline">\(P(X_i \mid C = c)\)</span> with a variety of distributions. When the features are binary, it might make sense to use a <a href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>, for instance. Our features will be real-valued, though, and we will model the features with normal distributions.</p>
<p>Considered as a method of GDA, this means we are modeling the class conditional distributions with <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">MVNs</a> <span class="math inline">\(Normal(\mu_c | \Sigma_c)\)</span> where each <a href="https://en.wikipedia.org/wiki/Covariance_matrix">covariance matrix</a> is <a href="https://en.wikipedia.org/wiki/Diagonal_matrix">diagonal</a>. (Recall that the off-diagonal entries of <span class="math inline">\(\Sigma_c\)</span> express the covariance between two features, while the diagonal entries of the matrix expresses the variance of individual features. Since we are assuming the features are independent, <a href="https://en.wikipedia.org/wiki/Correlation_and_dependence#Correlation_and_independence">they must also be uncorrelated</a>; hence, only diagonal entries can be non-zero.)</p>
<h2 id="example-1---independent-features">Example 1 - Independent Features</h2>
<p>Let’s first fit a Naive bayes classifier to a data set where the data is actually generated exactly how the NB classifier assumes it will be. Our model will be</p>
<table>
<tbody>
<tr class="odd">
<td>Classes</td>
<td><span class="math inline">\(C \sim \operatorname{Bernoulli}(p)\)</span></td>
</tr>
<tr class="even">
<td>Features for Class 0</td>
<td><span class="math inline">\((X, Y) \mid C = 0 \sim \operatorname{Normal}(\mu_0, \Sigma_0)\)</span></td>
</tr>
<tr class="odd">
<td>Features for Class 1</td>
<td><span class="math inline">\((X, Y) \mid C = 1 \sim \operatorname{Normal}(\mu_0, \Sigma_1)\)</span></td>
</tr>
</tbody>
</table>
<p>where</p>
<table>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p = 0.5\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu_0 = (0, 2)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\Sigma_0 = \begin{bmatrix}1 &amp; 0 \\ 0 &amp; 1.5\end{bmatrix}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu_1 = (2, 0)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\Sigma_1 = \begin{bmatrix}2 &amp; 0 \\ 0 &amp; 1\end{bmatrix}\)</span></td>
</tr>
</tbody>
</table>
<p>First let’s define the parameters and generate a sample of 4000 points and then also plot the optimal boundary. (All the necessary functions were defined in the previous post. You can find the code here: )</p>
<pre class="r"><code>p &lt;- 0.5
mu_0 &lt;- c(0, 2)
sigma_0 &lt;- matrix(c(1, 0, 0, 1.5), nrow = 2)
mu_1 &lt;- c(2, 0)
sigma_1 &lt;- matrix(c(2, 0, 0, 1), nrow = 2)

n &lt;- 4000
set.seed(31415)
sample_mvn &lt;- make_mvn_sample(n,
                              mu_0, sigma_0,
                              mu_1, sigma_1,
                              p)
density_mvn &lt;- make_density_mvn(mu_0, sigma_0,
                                mu_1, sigma_1,
                                p,
                                -3, 5, -3, 5)


(ggplot() +
 gg_density(density_mvn, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mvn, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mvn)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.25) +
 gg_optimal(density_mvn)) +
plot_annotation(&quot;The Optimal Decision Boundary&quot;)
</code></pre>
<p>We see as before that the optimal boundary runs through points of intersection of the contours. The fact that our features are independent means that the contours can “spread out” only horizontally or vertically. I mean that the major-axis of the ellipse drawn has to be either horizontal or vertical. A diagonal spread would mean that the features were correlated and not independent.</p>
<p>Now let’s look at how the classifier fits on this data.</p>
<pre class="r"><code>fit_mvn_nb &lt;- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mvn)
pred_mvn_nb &lt;- predict(fit_mvn_nb, newdata = density_mvn[, c(&quot;x&quot;, &quot;y&quot;)], type = &quot;prob&quot;)
density_mvn_nb &lt;- cbind(density_mvn, &quot;fitted&quot; = pred_mvn_nb[, &quot;1&quot;] - 0.5)

gg_plot_boundary(density_mvn_nb, sample_mvn, title = &quot;Naive Bayes&quot;)

anim &lt;- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_nb)
anim_save(&quot;/home/jovyan/work/bayeserror/nb_mvn_perfect.gif&quot;)
#+end_src r

So, we can see that the model fits the optimal boundary quite well.


Here is a confusion matrix. Accurate classification almost 99% of the time.

#+begin_src r
density_mvn_nb[, &quot;assigned&quot;] &lt;- ifelse(density_mvn_nb$fitted &gt; 0, 1, 0)

caret::confusionMatrix(factor(density_mvn_nb$class),
                       factor(density_mvn_nb$assigned))
</code></pre>
<h2 id="example-2---dependent-features">Example 2 - Dependent Features</h2>
<p>What happens when the features are correlated within each class? Let’s have our data model now be</p>
<table>
<tbody>
<tr class="odd">
<td><span class="math inline">\(p = 0.5\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu_0 = (0, 2)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\Sigma_0 = \begin{bmatrix}1 &amp; 0.5 \\ 0.5 &amp; 1.5\end{bmatrix}\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mu_1 = (2, 0)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\Sigma_1 = \begin{bmatrix}2 &amp; -0.5 \\ -0.5 &amp; 1\end{bmatrix}\)</span></td>
</tr>
</tbody>
</table>
<p>Note that the covariance matrices now have non-zero off-diagonal entries: the features are correlated.</p>
<pre class="r"><code>p &lt;- 0.5
mu_0 &lt;- c(0, 2)
sigma_0 &lt;- matrix(c(2, -0.5, -0.5, 1), nrow = 2)
mu_1 &lt;- c(2, 0)
sigma_1 &lt;- matrix(c(1, 0.5, 0.5, 1.5), nrow = 2)

n &lt;- 4000
set.seed(31415)
sample_mvn &lt;- make_mvn_sample(n,
                              mu_0, sigma_0,
                              mu_1, sigma_1,
                              p)
density_mvn &lt;- make_density_mvn(mu_0, sigma_0,
                                mu_1, sigma_1,
                                p,
                                -3, 5, -3, 5)


(ggplot() +
 gg_density(density_mvn, z = p_0_xy,
            alpha = 0.25) +
 gg_density(density_mvn, z = p_1_xy,
            alpha = 0.25) +
 gg_optimal(density_mvn)) +
(ggplot() +
 gg_sample(sample_mvn, alpha = 0.25) +
 gg_optimal(density_mvn)) +
plot_annotation(&quot;The Optimal Decision Boundary&quot;)
</code></pre>
<p>The optimal boundary is not too different.</p>
<pre class="r"><code>fit_mvn_nb &lt;- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mvn)
pred_mvn_nb &lt;- predict(fit_mvn_nb, newdata = density_mvn[, c(&quot;x&quot;, &quot;y&quot;)], type = &quot;prob&quot;)
density_mvn_nb &lt;- cbind(density_mvn, &quot;fitted&quot; = pred_mvn_nb[, &quot;1&quot;] - 0.5)
gg_plot_boundary(density_mvn_nb, sample_mvn, title = &quot;Naive Bayes&quot;)

anim &lt;- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_nb)

anim_save(&quot;/home/jovyan/work/bayeserror/nb_mvn_dependent.gif&quot;)
</code></pre>
<p>But the naive Bayes classifier isn’t able to get a as exact of a fit this time.</p>
<p>Here is a confusion matrix. Now only accurate about 86% of the time.</p>
<pre class="r"><code>density_mvn_nb[, &quot;assigned&quot;] &lt;- ifelse(density_mvn_nb$fitted &gt; 0, 1, 0)

caret::confusionMatrix(factor(density_mvn_nb$class),
                       factor(density_mvn_nb$assigned))
</code></pre>
<h2 id="example-3---the-misspecified-model">Example 3 - The Misspecified Model</h2>
<p>How badly does the model degrade as the features depart from independence? To investigate, let’s see how the fitted boundary changes as we vary the dependence structure in each class.</p>
<pre class="r"><code>
</code></pre>
<pre class="r"><code>fit_mvn_nb &lt;- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mvn)
pred_mvn_nb &lt;- predict(fit_mvn_nb, newdata = density_mvn[, c(&quot;x&quot;, &quot;y&quot;)], type = &quot;prob&quot;)
density_mvn_nb &lt;- cbind(density_mvn, &quot;fitted&quot; = pred_mvn_nb[, &quot;1&quot;] - 0.5)
gg_plot_boundary(density_mvn_nb, sample_mvn, title = &quot;Naive Bayes&quot;)

fit_mix_nb &lt;- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample_mix, kernel = TRUE)
pred_mix_nb &lt;- predict(fit_mix_nb, newdata = density_mix[, c(&quot;x&quot;, &quot;y&quot;)], type = &quot;prob&quot;)
density_mix_nb &lt;- cbind(density_mix, &quot;fitted&quot; = pred_mix_nb[, &quot;1&quot;] - 0.5)

gg_plot_boundary(density_mix_nb, sample_mix, title = &quot;Naive Bayes&quot;)


fit_and_predict_nb &lt;- function(sample, density) {
    fit_nb &lt;- naivebayes::naive_bayes(factor(class) ~ x + y, data = sample)
    pred_nb &lt;- predict(fit_nb, newdata = density[, c(&quot;x&quot;, &quot;y&quot;)], type = &quot;prob&quot;)
    density_nb &lt;- cbind(density, &quot;fitted&quot; = pred_nb[, &quot;1&quot;])
    density_nb
}

anim &lt;- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_nb)
anim_save(&quot;/home/jovyan/work/bayeserror/nb_mvn.gif&quot;)

anim &lt;- animate_boundary(sample_mixn, density_mix, 100, fit_and_predict_nb)
anim_save(&quot;/home/jovyan/work/bayeserror/nb_mix.gif&quot;)
</code></pre>
<h1 id="linear-discriminant-analysis">Linear Discriminant Analysis</h1>
<pre class="r"><code>
fit_lda &lt;- MASS::lda(class ~ x + y, data = density_mvn)
pred_lda &lt;- predict(fit_lda, newdata = density_mvn)
density_lda &lt;- cbind(density_mvn, &quot;fitted&quot; = pred_lda$posterior[, &quot;1&quot;] - 0.5)
</code></pre>
<h1 id="quadratic-discriminant-analysis">Quadratic Discriminant Analysis</h1>
<pre class="r"><code>
fit_mvn_qda &lt;- MASS::qda(class ~ x + y, data = sample_mvn)
pred_mvn_qda &lt;- predict(fit_mvn_qda, newdata = density_mvn)
density_mvn_qda &lt;- cbind(density_mvn, &quot;fitted&quot; = pred_mvn_qda$posterior[, &quot;1&quot;] - 0.5)
gg_plot_boundary(density_mvn_qda, sample_mvn, title = &quot;QDA&quot;)

fit_mvn_qda &lt;- MASS::qda(class ~ x + y, data = sample_mvn)
pred_mvn_qda &lt;- predict(fit_mvn_qda, newdata = density_mvn)
density_mvn_qda &lt;- cbind(density_mvn, &quot;fitted&quot; = pred_mvn_qda$posterior[, &quot;1&quot;] - 0.5)
gg_plot_boundary(density_mvn_qda, sample_mvn, title = &quot;QDA&quot;)


fit_and_predict_qda &lt;- function(sample, density) {
    fit_qda &lt;- MASS::qda(class ~ x + y, data = sample)
    pred_qda &lt;- predict(fit_qda, newdata = density)
    density_qda &lt;- cbind(density, &quot;fitted&quot; = pred_qda$posterior[, &quot;1&quot;])
    density_qda
}

anim_mvn_qda &lt;- animate_boundary(sample_mvn, density_mvn, 10, fit_and_predict_qda)

anim_save(&quot;/home/jovyan/work/bayeserror/qda_mvn.gif&quot;)

anim_mix_qda &lt;- animate_boundary(sample_mix, density_mix, 10, fit_and_predict_qda)

anim_save(&quot;/home/jovyan/work/bayeserror/qda_mix.gif&quot;)
</code></pre>
<h1 id="conclusion">Conclusion</h1>
<p>All of these classifiers involved trade-offs.</p>
  </section>
  
</article>

  </div>
</div>

<!-- Pagination -->
<nav class="paginate-container" aria-label="Pagination">
  <div class="pagination">
    
    <a class="previous_page text-gray-dark" rel="previous" aria-label="Previous Page" href="../../posts/decision/">⮜ Previous</a>
    

    
    <span class="next_page disabled">Next ⮞</span>
    
  </div>
</nav>

<!-- Talkyard Comments -->
<div class="container-m">
  <div class="rounded-2 box-shadow-medium px-2 py-1 bg-white">
    <script>talkyardServerUrl='https://comments-for-mathformachines-com.talkyard.net';</script>
    <script async defer src="https://c1.ty-cdn.net/-/talkyard-comments.min.js"></script>
    <!-- You can specify a per page discussion id on the next line, if your URLs might change. -->
    <div class="talkyard-comments" data-discussion-id style="margin-top: 45px;">
      <noscript>Please enable Javascript to view comments.</noscript>
      <p style="margin-top: 25px; opacity: 0.9; font-size: 96%">Comments powered by
        <a href="https://www.talkyard.io">Talkyard</a>.</p>
    </div>
  </div>
</div>

          </div>
          
          <div id="right" class="ml-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Tags</h1>
                <a style="font-size: 105%" href="../../tags/bayesian/">bayesian</a> <a style="font-size: 100%" href="../../tags/BMA/">BMA</a> <a style="font-size: 100%" href="../../tags/calculator/">calculator</a> <a style="font-size: 100%" href="../../tags/category-theory/">category-theory</a> <a style="font-size: 105%" href="../../tags/classification/">classification</a> <a style="font-size: 100%" href="../../tags/coordinates/">coordinates</a> <a style="font-size: 100%" href="../../tags/covectors/">covectors</a> <a style="font-size: 100%" href="../../tags/cql/">cql</a> <a style="font-size: 115%" href="../../tags/data-science/">data-science</a> <a style="font-size: 105%" href="../../tags/decision-boundaries/">decision-boundaries</a> <a style="font-size: 100%" href="../../tags/eigenvalues/">eigenvalues</a> <a style="font-size: 100%" href="../../tags/engrams/">engrams</a> <a style="font-size: 105%" href="../../tags/finance/">finance</a> <a style="font-size: 100%" href="../../tags/functional-programming/">functional-programming</a> <a style="font-size: 100%" href="../../tags/generalized-inverse/">generalized-inverse</a> <a style="font-size: 100%" href="../../tags/geometry/">geometry</a> <a style="font-size: 100%" href="../../tags/haskell/">haskell</a> <a style="font-size: 100%" href="../../tags/investing/">investing</a> <a style="font-size: 100%" href="../../tags/julia/">julia</a> <a style="font-size: 100%" href="../../tags/least-squares/">least-squares</a> <a style="font-size: 115%" href="../../tags/linear-algebra/">linear-algebra</a> <a style="font-size: 100%" href="../../tags/linear-equations/">linear-equations</a> <a style="font-size: 100%" href="../../tags/matrix-decomposition/">matrix-decomposition</a> <a style="font-size: 100%" href="../../tags/MCMC/">MCMC</a> <a style="font-size: 100%" href="../../tags/memory/">memory</a> <a style="font-size: 100%" href="../../tags/moore-penrose-inverse/">moore-penrose-inverse</a> <a style="font-size: 100%" href="../../tags/naive-bayes/">naive-bayes</a> <a style="font-size: 100%" href="../../tags/neural-networks/">neural-networks</a> <a style="font-size: 100%" href="../../tags/neuroscience/">neuroscience</a> <a style="font-size: 100%" href="../../tags/NLP/">NLP</a> <a style="font-size: 100%" href="../../tags/numpy/">numpy</a> <a style="font-size: 100%" href="../../tags/python/">python</a> <a style="font-size: 115%" href="../../tags/R/">R</a> <a style="font-size: 100%" href="../../tags/ReLUs/">ReLUs</a> <a style="font-size: 100%" href="../../tags/retirement/">retirement</a> <a style="font-size: 100%" href="../../tags/review/">review</a> <a style="font-size: 100%" href="../../tags/sage/">sage</a> <a style="font-size: 100%" href="../../tags/simulation/">simulation</a> <a style="font-size: 100%" href="../../tags/singular-values/">singular-values</a> <a style="font-size: 100%" href="../../tags/stacking/">stacking</a> <a style="font-size: 100%" href="../../tags/talk/">talk</a> <a style="font-size: 100%" href="../../tags/tensors/">tensors</a> <a style="font-size: 110%" href="../../tags/tutorial/">tutorial</a> <a style="font-size: 100%" href="../../tags/vectors/">vectors</a>
              </div>
              <div>
                <h1>Links</h1>
                <a href="https://www.r-bloggers.com/">R-bloggers</a>
              </div>
            </div>
          </div>
        </div>

        <footer class="border p-3 bg-white text-gray">
  Site proudly generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
</footer>

        
      </div>
  </body>
</html>
