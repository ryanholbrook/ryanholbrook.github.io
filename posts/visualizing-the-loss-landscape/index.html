
<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Visualizing the Loss Landscape of a Neural Network</title>
    <link href="https://fonts.googleapis.com/css?family=Merriweather:400&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro&display=swap" rel="stylesheet"> 
    <link href="https://unpkg.com/primer/build/build.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/style.css" />
      <!-- Syntax highlighting -->
  <link rel="stylesheet" href="../../css/github.css" />
  <script src="../../scripts/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  </head>
  <body class="bg-gray-dark">
    <div class="container-xxxl h-100 my-3">
      <div class="box-shadow-large bg-gray">

        <header id="header" class="bg-white">

  <div class="p-4" style="text-align: center">

    <a class="title" href="../../">
      Math for Machines
    </a>

  </div>
  
  <nav class="UnderlineNav UnderlineNav--left px-2 border-top">
    <div class="UnderlineNav-body">
      <a class="UnderlineNav-item" href="../../about/">
        <span style="font-size:1.25em;font-weight:bold">About</span>
      </a>
      <a class="UnderlineNav-item" href="../../courses/">
        <span style="font-size:1.25em;font-weight:bold">Kaggle Courses</span>
      </a>
      <a class="UnderlineNav-item" href="../../competitions/">
        <span style="font-size:1.25em;font-weight:bold">Kaggle Competitions</span>
      </a>
    </div>
    <div class="UnderlineNav-actions">
      <a class="UnderlineNav-item" href="../../archive/">
        <span style="font-size:1.25em;font-weight:bold">Archive</span>
      </a>
    </div>
  </nav>


  <!-- Scripts -->
  <!-- Enable MathJax -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS": { scale: 90, linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic:true } },
    });
    MathJax.Hub.Config({
        TeX: { extensions: ["color.js"] }
    });
    MathJax.Hub.Config({
        TeX: {
            Macros: {
                formbox: ["\\bbox[15px, border:1px solid Gray]{#1}", 1],
            }
        }
    });
  </script>
  <script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-133546767-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-133546767-1', { 'optimize_id': 'GTM-T3XD3JM'});
  </script>

</header>

        
        <div id="holy">

          <div id="left" class="mr-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Recent Posts</h1>
                <ul>
    
        <li>
          <a href="../../posts/breaking-into-tech/">How to Get a Job in Tech</a>
        </li>
    
        <li>
          <a href="../../posts/what-convnets-learn/">Visualizing What Convnets Learn</a>
        </li>
    
        <li>
          <a href="../../posts/visualizing-the-loss-landscape/">Visualizing the Loss Landscape of a Neural Network</a>
        </li>
    
        <li>
          <a href="../../posts/getting-started-with-tpus/">Getting Started with TPUs on Kaggle</a>
        </li>
    
        <li>
          <a href="../../posts/discriminant-analysis/">Six Varieties of Gaussian Discriminant Analysis</a>
        </li>
    
        <li>
          <a href="../../posts/decision/">Optimal Decision Boundaries</a>
        </li>
    
        <li>
          <a href="../../posts/least-squares-with-the-mp-inverse/">Least Squares with the Moore-Penrose Inverse</a>
        </li>
    
        <li>
          <a href="../../posts/eigenvalues-and-singular-values/">Understanding Eigenvalues and Singular Values</a>
        </li>
    
        <li>
          <a href="../../posts/visualizing-linear-transformations/">Visualizing Linear Transformations</a>
        </li>
    
        <li>
          <a href="../../posts/bayes-and-means/">What I'm Reading 1: Bayes and Means</a>
        </li>
    
</ul>

              </div>
            </div>
          </div>
            
          <div class="my-2">
          <div role="main" id="main">
  <div class="rounded-2 box-shadow-medium pb-3 px-4 bg-white">
    <!-- Post Header  -->
<div class="Subhead">
  <div class="Subhead-heading">
      <h2 class="mt-3 mb-1"><a id="post-title" href="../../posts/visualizing-the-loss-landscape/">Visualizing the Loss Landscape of a Neural Network</a></h2>
  </div>
  <div class="Subhead-description">
    
      <a href="../../tags/deep-learning/">deep-learning</a>, <a href="../../tags/sgd/">sgd</a>, <a href="../../tags/visualization/">visualization</a>, <a href="../../tags/python/">python</a>
    
    <div class="float-md-right" style="text-align=right">
      Published: December 30, 2020
      
    </div>
  </div>
</div>

<article>
  
  <div id="toc" class="Box mb-3">
    <h1>Table of Contents</h1>
    <ul class="incremental">
<li><a href="#the-linear-case">The Linear Case</a></li>
<li><a href="#making-random-slices">Making Random Slices</a></li>
<li><a href="#improving-the-view">Improving the View</a></li>
<li><a href="#plotting-the-optimization-path">Plotting the Optimization Path</a></li>
</ul>
  </div>
  
  
  <section id="content" class="pb-2 mb-4 border-bottom">
    <p>Training a neural network is an optimization problem, the problem of minimizing its <em>loss</em>. The <strong>loss function</strong> <span class="math inline">\(Loss_X(w)\)</span> of a neural network is the error of its predictions over a fixed dataset <span class="math inline">\(X\)</span> as a function of the network’s weights or other parameters <span class="math inline">\(w\)</span>. The <strong>loss landscape</strong> is the graph of this function, a surface in some usually high-dimensional space. We can imagine the training of the network as a journey across this surface: Weight initialization drops us onto some random coordinates in the landscape, and then SGD guides us step-by-step along a path of parameter values towards a minimum. The success of our training depends on the shape of the landscape and also on our manner of stepping across it.</p>
<p>As a neural network typically has many parameters (hundreds or millions or more), this loss surface will live in a space too large to visualize. There are, however, some tricks we can use to get a good two-dimensional of it and so gain a valuable source of intuition. I learned about these from <em>Visualizing the Loss Landscape of Neural Nets</em> by Li, et al. (<a href="https://arxiv.org/abs/1712.09913">arXiv</a>).</p>
<figure>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/loss-landscape-path.webm" type="video/webm">
  <source src="../../images/loss-landscape-path.mp4" type="video/mp4">
  Can't play the video for some reason! Click <a href="../../images/loss-landscape-path.gif">here</a> to download a gif.
</video>
</figure>

<h1 id="the-linear-case">The Linear Case</h1>
<p>Let’s start with the two-dimensional case to get an idea of what we’re looking for.</p>
<p>A single neuron with one input computes <span class="math inline">\(y = w x + b\)</span> and so has only two parameters: a weight <span class="math inline">\(w\)</span> for the input <span class="math inline">\(x\)</span> and a bias <span class="math inline">\(b\)</span>. Having only two parameters means we can view every dimension of the loss surface with a simple contour plot, the bias along one axis and the single weight along the other:</p>
<figure>
<video autoplay loop mutued playsinline controls>
  <source src="../../images/loss-landscape-linear.webm" type="video/webm">
  <source src="../../images/loss-landscape-linear.mp4" type="video/mp4">
  Can't play the video for some reason! Click <a href="../../images/loss-landscape-linear.gif">here</a> to download a gif.
</video>
<figcaption>Traversing the loss landscape of a linear model with SGD.</figcaption>
</figure>

<p>After training this simple linear model, we’ll have a pair of weights <span class="math inline">\(w_c\)</span> and <span class="math inline">\(b_c\)</span> that should be approximately where the minimal loss occurs – it’s nice to take this point <span class="math inline">\((w_c, b_c)\)</span> as the center of the plot. By collecting the weights at every step of training, we can trace out the path taken by SGD across the loss surface towards the minimum.</p>
<p>Our goal now is to get similar kinds of images for networks with any number of parameters.</p>
<h1 id="making-random-slices">Making Random Slices</h1>
<p>How can we view the loss landscape of a larger network? Though we can’t anything like a complete view of the loss surface, we can still get <em>a</em> view as long as we don’t especially care <em>what</em> view we get; that is, we’ll just take a random 2D slice out of the loss surface and look at the contours that slice, hoping that it’s more or less representative.</p>
<p>This slice is basically a coordinate system: we need a center (the origin) and a pair of direction vectors (axes). As before, let’s take the weights <span class="math inline">\(W_c\)</span> from the trainined network to act as the center, and the direction vectors we’ll generate randomly.</p>
<p>Now the loss at some point <span class="math inline">\((a, b)\)</span> on the graph is taken by setting the weights of the network to <span class="math inline">\(W_c + a W_0 + b W_1\)</span> and evaluating it on the given data. Plot these losses across some range of values for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, and we can produce our contour plot.</p>
<figure>
<img src="../../images/random-loss-surface.png" title="random-loss-surface" alt="Random slices through a high-dimensional loss surface" /><figcaption>Random slices through a high-dimensional loss surface</figcaption>
</figure>
<p>We might worry that the plot would be distorted if the random vectors we chose happened to be close together, even though we’ve plotted them as if they were at a right angle. It’s a nice fact about high-dimensional vector spaces, though, that any two random vectors you choose from them will usually be close to orthogonal.</p>
<p>Here’s how we could implement this:</p>
<pre class="python"><code>import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import callbacks, layers

class RandomCoordinates(object):
    def __init__(self, origin):
        self.origin_ = origin
        self.v0_ = normalize_weights(
            [np.random.normal(size=w.shape) for w in origin], origin
        )
        self.v1_ = normalize_weights(
            [np.random.normal(size=w.shape) for w in origin], origin
        )

    def __call__(self, a, b):
        return [
            a * w0 + b * w1 + wc
            for w0, w1, wc in zip(self.v0_, self.v1_, self.origin_)
        ]


def normalize_weights(weights, origin):
    return [
        w * np.linalg.norm(wc) / np.linalg.norm(w)
        for w, wc in zip(weights, origin)
    ]


class LossSurface(object):
    def __init__(self, model, inputs, outputs):
        self.model_ = model
        self.inputs_ = inputs
        self.outputs_ = outputs

    def compile(self, range, points, coords):
        a_grid = tf.linspace(-1.0, 1.0, num=points) ** 3 * range
        b_grid = tf.linspace(-1.0, 1.0, num=points) ** 3 * range
        loss_grid = np.empty([len(a_grid), len(b_grid)])
        for i, a in enumerate(a_grid):
            for j, b in enumerate(b_grid):
                self.model_.set_weights(coords(a, b))
                loss = self.model_.test_on_batch(
                    self.inputs_, self.outputs_, return_dict=True
                )[&quot;loss&quot;]
                loss_grid[j, i] = loss
        self.model_.set_weights(coords.origin_)
        self.a_grid_ = a_grid
        self.b_grid_ = b_grid
        self.loss_grid_ = loss_grid

    def plot(self, range=1.0, points=24, levels=20, ax=None, **kwargs):
        xs = self.a_grid_
        ys = self.b_grid_
        zs = self.loss_grid_
        if ax is None:
            _, ax = plt.subplots(**kwargs)
            ax.set_title(&quot;The Loss Surface&quot;)
            ax.set_aspect(&quot;equal&quot;)
        # Set Levels
        min_loss = zs.min()
        max_loss = zs.max()
        levels = tf.exp(
            tf.linspace(
                tf.math.log(min_loss), tf.math.log(max_loss), num=levels
            )
        )
        # Create Contour Plot
        CS = ax.contour(
            xs,
            ys,
            zs,
            levels=levels,
            cmap=&quot;magma&quot;,
            linewidths=0.75,
            norm=mpl.colors.LogNorm(vmin=min_loss, vmax=max_loss * 2.0),
        )
        ax.clabel(CS, inline=True, fontsize=8, fmt=&quot;%1.2f&quot;)
        return ax
</code></pre>
<p>Let’s try it out. We’ll create a simple fully-connected network to fit a curve to this parabola:</p>
<pre class="python"><code># Create some data
NUM_EXAMPLES = 256
BATCH_SIZE = 64
x = tf.random.normal(shape=(NUM_EXAMPLES, 1))
err = tf.random.normal(shape=x.shape, stddev=0.25)
y = x ** 2 + err
y = tf.squeeze(y)
ds = (tf.data.Dataset
      .from_tensor_slices((x, y))
      .shuffle(NUM_EXAMPLES)
      .batch(BATCH_SIZE))
plt.plot(x, y, 'o', alpha=0.5);
</code></pre>
<p><img src="../../images/loss-surface-parabola.png" /></p>
<pre class="python"><code># Fit a fully-connected network (ie, a multi-layer perceptron)
model = keras.Sequential([
  layers.Dense(64, activation='relu'),
  layers.Dense(64, activation='relu'),
  layers.Dense(64, activation='relu'),
  layers.Dense(1)
])
model.compile(
  loss='mse',
  optimizer='adam',
)
history = model.fit(
  ds,
  epochs=200,
  verbose=0,
)

# Look at fitted curve
grid = tf.linspace(-4, 4, 3000)
fig, ax = plt.subplots()
ax.plot(x, y, 'o', alpha=0.1)
ax.plot(grid, model.predict(grid).reshape(-1, 1), color='k')
</code></pre>
<p><img src="../../images/loss-surface-parabola-fit.png" /></p>
<p>Looks like we got an okay fit, so now we’ll look at a random slice from the loss surface:</p>
<pre class="python"><code># Create loss surface
coords = RandomCoordinates(model.get_weights())
loss_surface = LossSurface(model, x, y)
loss_surface.compile(points=30, coords=coords)

# Look at loss surface
plt.figure(dpi=100)
loss_surface.plot()
</code></pre>
<p><img src="../../images/loss-surface-random-slice-result.png" /></p>
<h1 id="improving-the-view">Improving the View</h1>
<p>Getting a good plot of the path the parameters take during training requires one more trick. A path through a random slice of the landscape tends to show too little variation to get a good idea of how the training actually proceeded. A more representative view would show us the directions through which the parameters had the <em>most</em> variation. We want, in other words, the first two principal components of the collection of parameters assumed by the network during training.</p>
<pre class="python"><code>from sklearn.decomposition import PCA

# Some utility functions to reshape network weights
def vectorize_weights_(weights):
    vec = [w.flatten() for w in weights]
    vec = np.hstack(vec)
    return vec


def vectorize_weight_list_(weight_list):
    vec_list = []
    for weights in weight_list:
        vec_list.append(vectorize_weights_(weights))
    weight_matrix = np.column_stack(vec_list)
    return weight_matrix


def shape_weight_matrix_like_(weight_matrix, example):
    weight_vecs = np.hsplit(weight_matrix, weight_matrix.shape[1])
    sizes = [v.size for v in example]
    shapes = [v.shape for v in example]
    weight_list = []
    for net_weights in weight_vecs:
        vs = np.split(net_weights, np.cumsum(sizes))[:-1]
        vs = [v.reshape(s) for v, s in zip(vs, shapes)]
        weight_list.append(vs)
    return weight_list


def get_path_components_(training_path, n_components=2):
    # Vectorize network weights
    weight_matrix = vectorize_weight_list_(training_path)
    # Create components
    pca = PCA(n_components=2, whiten=True)
    components = pca.fit_transform(weight_matrix)
    # Reshape to fit network
    example = training_path[0]
    weight_list = shape_weight_matrix_like_(components, example)
    return pca, weight_list


class PCACoordinates(object):
    def __init__(self, training_path):
        origin = training_path[-1]
        self.pca_, self.components = get_path_components_(training_path)
        self.set_origin(origin)

    def __call__(self, a, b):
        return [
            a * w0 + b * w1 + wc
            for w0, w1, wc in zip(self.v0_, self.v1_, self.origin_)
        ]

    def set_origin(self, origin, renorm=True):
        self.origin_ = origin
        if renorm:
            self.v0_ = normalize_weights(self.components[0], origin)
            self.v1_ = normalize_weights(self.components[1], origin)
</code></pre>
<p>Having defined these, we’ll train a model like before but this time with a simple callback that will collect the weights of the model while it trains:</p>
<pre class="python"><code># Create data
ds = (
    tf.data.Dataset.from_tensor_slices((inputs, outputs))
    .repeat()
    .shuffle(1000, seed=SEED)
    .batch(BATCH_SIZE)
)


# Define Model
model = keras.Sequential(
    [
        layers.Dense(64, activation=&quot;relu&quot;, input_shape=[1]),
        layers.Dense(64, activation=&quot;relu&quot;),
        layers.Dense(64, activation=&quot;relu&quot;),      
        layers.Dense(1),
    ]
)

model.compile(
    optimizer=&quot;adam&quot;, loss=&quot;mse&quot;,
)

training_path = [model.get_weights()]
# Callback to collect weights as the model trains
collect_weights = callbacks.LambdaCallback(
    on_epoch_end=(
        lambda batch, logs: training_path.append(model.get_weights())
    )
)

history = model.fit(
    ds,
    steps_per_epoch=1,
    epochs=40,
    callbacks=[collect_weights],
    verbose=0,
)
</code></pre>
<p>And now we can get a view of the loss surface more representative of where the optimization actually occurs:</p>
<pre class="python"><code># Create loss surface
coords = PCACoordinates(training_path)
loss_surface = LossSurface(model, x, y)
loss_surface.compile(points=30, coords=coords, range=0.2)
# Look at loss surface
loss_surface.plot(dpi=150)
</code></pre>
<p><img src="../../images/loss-surface-pca-slice-result.png" /></p>
<h1 id="plotting-the-optimization-path">Plotting the Optimization Path</h1>
<p>All we’re missing now is the path the neural network weights took during training in terms of the transformed coordinate system. Given the weights <span class="math inline">\(W\)</span> for a neural network, in other words, we need to find the values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> that correspond to the direction vectors we found via PCA and the origin weights <span class="math inline">\(W_c\)</span>.</p>
<p><span class="math display">\[W - W_c = a W_0 + b W_1\]</span></p>
<p>We can’t solve this using an ordinary inverse (the matrix <span class="math inline">\( \left[\begin{matrix} W_0 &amp; W_1 \end{matrix} \right] \)</span> isn’t square), so instead we’ll use the Moore-Penrose pseudoinverse, which will give us a least-squares optimal projection of <span class="math inline">\(W\)</span> onto the coordinate vectors:</p>
<p><span class="math display">\[\left[\begin{matrix} W_0 &amp; W_1 \end{matrix}\right]^+ (W - W_c) = (a, b)\]</span></p>
<p>This is the ordinary least squares solution to the equation above.</p>
<pre class="python"><code>def weights_to_coordinates(coords, training_path):
    &quot;&quot;&quot;Project the training path onto the first two principal components
using the pseudoinverse.&quot;&quot;&quot;
    components = [coords.v0_, coords.v1_]
    comp_matrix = vectorize_weight_list_(components)
    # the pseudoinverse
    comp_matrix_i = np.linalg.pinv(comp_matrix)
    # the origin vector
    w_c = vectorize_weights_(training_path[-1])
    # center the weights on the training path and project onto components
    coord_path = np.array(
        [
            comp_matrix_i @ (vectorize_weights_(weights) - w_c)
            for weights in training_path
        ]
    )
    return coord_path


def plot_training_path(coords, training_path, ax=None, end=None, **kwargs):
    path = weights_to_coordinates(coords, training_path)
    if ax is None:
        fig, ax = plt.subplots(**kwargs)
    colors = range(path.shape[0])
    end = path.shape[0] if end is None else end
    norm = plt.Normalize(0, end)
    ax.scatter(
        path[:, 0], path[:, 1], s=4, c=colors, cmap=&quot;cividis&quot;, norm=norm,
    )
    return ax
</code></pre>
<p>Applying these to the training path we saved means we can plot them along with the loss landscape in the PCA coordinates:</p>
<pre class="python"><code>pcoords = PCACoordinates(training_path)
loss_surface = LossSurface(model, x, y)
loss_surface.compile(points=30, coords=pcoords, range=0.4)
ax = loss_surface.plot(dpi=150)
plot_training_path(pcoords, training_path, ax)
</code></pre>
<p><img src="../../images/loss-surface-pca-path-result.png" /></p>
  </section>
  
</article>

  </div>
</div>

<!-- Pagination -->
<nav class="paginate-container" aria-label="Pagination">
  <div class="pagination">
    
    <a class="previous_page text-gray-dark" rel="previous" aria-label="Previous Page" href="../../posts/getting-started-with-tpus/">⮜ Previous</a>
    

    
    <a class="next_page text-gray-dark" rel="next" aria-label="Next Page" href="../../posts/what-convnets-learn/">Next ⮞</a>
    
  </div>
</nav>

<!-- Talkyard Comments -->
<div class="container-m">
  <div class="rounded-2 box-shadow-medium px-2 py-1 bg-white">
    <script>talkyardServerUrl='https://comments-for-mathformachines-com.talkyard.net';</script>
    <script async defer src="https://c1.ty-cdn.net/-/talkyard-comments.min.js"></script>
    <!-- You can specify a per page discussion id on the next line, if your URLs might change. -->
    <div class="talkyard-comments" data-discussion-id style="margin-top: 45px;">
      <noscript>Please enable Javascript to view comments.</noscript>
      <p style="margin-top: 25px; opacity: 0.9; font-size: 96%">Comments powered by
        <a href="https://www.talkyard.io">Talkyard</a>.</p>
    </div>
  </div>
</div>

          </div>
          
          <div id="right" class="ml-2 px-3 pb-3 text-gray bg-white">
            <div id="side">
              <div>
                <h1>Tags</h1>
                <a style="font-size: 100%" href="../../tags/advice/">advice</a> <a style="font-size: 105%" href="../../tags/bayesian/">bayesian</a> <a style="font-size: 100%" href="../../tags/BMA/">BMA</a> <a style="font-size: 100%" href="../../tags/calculator/">calculator</a> <a style="font-size: 100%" href="../../tags/career/">career</a> <a style="font-size: 100%" href="../../tags/category-theory/">category-theory</a> <a style="font-size: 105%" href="../../tags/classification/">classification</a> <a style="font-size: 100%" href="../../tags/convnets/">convnets</a> <a style="font-size: 100%" href="../../tags/coordinates/">coordinates</a> <a style="font-size: 100%" href="../../tags/covectors/">covectors</a> <a style="font-size: 100%" href="../../tags/cql/">cql</a> <a style="font-size: 115%" href="../../tags/data-science/">data-science</a> <a style="font-size: 105%" href="../../tags/decision-boundaries/">decision-boundaries</a> <a style="font-size: 105%" href="../../tags/deep-learning/">deep-learning</a> <a style="font-size: 100%" href="../../tags/eigenvalues/">eigenvalues</a> <a style="font-size: 100%" href="../../tags/engrams/">engrams</a> <a style="font-size: 105%" href="../../tags/finance/">finance</a> <a style="font-size: 100%" href="../../tags/functional-programming/">functional-programming</a> <a style="font-size: 100%" href="../../tags/generalized-inverse/">generalized-inverse</a> <a style="font-size: 100%" href="../../tags/geometry/">geometry</a> <a style="font-size: 100%" href="../../tags/haskell/">haskell</a> <a style="font-size: 100%" href="../../tags/investing/">investing</a> <a style="font-size: 100%" href="../../tags/julia/">julia</a> <a style="font-size: 100%" href="../../tags/kaggle/">kaggle</a> <a style="font-size: 100%" href="../../tags/LDA/">LDA</a> <a style="font-size: 100%" href="../../tags/least-squares/">least-squares</a> <a style="font-size: 115%" href="../../tags/linear-algebra/">linear-algebra</a> <a style="font-size: 100%" href="../../tags/linear-equations/">linear-equations</a> <a style="font-size: 100%" href="../../tags/matrix-decomposition/">matrix-decomposition</a> <a style="font-size: 100%" href="../../tags/MCMC/">MCMC</a> <a style="font-size: 100%" href="../../tags/memory/">memory</a> <a style="font-size: 100%" href="../../tags/moore-penrose-inverse/">moore-penrose-inverse</a> <a style="font-size: 100%" href="../../tags/neural-networks/">neural-networks</a> <a style="font-size: 100%" href="../../tags/neuroscience/">neuroscience</a> <a style="font-size: 100%" href="../../tags/NLP/">NLP</a> <a style="font-size: 100%" href="../../tags/numpy/">numpy</a> <a style="font-size: 110%" href="../../tags/python/">python</a> <a style="font-size: 100%" href="../../tags/QDA/">QDA</a> <a style="font-size: 110%" href="../../tags/R/">R</a> <a style="font-size: 100%" href="../../tags/ReLUs/">ReLUs</a> <a style="font-size: 100%" href="../../tags/retirement/">retirement</a> <a style="font-size: 100%" href="../../tags/review/">review</a> <a style="font-size: 100%" href="../../tags/sage/">sage</a> <a style="font-size: 100%" href="../../tags/sgd/">sgd</a> <a style="font-size: 100%" href="../../tags/simulation/">simulation</a> <a style="font-size: 100%" href="../../tags/singular-values/">singular-values</a> <a style="font-size: 100%" href="../../tags/stacking/">stacking</a> <a style="font-size: 100%" href="../../tags/talk/">talk</a> <a style="font-size: 100%" href="../../tags/tensorflow/">tensorflow</a> <a style="font-size: 100%" href="../../tags/tensors/">tensors</a> <a style="font-size: 100%" href="../../tags/tpus/">tpus</a> <a style="font-size: 110%" href="../../tags/tutorial/">tutorial</a> <a style="font-size: 100%" href="../../tags/vectors/">vectors</a> <a style="font-size: 105%" href="../../tags/visualization/">visualization</a>
              </div>
              <div>
                <h1>Links</h1>
                <a href="https://www.r-bloggers.com/">R-bloggers</a>
              </div>
            </div>
          </div>
        </div>

        <footer class="border p-3 bg-white text-gray">
  <div class="d-flex flex-justify-between flex-items-end">
    <div>
      Site proudly generated by <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
    </div>
    <div>
<span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Math for Machines</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://mathformachines.com" property="cc:attributionName" rel="cc:attributionURL">Ryan Holbrook</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.
    </div>
</footer>

        
      </div>
  </body>
</html>
